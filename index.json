[{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/resource-engineering/overview/","title":"Overview","tags":[],"description":"","content":"A Resource is an abstract term for a leaf level component of an Assembly Service. Resource instances ultimately represent infrastructure and the software running at a location to fulfil a function.\nAgile Lifecycle Manager (ALM) requires a descriptor to identify the properties, transitions, operations, metrics, and the infrastructure available for each resource instance. The Resource can then be added to any number of Assembly designs and instantiated upon creation of an Assembly.\nAlthough ALM will determine the transitions and operations performed on a Resource instance as an Assembly instance moves through its lifecycle, it is the role of a Resource Manager to communicate with a target location (VIM) to bring it into service and, under the instruction of the Intent Engine, to perform discrete transitions and operations on it. This relationship with Resource Managers allows ALM to abstract both the specifics of communicating with a given VIM and the execution of given tasks to specialised Resource Managers.\nResource Managers Open source Resource Manager implementations exist and ALM is designed to operate with multiple Resource Managers concurrently. ALM comes with a carrier-grade Resource Manager out-of-the-box, named Brent. This Resource Manager integrates with Resource Drivers to support managing multiple infrastructure (e.g VIM) types and lifecycle scripting technologies.\nThe remainder of this guide focuses on how to build Resource packages managed by Brent. Read on to begin creating Resource packages and working with drivers.\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/administration/configuration/accessing-vault/","title":"Accessing Vault","tags":[],"description":"","content":"The open source HashiCorp Vault is used to securely store configuration data for Agile Lifecycle Manager (ALM). It is exposed on an ingress port as host vault.lm\nVault has a GUI which can be used to alter the data held. This GUI is protected by a Vault Token with the token allocated to vault during the installation at which time the time-to-live for the token is also set. On expiry, this key will need to be changed or reset to its current value to maintain access to vault for both users and the set of ALM microservices.\nan example Vault URL being\nhttps://vault.lm:32443\rThe exact ingress port will be specific to your deployment and can be found by querying the port for the ingress controller.\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/administration/","title":"Administration","tags":[],"description":"","content":""},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/administration/configuration/","title":"Configure LM","tags":[],"description":"","content":""},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/resource-engineering/resource-packages/brent/create-brent-resource-package/","title":"Create a Brent Resource Package","tags":[],"description":"","content":"Introduction and Structure A Brent resource package is a zip file conforming to a specific layout. For example, if the resource uses the Openstack driver and Ansible driver, the resource package directory will be as follows:\nDefinitions/\rlm/\rresource.yaml # mandatory\rLifecycle/\ransible/\rconfig/\rhost_vars/\rmyhost.yaml\rinventory\rscripts/\rInstall.yaml\rStart.yaml\ropenstack/\rheat.yaml\rThe Definitions directory contains the LM descriptor file. The Lifecycle directory should be populated with scripts and templates required by the chosen driver(s).\nCreating a Resource Package You can create a Resource package by creating the necessary directories and files described in the section below, then zipping the contents of the parent directory using any zip command line tools or applications. However, it is more efficient to first create a skeleton Resource package using LMCTL v2.4.1+. To do this, use the project create command with the brent --rm option:\nmkdir Example \u0026amp;\u0026amp; cd Example\rlmctl project create --type Resource --rm brent\rWhen you are ready to build your project, use the project build command:\nlmctl project build\rThis will create an LMCTL package at _lmctl/build. Note that the LMCTL package is not a Resource package. The package and it\u0026rsquo;s structure is specific to LMCTL, the Resource is actually inside. You should continue to use LMCTL to manage the package rather than attempting to directly onboard it through Rest APIs.\nYou can push the package to a Agile Lifecycle Manager (ALM) environment with project push:\nlmctl project push myenvironment\rSee the example resource package guide for more details on the construction of a Brent resource package, including how to build on the skeleton generated by lmctl.\nIf you have an existing example resource package for LM v2.1 that you wish to reuse, you can install LMCTL v2.5.0+ and use --autocorrect flag to adapt your package to work with LM v2.2:\nlmctl project validate --autocorrect\rNext Steps Once you understand the basics of a Resource package, you may move on to understand Drivers and how they are used by Brent to manage the Resource.\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/reference/security/default-client-credentials/","title":"Default Client Credentials","tags":[],"description":"","content":"During the default installation of Agile Lifecycle Manager (ALM) your system is deployed with a set of credentials configured for three standard clients. Unless changed (recommended), these will be as follows;\nDefault Clients    Client ID Secret Details     LmClient pass123 Default admin client. The id and secret can be configured at installation   NimrodClient pass123 Client used by the LM user interface. The secret can be configured at installation   DokiClient pass123 Client used by the LM behaviour center. The secret can be configured at installation    "},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/installation/cicdhub/cicd-hub-start/","title":"Getting Started","tags":[],"description":"","content":"This section details how to install the CI/CD Hub using Helm on an existing Kubernetes cluster. This will install the following services:\n Gogs: Lightweight self-hosted Git service Nexus: Artifact repository manager Jenkins: Automation server to enable continuous integration and continuous delivery Openldap: Open-source implementation of the Lightweight Directory Access Protocol, for user management of Agile Lifecycle Manager (ALM) environments Docker Registry: Registry for hosting docker images Nginx Ingress: Ingress controller to support accessing some services with Ingress  Pre-requisites Kubernetes A Kubernetes cluster is required to install the Hub. This environment must be configured with:\n a Storage Class to provision persistent volumes in your cluster a Namespace to install into (you may use the default)  In addition, you will need the following client tools, pre-configured with access to your Kubernetes environment:\n Helm kubectl  ICP If installing on ICP we recommend you read through our ICP Pre-Install Considerations before continuing.\nCI/CD Hub Artifacts Download the CI/CD Hub Helm chart from the releases page on GitHub.\nOffline Install If you need to install the CI/CD Hub in an environment without internet access, please read through the Offline Install Instructions to pre-pull additional artifacts required during the install.\nConfiguration The CI/CD Hub is configured through Helm chart values.\nYou may check the default configuration values of the chart using helm inspect:\nhelm inspect values \u0026lt;cicdhub-helm-chart\u0026gt;\rConfiguration Steps This following sections will take you through the default configuration of the chart. They will also explain how to override them (at your discretion), using a custom values file. This file is a custom YAML formatted file, which you must create when changing values, so it can later be passed during installation.\nCreate the custom file:\ntouch custom-values.yaml\rAny future references to the custom values should be understood to mean this file.\nStart configuring your installation with Sizing Limits\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/installation/lm/production/production-start/","title":"Getting Started","tags":[],"description":"","content":"This section details how to install Agile Lifecycle Manager (ALM) using Helm on an existing Kubernetes cluster. This will install the following Helm charts:\n helm-foundation - reference Helm Chart for deploying the third-party components required by ALM. lm-configurator - Helm Chart executing a Kubernetes job to create relevant schemas and other configuration required by ALM. lm-helm - Helm Chart for deploying the ALM Microservices  Pre-requisites Kubernetes A Kubernetes cluster is required to install ALM. This environment must be configured with:\n a Storage Class to provision persistent volumes in your cluster a Namespace to install into (you may use the default)  In addition, you will need the following client tools, pre-configured with access to your Kubernetes environment:\n Helm kubectl  The nodes in your cluster should have synchronized clocks. To synchronize your clocks, you can use network time protocol (NTP). For more information about setting up NTP, see the user documentation for your operating system.\nICP If installing on ICP we recommend you read through our ICP Pre-Install Considerations before continuing.\nALM Artifacts You should already have the lifecycle Manager package which contains the Helm charts and binaries required for the installation of LM. This should contain:\n lm-helm-charts - the Helm charts that will install ALM lm-docker-source - the binaries requires to build the LM Docker images  Configuration ALM is configured through Helm chart values.\nYou may check the default configuration values of the chart using helm inspect:\nhelm inspect values \u0026lt;lm-helm-chart\u0026gt;\rConfiguration Steps The configuration steps further in this guide will take you through the default configuration of the chart. They will also explain how to override them (at your discretion), using a custom values file. This file is a custom YAML formatted file, which you must create when changing values, so it can later be passed during installation.\nCreate the custom file:\ntouch custom-values.yaml\rAny future references to the custom values should be understood to mean this file.\nBefore making any configuration changes, it is necessary to make the docker images available.\nBuild Docker Images Before you can install LM you will need to build the Docker images for all its components.\nThe lm-docker-source distribution includes the sources necessary to build docker images for the ALM applications.\nPre-requisites Before proceeding with the installation you will need:\n Docker Docker Compose  Build Docker Images If the ALM Docker images already exist in a registry, skip to Using a Docker Registry\nFrom a shell, navigate to the root of the lm-docker-source and use docker-compose to build the images from the docker-compose.yml file included in the distribution:\ndocker-compose build\rOnce completed the images will be available on the local docker machine:\ndocker images\rapollo ${apollo.version} 1f22cdeeb226 1 hour ago 374MB\rconductor ${conductor.version} a0c5615baa5b 1 hour ago 450MB\rdaytona ${daytona.version} 18d1ab71943b 1 hour ago 387MB\rgalileo ${galileo.version} cc175dfc2324 1 hour ago 461MB\rishtar ${ishtar.version} 023896abd74e 1 hour ago 374MB\rnimrod ${nimrod.version} facf6ac62932 1 hour ago 379MB\rrelay ${relay.version} 50e381fc1f84 1 hour ago 369MB\rtalledega ${talledega.version} 8aeb064b3a29 1 hour ago 432MB\rwatchtower ${watchtower.version} 7268ba88d14b 1 hour ago 383MB\rlm-configurator ${lm-configurator.version} 0434602b9bfc 1 hour ago 683MB\rPush to Docker Registry It is recommended to push the Docker images to a Docker Registry, making them available to pull from any node in your cluster. You can skip the Docker Registry steps if running a single node cluster on which the images were built.\nTo push the images, first open the docker-compose.yml file for editing. Update the image value, for each service under services, with the host and port of your Docker Registry:\n...\rservices:\rapollo:\rimage: myregistry:5000/apollo:${apollo.version}\r...\rInsecure Registry If your Docker Registry is unsecured you need to ensure it is included in as an insecure registry in your /etc/docker/daemon.json':\n{\r\u0026quot;insecure-registries\u0026quot;: [\u0026quot;myregistry:5000\u0026quot;]\r}\rNow build and push the images with docker-compose:\ndocker-compose build\rdocker-compose push\rUsing a Docker Registry Once docker images exist in the Docker Registry, you will be able to configure the helm charts to pull the images from it when installing ALM with lm-helm-charts.\nIf you skipped the build steps above and are hosting an insecure registry, see Insecure Registry\nTo allow Helm to pull images from your registry, it is necessary to add the following to your custom Helm values file, modifying as necessary:\nglobal: docker: registryEnabled: true imagePullPolicy: IfNotPresent registryIp: \u0026#34;myregistry\u0026#34; registryPort: 5000 For ICP installations you will have generated an imagePullSecret for the namespace into which lm-configurator is being installed, it will need to be added to your custom Helm values file:\nimagePullSecrets: - \u0026lt;your imagePullSecret name\u0026gt; For information on creating an imagePullSecret, see the ICP documentation here\nOffline Install If you need to install ALM in an environment without internet access, please read through the Offline Install Instructions to pre-pull additional artifacts required during the install.\nNext Steps Start configuring your installation by reviewing the Access Configuration\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/installation/resource-manager/ansible-rm/ansible-rm-start/","title":"Getting Started","tags":[],"description":"","content":"This section details a recommended installation of the IBM OSSLM Ansible Resource Manager.\nPre-requisites Kubernetes A Kubernetes cluster is required to install the Ansible RM. This environment must be configured with:\n a Storage Class to provision persistent volumes in your cluster a Namespace to install into (you may use the default)  In addition, you will need the following client tools, pre-configured with access to your Kubernetes environment:\n Helm kubectl  Agile Lifecycle Manager (ALM) We recommend installing a ALM environment first so the Ansible RM may share the existing Kafka cluster.\nAnsible RM Helm Chart Download the Ansible RM Helm chart from the releases page on GitHub.\nConfiguration The Ansible RM is configured through Helm chart values.\nYou may check the default configuration values of the chart using helm inspect:\nhelm inspect values \u0026lt;ansible-rm-helm-chart\u0026gt;\rThis guide will describe how to override the default settings, by referring to a custom values file. This file is a custom YAML formatted file, which you must create so it can later be passed during installation.\nCreate the custom file:\ntouch custom-values.yaml\rAny future references to the custom values should be understood to mean this file.\nStart configuring your installation with Storage\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/network-service-design/how-to-design-a-network-service/","title":"How to Design a Network Service","tags":[],"description":"","content":"There are a number of tasks involved in designing a Network Service (NS). In order to understand what a network service is, we first need to look at what it consists of.\nWhat is a Network Service? A Network Service, is a collection of VNF and/or PNF that will be used by Agile Lifecycle Manager (ALM) to instantiate either a complete end-to-end service for a customer or, a significant portion of a customer’s service. A network service (NS) is created by using a ALM Assembly Descriptor. The descriptor will compose and reference other Network Services, VNF or PNF. Generally, the Network Service will not refer directly to VNFC.\n   Component Sub-component Description     Network Service Network Service Descriptor (Assembly) A Network Service Descriptor (Assembly) is a yam file that describes the properties and composition of the Network Service.   Service behaviour test scenarios scripts that are used to test NS functionality and are created during the design phase.    VNF A VNF provides a single function within a Network Service. A Network Service may consist of one or multiple VNFs.     Tasks to develop a Network Service To develop a Network Service, there are multiple steps that are outlined in the picture below: Tools to develop a Network Service To develop a NS, there are multiple tools that are used to allow people to collaborate on creating, testing, maintaining and automating the development of Network Services.\nCI/CD Hub The Continuous Integration / Continuous Delivery Hub, or CI/CD Hub, provides a set of standards, tools and automation patterns designed to simplify and accelerate the network service continuous integration and deployment journey. The CI/CD Hub includes a source code repository, image repository and an automation server.\nMore information about the tools in the CI/CD Hub can be found here .\nALM environment Designing a NS is done through the Designer section of ALM. This is typically done on a local or dedicated development environment. The Designer has a graphical User Interface with which you can drag and drop VNF and other elements, establish relationships, modify properties, etc.\nLMCTL LMCTL is a command line tool that allows you to create, pull, push and test Network Service projects to a ALM environment. LMCTL is also used by the automation server to issue these commands and execute the automated pipeline. The LMCTL tool can be installed from the CI/CD Hub.\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/installation/lm/alm-start/","title":"Install and Set Up Lifecycle Manager","tags":[],"description":"","content":"Choosing your Deployment The first step in installing the Agile Lifecycle Manager (ALM) is to determine whether you are installing a Development or Production deployment.\nAll in one Deployment Refer to this guide for an all in one deployment\nOpenShift Deployment Refer to this guide for a development OpenShift deployment\nStaging and Production Deployment Refer to this guide for a staging or production deployment\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/installation/","title":"Installation","tags":[],"description":"","content":""},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/cicd/introduction/","title":"Introduction","tags":[],"description":"","content":"Pre-requisites The following links introduce terms used through out this section:\n  Key ALM concepts introduce the Agile Lifecycle Manager (ALM) programming model used to model VNFs and Network Services.\n  Assembly and resource specifications give detailed descriptions of how to model lifecycles in ALM.\n  Introduction Cloud DevOps best practices and principles are at the heart of the ALM solution. To scale any Cloud based networking program, a unified operations and engineering model is combined with a set of automation tools that can simplify and automate the complexities of an end-to-end VNF or Network Service lifecycle.\nThe ALM CI/CD tools and processes are designed to simplify and automate the following DevOps tasks:\n Onboard VNFs and design Network Services: Quickly integrate and package the lifecycle actions required to operate a VNF or any virtual or physical network appliance. Behaviour Testing in pre-production: Deploy VNFs to pre-production environments and easily script complex operational behaviour tests to ensure the onboarded VNF behaves as expected in all \u0026ldquo;day 1\u0026rdquo; or \u0026ldquo;day 2\u0026rdquo; lifecycle tasks. Deploy to production: Once fully tested, auto deploy to production environments. Monitor and Change: Monitor and sense environmental or VNF state and auto scale, heal or move components of the network service. Report and Resolve issues: Errors in lifecycle actions or VNF software found in production are reported and trigger an upgrade process that rebuilds a new version of the VNF.  The CI/CD Hub wraps the core ALM automation capabilities with tools that support the \u0026ldquo;day 0\u0026rdquo; VNF onboarding and testing processes and also the \u0026ldquo;day 2\u0026rdquo; VNF change management tasks.\nThe CI/CD Hub provides a set of tools that manage VNF and Network Services artifacts across the following NFV orchestration systems:\n LM NFVO: Packages of assembly descriptors and behaviour tests for network service versions are packaged and deployed to ALM instances . LM Generic VNFM: Software Images, resource descriptors, lifecycle scripts and behaviour tests that wrap a VNF or PNF are packaged and deployed to ALM and its resource managers. 3rd Party VNFMs: External VNF artifacts are packaged and deployed to 3rd party VNFMs. Virtual Infrastructure Managers: VNF component software image versions are deployed to VIMs.  The picture above shows a complete ALM CI/CD Environment. ALM design tools create VNF/Network Service descriptors and behaviour test scripts, using a Git repository as their source version control. Supplemental artifacts such as VIM software images are stored in a general repository such as Nexus.\nVersions of VNF and Network Service packages are taken from these repositories and built and tested in development and pre-production environments. A CI server, such as Jenkins, pulls ALM artifacts from Git and deploys to LM build slaves, the CI server also pushes software images to the appropriate development or pre-production VIMs attached to the LM build slaves. Any included behaviour tests in the VNF or Network Service project are run by the CI server to validate this versions expected behaviour.\nOn successful completion of VNF or Network Service behaviour tests, the CI server uses ALM tools to package a version of a binary VNF and Network Service package and stores it in the general repository.\nThe CI/CD Hub provides a set of open source software components that play the Git source control, general repository and CI server roles as defined above. The CI/CD Hub is a reference implementation intended to demonstrate how to implement a ALM CI/CD pipeline, it is not a supported product. The CI/CD Hub can be used to run a production pipeline, or you can swap components out and use the tool of your choice, but the CI/CD Hub project is intended for demonstration purposes only.\nThe CI/CD Hub reference implementation provides installer scripts to stand up and attach the following software tools to your ALM design and build slaves.\n Git: A lightweight Gogs Git repository is installed as the descriptor and behaviour script source control server Nexus: Nexus general repository is installed to provide a general image and package repository. Jenkins: Jenkins CI Server is installed to automate the package build and release processes.  A basic getting started guide and instructions how to run a \u0026ldquo;hello world\u0026rdquo; demo is also installed to the Gogs server. You can learn more about the CI/CD Hub software here.\nAs stated above, development and build slave ALM instances need to be in place and attached to the CI/CD Hub as appropriate, please follow LM installation guide and additional configuration detailed in the CI/CD Hub guide to \u0026ldquo;connect\u0026rdquo; LM instances:\n ALM Design Tools: ALM instances for designing descriptors combined with the LMCTL command line tool push/pull VNF or Network Service projects to the CI/CD Hub Git repository.   ALM Build Slaves: ALM instances can be configured to use the CI/CD Hub shared services, e.g. OpenLDAP and managed by the Jenkins CI Server to auto deploy, test and package VNFs or Network Services.  VNF and Network Service Packages The CI/CD Hub extends \u0026ldquo;standard\u0026rdquo; Cloud software toolchains with the LMCTL tool that manages ALM packages.\nThe following packages need to be managed by the LM CI/CD process and tools:\n Network Service Package: Network service descriptors that organize VNFs are combined with behaviour tests. Native VNF Packages: VNF artifacts designed and built to be run in the ALM VNFM. Foreign VNF Packages: VNF artifacts designed and built to be run on a 3rd party VNFM.  Package Contents VNF and Network Service packages can be versioned and distributed to ALM environments. ALM command line tools aid in the creation and management of these binary NFV packages.\nThe sections below give an overview of the types of packages included in the CI/CD process.\nVNF Package A VNF package can contain the following artifacts:\n VNF Descriptor: This assembly descriptor declares properties and values, organizes any children VNFCs and defines operations and policies. VNFC resource descriptors: This resource descriptor declares properties, supported lifecycle actions and any metrics produced by the VNFC VNFC Lifecycle scripts: Depending on the resource manager used to execute lifecycle actions, appropriate scripts or software is provided that \u0026ldquo;run\u0026rdquo; each supported lifecycle action. VNF Behaviour tests: ALM behaviour tests are included that run the VNF and its VNFCs through a set of functional tests.  Network Service Package A typical Network Service package will contain the following artifacts:\n Network Service Descriptor: This assembly descriptor declares properties and values, organizes any children VNFs and defines operations and policies Network Service Behaviour tests: ALM behaviour tests are included that run the Network Service and its VNFs through a set of performance and operational interoperability tests.  Package lifecycle Network Service and VNF Packages have a simple state model.\n Development: VNF or Network Service engineers are in the early stages of package development and perform their own local testing. Pre-Production/Test: Packages are ready for exhaustive testing triggering a \u0026ldquo;build\u0026rdquo;. Production: Packages have passed all exhaustive testing and have been deemed ready for production.  The CI/CD methodology and process can handle packages in these various states appropriately. As seen above, Network Service package development are dependent on VNFs being in pre-production state or higher.\nThe picture above shows the types of environments and supplemental packages in a typical package workflow.\nOnboarding/development tasks typically use shared local ALM and virtual VIM environments to perform the initial creation of VNF or Network Service packages. In addition the engineer will typically create his/her own unit test style VNF packages that are used to test the target VNF or Network Service package behaviour is operationally correct.\nIn the pre-production/test stage, packages are moved to environment representative of the production environment. ALM and VIM environments are typically dedicated to running performance and interoperability tests. The environments are owned by the automated pipeline.\nOnce fully tested, packages are available to be deployed to the Production environment.\nTest Packages Behaviour testing a VNF or Network Service requires a set of Test VNFs be developed that run functional tests or generate and monitor traffic. Test VNF package lifecycles are identical to the VNFs under test, binary packages with versions are run through the same CI/CD process.\nIn addition to the Test VNF package that performs the actual test, a set of behaviour scripts are included with the test package that run a series of tests and evaluate of the behaviour reported by one of more test VNFs is as expected.\nSee behaviour testing for more details\nVNF CI/CD Process This sections lays out the VNF package lifecycle workflow.\n Load Images: Upload one or more VNFC software appliance images to the general repository. Create VNFC Lifecycle: For each VNFC in the VNF, create a resource lifecycle and include the scripts or software that implement the standard lifecycle actions. Create/Load VNF Package: For native packages, create a new package version. For foreign packages load a version. Create Test Packages: Create the test packages that will test VNF behaviour. Create or clean environment: Create or clean a development or pre-production environment. Load VNF package version: Load the VNF under test into the target environment. Load versions of test packages: Load dependent test packages Run test packages and store: Run behaviour test and store the results. Progress package to next state/stage: On success create a binary package with a date and a version.  Network Service CI/CD Process This sections lays out the Network Service package workflow.\n Design network service: Design network service, including one of more VNF descriptors. Create package version: Create a network service package version. Create Test packages: Include test packages and behaviour tests that will evaluate the network service behaviour. Create or clean environment: Clean or create a development or pre-production environment. Load dependent VNF packages: Load the network service and all of its dependent VNF packages to the target environment. Load version of test packages: Load the test packages required to run behaviour scripts to the target environment. Run Tests and store package on success: Run network service behaviour tests and store the results. Move package to next state/stage: On success, create a dated version of the network service and store in the general repository.  Next Steps and further reading To get started with a new project see getting started guide.\nTo learn more about the CI/CD Hub software, read the software overview section\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/administration/configuration/languages/language-overview/","title":"Languages Overview","tags":[],"description":"","content":"The Agile Lifecycle Manager (ALM) User Interface supports multiple languages via translation.json files. A translation file can be provided for each locale the user requires which contains mappings for all text within the ALM UI (see sample translation file below for reference).\n \u0026lt;li\u0026gt;\r\u0026lt;a href=\u0026quot;https://pages.github.ibm.com/TNC/alm-docs.github.io//user-guides/administration/configuration/languages/language-overview.files/translation.json\u0026quot; \u0026gt;\rtranslation.json\r\u0026lt;/a\u0026gt;\r(29 kB)\r\u0026lt;/li\u0026gt;\r Adding Locales to ALM  To set the translations for each locale of the ALM UI you need to produce a locales.tar containing the necessary elements. The TAR should include a directory for each potential locale. Each locale is represented by a directory which should include the translation.json to be used. The name of the directory should match the ISO language code it targets (ISO 639-1 codes, ISO 3166-2 codes).  +-- en-GB\r| +-- translation.json\r+-- en-US\r+-- translation.json\rProduce a TAR called locales.tar (mandatory name) using the tar command, including each locales directory:  tar -cvzf locales.tar en-GB en-US\rThe locales.tar must be loaded into your Kubernetes environment by creating a ConfigMap.  kubectl create configmap lm-locales --from-file locales.tar\rMultiple language support can be enabled during ALM installation or for an already running ALM.  "},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/installation/lm/openshift/openshift/","title":"OpenShift Origin","tags":[],"description":"","content":"This section details how to set up a local OpenShift Origin (community distribution of OpenShift) all-in-one cluster and configure that cluster for installing Agile Lifecycle Manager (ALM).\nPre-requisites  Previous knowledge of basic installation process with LM Helm charts. Linux machine (or Linux VM running on another platform) to install OpenShift cluster.  OpenShift all-in-one set-up This guide follows OpenShift Origin (version 3.11) documentation.\nDocker  Install Docker (version 1.13 or later) onto your Linux machine. Configure the Docker daemon with an insecure registry parameter of 172.30.0.0/16. Also include the Docker registry from which you will get your LM Docker images (read more about registry of your Docker images here). Edit file /etc/docker/daemon.json:  { \u0026#34;insecure-registries\u0026#34;: [ \u0026#34;172.30.0.0/16\u0026#34;, \u0026#34;myregistry:5000\u0026#34; ] }  Restart the Docker daemon:  sudo systemctl daemon-reload\rsudo systemctl restart docker\rFirewall Ensure that your firewall allows access to the OpenShift master API (8443/tcp) and DNS (53/udp) endpoints. In RHEL and Fedora, you can create a new firewalld zone to enable this access:\n Find Docker bridge network container subnet - you should get a value like 172.17.0.0/16  docker network inspect -f \u0026quot;{{range .IPAM.Config }}{{ .Subnet }}{{end}}\u0026quot; bridge\r Create a new firewalld zone for the subnet and grant it access to the correct ports:  firewall-cmd --permanent --new-zone dockerc\rfirewall-cmd --permanent --zone dockerc --add-source 172.17.0.0/16\rfirewall-cmd --permanent --zone dockerc --add-port 8443/tcp\rfirewall-cmd --permanent --zone dockerc --add-port 53/udp\rfirewall-cmd --permanent --zone dockerc --add-port 8053/udp\rfirewall-cmd --reload\rOpenShift Client Tool  Download the oc binary from OpenShift Origin Releases page to your machine. For this set-up we would be using client tool version 3.11. Place the binary in your path.  Start an OpenShift cluster Run the following as a user that has permission to run Docker commands:\noc cluster up\rConfiguration After your OpenShift cluster has been up, log in as system admin to perform further configuration steps:\noc login -u system:admin\rHelm ALM will be installed using Helm charts, so you would need Helm client and server:\n Go into the project (similar to Kubernetes namespace) meant for Tiller:  oc project kube-system\r Run the following command (replace with desired Helm version and Tiller namespace):  oc process -f https://github.com/openshift/origin/raw/master/examples/helm/tiller-template.yaml -p TILLER_NAMESPACE=kube-system -p HELM_VERSION=v2.14.3 | oc create -f -\r Check that Tiller pod is running by command oc get po. Grant admin cluster role to Helm  oc adm policy add-cluster-role-to-user cluster-admin system:serviceaccount:kube-system:tiller\r Check that Tiller role is correct by command helm ls. There should be an empty output.  Security policies for LM\u0026rsquo;s Foundation chart These configuration steps only apply to Agile Lifecycle Manager (ALM) helm-foundation chart (version 2.1 GA).\n Grant all authenticated users access to the anyuid security context constraint (SCC):  oc adm policy add-scc-to-group anyuid system:authenticated\r Go to the project (namespace) that you plan to install LM Helm charts in:  oc project myproject\r Assign privileged SCC to specific foundation service accounts (replace myproject with your project):  oc adm policy add-scc-to-user privileged system:serviceaccount:myproject:default\roc adm policy add-scc-to-user privileged system:serviceaccount:myproject:foundation\roc adm policy add-scc-to-user privileged system:serviceaccount:myproject:foundation-nginx-ingress\roc adm policy add-scc-to-user privileged system:serviceaccount:myproject:foundation-elasticsearch-client\roc adm policy add-scc-to-user privileged system:serviceaccount:myproject:foundation-elasticsearch-data\roc adm policy add-scc-to-user privileged system:serviceaccount:myproject:foundation-elasticsearch-master\rLocal HostPath storage OpenShift supports many Persistent Volume Types but in this case we\u0026rsquo;ll be looking at hostPath Persistent Volumes created by helm-foundation. Read more about LM storage set-up here and helm inspect command on your helm-foundation chart. Default path on the host will be determined by the location from which you run the oc cluster up command, most likely $HOME/openshift.local.clusterup/openshift.local.pv/.\n Create a sub-directory for LM volumes:  sudo mkdir -p $HOME/openshift.local.clusterup/openshift.local.pv/lm\rsudo chmod -R 666 $HOME/openshift.local.clusterup/openshift.local.pv/lm\r Setup required local directories under $HOME/openshift.local.clusterup/openshift.local.pv/lm Put the full path in the values file intended for helm-foundation chart:  volumesInit: enabled: true hostPath: /home/\u0026lt;your-user\u0026gt;/openshift.local.clusterup/openshift.local.pv/lm Notes and next steps  The OpenShift cluster is now ready for installing LM Helm charts. Start your installation by reviewing the Getting Started guide. Since this OpenShift cluster is a single-node set-up for development purposes, it is not suitable for production deployment of LM. Choose minimal flavour in the sizing section Remember to run helm install with --namespace option pointing to the project that you configured earlier.  "},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/behaviour-testing/overview/","title":"Overview","tags":[],"description":"","content":"Behaviour Testing Behaviour testing allows you to simulate scenarios that will test the lifecycle of your VNFs/Network Services. It allows you to automate creating instances of your services in test environments, simulating traffic or lifecycle processes, whilst monitoring and asserting metric values are as expected.\nEach scenario can be executed independently to confirm a particular aspect or feature of your service is working. The full collection of the scenarios related to a service can also be executed as a test suite, useful in a CI/CD process to re-test the service after a change has been made.\nTo begin using the behaviour testing feature of the Agile Lifecycle Manager (ALM) open one of your Assembly designs and click on the \u0026ldquo;Behaviour Testing\u0026rdquo; tab at the bottom of the design.\nIn this screen you will see the three main aspects of behaviour testing:\n Assembly Configurations - planned Assemblies to use in scenarios Scenarios - test scripts executed to simulate situations to confirm the behaviour of your Assemblies Results - results of each execution of a scenario for analysis  Next Steps Read how to create and manage Assembly Configurations and Scenarios so you can test your Assembly design.\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/resource-engineering/resource-packages/overview/","title":"Overview","tags":[],"description":"","content":"Structure The structure of a resource package is determined by the resource manager that uses it. For Brent, see Brent Resource Packages.\nNext Steps Once you understand the basics of a Resource package, you may move on to understand Drivers and how they are used by Brent to manage the Resource.\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/installation/resource-manager/rm-overview/","title":"Resource Manager Overview","tags":[],"description":"","content":"Introduction The Agile Lifecycle Manager (ALM) provides open integration of external virtual and physical resources to be assembled with others into complete services. While orchestrating Assemblies, LM leverages a set of one or more external Resource Managers (RM) through their northbound APIs to orchestrate the lifecycles of the managed resource instances deployed to Virtual Infrastructure Managers (VIMs). The Resource Managers are responsible for communicating with VIMs to request actions on cloud infrastructure compute, storage and network resources in support of deployed resource instances lifecycles.\nThe role of the Resource Manager As stated, the Lifecycle Manager uses a Resource Manager to communicate and perform lifecycle transitions for resources through a VIM. This allows LM to abstract both the specifics of communicating with a given VIM and the execution of given commands. Thus an RM may exist to cater for a specific VIM type or for a scripting type for example. It is also possible to deploy RMs to cater for specific VIM instances rather than types. This may be done to either manage capacity or indeed for security reasons.\nIt is possible to register any number of RMs with LM though a given RM should only be registered once. An RM is uniquely identified in LM with its name though from a registration perspective it is defined by its URL.\nLM ships with an existing Resource Manager commonly referred to as Brent (see section below). This is automatically onboarded in to LM at installation time. For other RMs, LM needs to be made aware of their existence by onboarding them.\nBrent Brent is a Carrier-grade Resource Manager shipped with ALM and is automatically onboarded into LM by the Kubernetes lm-post-configurator job.\nBrent performs the following functions:\n Resource package management - resource packages are uploaded to Brent (either using the REST API or by using LMCTL) and the types are then imported into ALM when Brent is onboarded or refreshed. Infrastructure Key management - infrastructure keys include SSH keys for securely communicating with running instances (such as VMs) in a infrastructure object. Execution of operations against resource instances running in a Virtualized Infrastructure Manager (VIM) block. Resource drivers onboarded to Brent serve as the mechanism of achieving this by managing infrastructure and running lifecycle operations against them.  Read more about Brent by following the Resource engineering user guide\nResource Descriptors A Resource Descriptor is ultimately present in both LM and the RM. However the RM is considered the master and LM retrieves the descriptors from the RM.\nOn onboarding of an RM, via an action on the UI and as a result of a push of a resource to the RM via LMCTL, LM will refresh its inventory of resources (descriptors) for a given RM keeping its descriptors in sync. This is important to ensure that both the set of properties and the set of Lifecycle Transitions/Operations are matched between the RM and LM.\nRM Installation Considerations An RM must be installed in the same namespace as the LM to which it is being connected. This is to ensure security is maintained with the internal hostname based certificates generated during the install.\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/administration/security/","title":"Security","tags":[],"description":"","content":""},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/reference/behaviour-testing/step-reference/","title":"Step Reference","tags":[],"description":"","content":""},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/administration/configuration/themes/theme-overview/","title":"Themes Overview","tags":[],"description":"","content":"Changing the look of the Agile Lifecycle Manager (ALM) User Interface UI is supported via Theme Packages. A theme can be provided for each unique look that is needed for the ALM UI.\nChanging the Theme of the ALM UI  To change the theme of the UI you need to produce a theme.tar containing the necessary elements. The TAR should include a directory for each potential theme. Each theme is represented by a directory which should include the favicon, stylesheet and images to be used:  +-- mytheme (change to your Theme name)\r| +-- favicon.ico\r| +-- theme.css\r| +-- images (directory)\r| +-- image-file-1.png\r| +-- image-file-2.png\r| +-- \u0026lt;etc. rest of the image files\u0026gt;\r+-- myothertheme (change to any other Theme names)\r+-- favicon.ico\r+-- theme.css\r+-- images (directory)\r+-- \u0026lt;etc. rest of the image files\u0026gt;\rProduce a TAR called theme.tar (this name is mandatory) using the tar command, including each Theme directory:  tar -cvzf theme.tar mytheme myothertheme\rThe theme.tar must be loaded into your Kubernetes environment by creating a ConfigMap.  kubectl create configmap lm-themes --from-file theme.tar\rThe theme can be enabled during ALM installation or for an already running ALM.  "},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/what-is-stratoss/","title":"What is ALM?","tags":[],"description":"","content":"Agile Lifecycle Manager (ALM) is a cloud networking automation platform that embraces the IT DevOps movement and fully automates the operational lifecycle of cloud network services.\nALM enables complex network services to be designed, created and continuously optimized across hybrid and distributed cloud environments. Our revolutionary intent-based automation engine along with the in-built automated test and DevOps tools deliver a 30 times reduction in operational effort. As an \u0026lsquo;automation\u0026rsquo; product, it distinguishes itself by fully automating the day 1 and 2 ongoing maintenance and change management scenarios, rather than just orchestrating the day 0 \u0026lsquo;turn it on\u0026rsquo; scenario.\nALM enables the radical cloud networking automation that is required The advancement of Cloud technologies such as virtualization, 5G, Edge and Distributed Cloud along with the continuous pressure to rapidly bring new services to market at a much lower cost brings a massive increase in complexity. Services that traditionally were very static and hardware-oriented, now consist of thousands of dynamic pieces of software that need to be managed and changed in real-time across a variety of locations, clouds, and networks. The traditional model and workflow-based tools that rely on manual effort to design, test, and maintain these services does not enable commercially viable services.\nThe new world of dynamic and distributed services requires new tools that deliver a much higher degree of automation. The tools should enable a service factory that automate the day 0, 1, and 2 use cases at a fraction of the cost and manual effort. ALM is a unique cloud native product and has been built from scratch over the last 3 years to deliver a much higher degree of automation based on radical design principles:\n Opinionated Model: The product is architected around a product philosophy where all service design configuration is standardized once and then utilized repeatedly across the IT/network landscape. Brutal automation: Rather than the legacy custom and manual workflow, the product ‘imposes’ the design based on the opinionated model. Service-centric Intent: The Intent-based engine automates all service lifecycles for day 0, 1, and 2 use cases without manual programming Vendor-neutrality: The product is designed to reduce vendor lock-in by providing consistent vendor-neutral configurations. DevOps: In-built DevOps and CI/CD tools and processes that enable automated pipelines and change management.  The key concepts of ALM are further explained in this section.\nWhat can you do with ALM ALM automates the operational lifecycle of cloud network services across day 0, 1, and 2 scenarios without almost any manual effort. With ALM you can:\n Onboard VNFs and Network Services with 75% less manual effort Rapidly design VNF and Network Services without having to create any workflow for day 0, 1, and 2 scenarios: all you need to do is to select the service components and declare the relationships and dependencies Design service-chains that span multiple domains such as SDN, physical, and virtual cloud environments including the infrastructure itself Automate behaviour driven testing for VNF and Network Services to ensure services have less errors once they go in production Deploy and manage services across multi-cloud and multi-location Execute day 1 and 2 lifecycle actions automatically by the Intent Engine that automatically brings services from where they are now, to the intended state without manual programming Monitor services that are in production and auto-heal and scale Apply upgrade, migrate or changes to the shape of the service automatically without any manual workflow Create CI/CD pipelines to automate the design, testing, and change management scenarios of VNF and Network Services  Watch ALM in action  Title: ALM 2.0 Overview Target audience: Sales, pre-sales, engineers, product management Content: Background, value proposition, what\u0026rsquo;s new in 2.0, live demonstration of key features Duration: 21 minutes and 52 seconds    "},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/reference/descriptor-specification/assembly-descriptor/","title":"Assembly Descriptor","tags":[],"description":"","content":"Introduction This document describes the assembly descriptors that are used by the Agile Lifecycle Manager (ALM). ALM needs to have descriptions of the building blocks of applications that it is going to manage. The basic building blocks are described in resource descriptors (resource descriptors are described in separate document). Sets of these are composed into assembly descriptors to allow designers to describe a complete application/service that they need LM to manage.\nWithin the assembly will be a description of the relationships between resources that allow configuration to be applied to the actual instances of the components that LM will manage. Assemblies may also reference assemblies and existing infrastructure items, such as network instantiated outside of LM.\nNaming The assembly descriptor name field will contain the following string:\nassembly::name::1.0\nThe name must start with a letter (either case) and can include letter and numbers and underscore and hyphens (minus sign). The name must not contain spaces. It must end with either a letter (either case) or a number.\nBoth name and version are mandatory.\nSections Header The header includes the name and the description of the descriptor.\nname: assembly::Streamer_cluster::1.0\rdescription: An Assembly for a front end cluster comprising of a loadbalancer supported by an authorisation proxy and video streamers using a shared NFS based storage\rProperties (The properties section occurs in several places. The rules defined here apply to the top-level property section for the descriptor. Rules applicable to property names and the “value” field can be applied to all property sections)\nThis section contains the properties that belong to assembly descriptors. These include the full set of properties that are required to orchestrate them through to the Active state. These can be understood as the context for the management of the item during its lifecycle.\nproperties:\rdeploymentLocation: # the name of the property ​ type: string\r​ required: true\r​ description: The name of the openstack project (tenant) to install this assembly in. resourceManager:\r​ value: '${resourceManager}'\rnumOfStreamers:\r​ type: string\r​ description: the number of streamers that should be created at install time\r​ default: 2\rtenant_key_name:\r​ type: string\r​ required: true\r​ description: The SSH key for the current tenant\rflavor:\r​ value: m1.small\rcluster_public_ip_address:\r​ type: string\r​ description: the public IP address for this cluster\r​ read-only: true\r​ value: '${balancer.publicIp}'\rEach property name must be unique within its property section. The name cannot contain dot (period) characters.\nCurrently, the type field is not used, and all properties are assumed to be of type “string”. This field will be used in the future to allow handling of different types of data (for example: dates, IP addresses, encrypted values, etc…). It is recommended users omit this field or use the default value of “string” to avoid compatibility issues in the future.\nProperties are optional unless explicitly defined as required by the inclusion of a required: true flag. This only affects the top-level assembly and means that a value must be present (for example; not null) for a property. This can be evaluated from the “value” field, a “default” or passed in from the intent request.\nProperties marked as read-only: true will not be overridden by values mapped in from an enclosing assembly or from the intent request. This is typically used for properties that are calculated from or returned by the resource itself.\nProperties may be declared with a default value or a specific value or neither. Where the value field is used it may either be an explicit value or it may reference to another property within the descriptor. This will happen in assemblies where properties given to the assembly may be used within the various other property sections. When referencing a property in the assemblies’ main property section the reference will look as follows:\nvalue: ‘${max_connections}’\nIf the reference is to a property within the assemblies other sections the reference must include the name of the enclosing object, for example\nvalue: ‘${balancer.publicIp}’\nThis references the property publicIp within the balancer section of the composition section.\ndeploymentLocation is a special property that is used by the LM to place the resultant resource in the correct location. It will only appear in an assembly descriptor. The contents of the property will be specific to the resource manager that handles the resource.\nresourceManager is another special property that passes the name of the resource manager instance that will be used to manage the resource.\nLM will assign an internal name and identifier for each resource and assembly instance it creates. These values can be useful to give unique names for servers etc. To access them a property may have its value set to ${instance.name} or ${instance.id}.\nCapabilities and Requirements These two sections allow designers to explain what functions the assemblies are implementing or need before they can work successfully. These might be expressing that networks or various types must be available for the resource instances to work or it may be describing that a resource supports, for example, incoming http requests.\nThe type is a string that expresses the capability or requirement. The values in these strings will have to be agreed across an organization and where possible they should be agreed by the industry. Resource capabilities should use common industry terms. In the examples below the idea is that httpStreamOutput indicates that the capability is using the http protocol in a stream form and in an output direction. The OS::Neutron:Net is the resource type from openstack associated with a network instantiated within neutron.\nCapabilities These are used to enable service designers to understand what function an assembly provides.\ncapabilities:\r​ VideoStream:\r​ type: httpStreamOutput\rcapabilities:\r​ Network: ​ type: neutronNetwork\rRequirements Similar to the capabilities section, the requirements contain the list of capabilities that the assembly needs to be provided for them to work.\nrequirements:\r​ VideoNetwork:\r​ type: neutronNetwork\r​ ManagementNetwork:\r​ type: neutronNetwork\r​ RemoteNFSMountPoint:\r​ type: nfsExportMountpoint\rOperations This section defines sets operations that can be called to enable relationships to be created between resources and/or assemblies. Where an assembly/resource descriptor describes an operation, the enclosing assembly may expose this by referencing the lower level operation. In effect it can promote an operation from a contained assembly/resource.\noperations:\r​ SetLBBalancer:\r​ source-operation: balancer.AddHttpStreamOutput\rComposition and References Definition:Components - A component is a resource or assembly that is included within an assembly composition section and will be instantiated as a result of requesting a new instance of the enclosing assembly\nAssemblies allow a designer to group a set of resources and assemblies, collectively known as components, into an assembly to create a new application/service. Those used within the composition section will be instantiated and managed by LM.\nWhen LM has already instantiated an assembly it is possible for another assembly to share the instance by referencing it within the references section. The references section can also refer to existing objects that may have been created outside LM. LM will resolve both of these types of references from the properties supplied and access to the instances properties and operations is then available to the referencing assembly.\nComposition Assemblies gather resources and other assemblies together for either a whole or part of a solution. The composition section is used to reference the components that will be instantiated as part of the Installation of the assembly.\ncomposition:\rstreamer: # The name\r​ type: resource::c_streamer::1.0\r​ quantity: ${numOfStreamers}\r​ properties:\r​ \\#not shown for brevity\rbalancer:\r​ type: resource::c_balancer::1.0\r​ quantity: 1\r​ properties:\r​ \\#not shown for brevity\rnet_video:\r​ type: resource::net_video::1.0\r​ quantity: 1\r​ properties:\r​ \\#not shown for brevity\rEach entry in this section must give a name to the item which will form the basis for the instance name for the actual running components. It also includes a quantity that defaults to 1. In a non-clustered environment, the “quantity” property defines exactly how many instances will be created.\nThe rules governing properties are defined properties section\ncomposition:\rstreamer:\r​ type: resource::c_streamer::1.0\r​ cluster:\r​ \\# not shown for brevity\r​ properties:\r​ deploymentLocation:\r​ value: '${deploymentLocation}'\r​ resourceManager:\r​ value: '${resourceManager}'\r​ flavor:\r​ value: m1.small\r​ server_name:\r​ value: ${instance.name}\r​ referenced-video-network:\r​ value: ${net_video.network-id}\r​ availability_zone:\r​ value: DMZ\r​ mgmtIp:\r​ type: string\r​ description: MGMT IpAddress of server\r​ read-only: true Clusters It is also possible to define a cluster section for a component that indicates that the component of the assembly may comprise of more than one instance of the same type (node) to support capacity and or availability requirements.\n initial-quantity is an optional property that must be between the minimum and maximum nodes values. If not set, it defaults to “minimum-nodes”. minimum-nodes is optional and defaults to “1” if not set. It can be set to \u0026lsquo;0\u0026rsquo; if no instances of the component are required at initial install. maximum-nodes is optional and if set it must be greater than or equal to “minimum-nodes”. scaling-increment is optional and defaults to “1”, it determines the number of instances added or removed from the cluster during scaling.  composition:\rstreamer:\r​ type: resource:: c_streamer::1.0\r​ cluster:\r​ initial-quantity: ${numOfServers} ​ minimum-nodes: 1 ​ maximum-nodes: 4 ​ scaling-increment: 1 ​ properties:\r​ data:\r​ value: ${data}\r​ ip_address:\r​ read-only: true\rReferences The reference section is similar to the composition section except that the items referenced in this section must be pre-existing before LM will instantiate any of the items in the assembly’s composition section.\nTwo types of references can be resolved by LM\n existing assembly instances external resources that are managed directly by a resource manager.  Assembly references require the full name of the assembly within type field. The example below shows using the semantic versioning to allow more flexibility when resolving to instances of the assembly. The properties are used to help LM find the instance of the item required by the current assembly. With items that have been created through LM the referencing assembly can refer to any of the instance’s properties from the items property section. Referenced assemblies can be used by the enclosing assembly to establish relationships.\nResource instances managed directly by a resource manager may be referenced. These will have resource descriptors as any resource, however they will not include the Install or Uninstall lifecycle steps.\nTo read the references section each item has a local name used to refer to the item with in the assembly. The type directs LM to fetch the required resource type. The properties are then used by LM to narrow down to a single instance of the resource type that can be used by the enclosing assembly. If LM finds more than one resource that fits the information provided an error occurs and the assembly will not be instantiated.\nreferences:\rstorage: # reference to an existing assembly instance\r​ type: assembly::storage_cluster::^1.0\r​ properties:\r​ deploymentLocation:\r​ value: '${deploymentLocation}'\r​ resourceManager:\r​ value: '${resourceManager}'\r​ name:\r​ value: '${storage-name}'\rmanagement-network: # reference to a neutron network not created by the Agile Lifecycle Manager (ALM) ​ type: resource::ucd_network::1.0\r​ properties:\r​ deploymentLocation:\r​ value: '${deploymentLocation}' ​ resourceManager:\r​ value: '${resourceManager}' ​ name: ​ value: ${management-network-name} Once found the properties of these referenced items may be accessed using the following method:\n${referenced-item-name.property-name}\n balancer:\r​ type: 'resource::c_balancer::1.0'\r​ quantity: '1'\r​ properties:\r​ …\r​ referenced-management-network:\r​ value: '${management-network.id}'\rAll the properties from the assembly instances referenced are available for use in the above manner. For resources the properties available will be defined in the resource descriptor.\nRelationships Relationships are established between two components that enable the “requirements” of one component (known as the “target”) to be satisfied by another component providing the “capability” (known as the “source”).\nDefining Relationships The “source” and “target” of a component is defined in the following fields respectively:\n source-capabilities target-requirements  In order to define a relationship between two components, the name of each component (as defined in the “composition” or “reference” section of the descriptor) is combined with the name of the capability or requirement accordingly.\nFor example:\nsource-capabilities:\r​ - A.capability-3\rtarget-requirements:\r​ - B.requirement-3\rsource-capabilities is divided up as follows:\n A -\u0026gt; Is from composition section “.” -\u0026gt; Is a delimiter agreed upon capability-3 -\u0026gt; Is the name of the “capability” defined within the organization.\rA reference component can only be defined as a source-capability. In this instance, only the name of the reference needs to be provided.\nWithin a relationships definition the “properties” field may refer to the components defined under the “source-capability” and “target-requirements” fields as “source” and “target” respectively.\nFor example:\n property1:\r​ value: ${source.name}\r​ property2:r\r​ value: ${target.name}\rAbove ${source.name} and ${target.name} is used to refer to the “source” components (as defined in source-capabilities) “name” property and “target” components (as defined in target-requirements) “name” property accordingly.\nThe “lifecycle” section within relationships consist of two transitions:\n Create Cease  The above transitions allow designers to specify what operations to perform during the Creation and Cessation (or removal) of a relationship for a source and target component.\nFor example:\n lifecycle:\r​ Cease:\r​ - target.CeaseRelationship\r​ - source.CeaseRelationship\r​ Create:\r​ - target.CreateRelationship\r​ - source.CreateRelationship\rThe operation(s) called will depend on the components involved. The operations are called in the order they appear in the Create or Cease sections. A relationship may only call one operation, i.e. only a target or source operation.\nOperations are referenced as:\n source.\u0026lt;operation-name\u0026gt; target.\u0026lt;operation-name\u0026gt;  \u0026lt;operation-name\u0026gt; refers to an operation defined in the assembly or resource descriptor associated with the component.\nEstablishing Relationships Relationships are created when the components to be related are in particular states.\nThe following fields are used to define the state required to establish a relationship:\n source-state target-state  For example, if the following source and target states are defined as:\n source-state: Active\r​ target-state: Active\rBy default this would mean that the relationship would be created when the source is in the Active state and before the target has transitioned to the Active state.\nFurther control when defining relationships is available via the “source-state-modifier” and “target-state-modifier” fields. These are used to define whether relationships are established before (pre) or after (post) they reach their source or target state as defined under source-state and target-state previously.\nFor example:\n source-state-modifier – if not present default is post target-state-modifier – if not present default is: pre  Relationships are always ceased/removed before the associated component leaves the state defined in the source-state and target-state fields mentioned above.\nrelationships: nfs_mount:\r​ source-capabilities: ​ - storage.NFSMountpoint\r​ target-requirements: ​ - streamer.RemoteNFSMountPoint\r​ source-state: Active\r​ target-state: Inactive\r​ properties:\r​ remote_nfs_port:\r​ value: '2049'\r​ remote_nfs_server_ip:\r​ value: '${source.privateIp}'\r​ remote_mount_point:\r​ value: '/'\r​ local_mount_point:\r​ value: '/mnt' ​ lifecycle:\r​ Create:\r​ - source.MountStorage\r​ Cease:\r​ - source.UnmountStorage\rLike the overall assembly and resources relationships have a set of properties that are available to the operations associated with the lifecycle transitions of the relationship.\nPlacement To deploy components to the correct location LM will use two properties called deploymentLocation and resourceManager. The resourceManager property will be used to find the correct Resource Manager that manages the resource for the location defined in the deploymentLocation property. The combination of these two uniquely identifies where and how a resource will be managed.\nA Placement is also involved when trying to resolve the instances defined in the references section. Before a reference can be resolved any associated placement rules will have been applied. This will then allow LM to find the appropriate instance of the reference required. The two properties will also be needed on each reference.\nMetrics and Policies A resource descriptor may indicate that the underlying resource will emit one or more metrics.\nExample metrics as found in a resource descriptor.\nmetrics:\r​ lb_integrity: ​ type: metric::integrity\r​ publication-period: ${integrity_publication_period}\r​ lb_load:\r​ type: metric::load ​ publication-period: ${publication_period}\rLoad metrics can be promoted in an assembly using a similar mechanism to operations:\nmetrics:\r​ b1_load: ​ source-metric: B1.load\rWithin an assembly the policy section will contain details of the policies for the underlying resources load metric and how that should be used to mange the scaling of components.\nEach policy has a name, the associated metric, an action and a set of properties that are used to handle the policy.\nExample Policies policies: scaleStreamer: ​ type: policy::scale\r​ metric: A.load ​ target: B\r​ properties:\r​ scaleOut_threshold: ${scaleOut_threshold}\r​ scaleIn_threshold: ${scaleIn_threshold}\r​ smoothing: ${scale_smoothing} The example above shows the policy associated with the load metric on a resource. This is used to ScaleOut and ScaleIn a component – indicated by the value in the target properties.\nThe example shows that the metric produced by A called load will be used to indicate when the target – B- will be scaled. Load is expressed as a percentage and the thresholds are simple integers. When the threshold is broken the scale event associated with the threshold will be enacted. To prevent this happening each time the load spikes, a smoothing value is applied. The threshold must be breached at least the number of times indicated by the smoothing value before the action will be enacted.\nExample of smoothing:\nYaml Examples Example of an assembly with policies:\nname: assembly::h_bta::1.0\rdescription: Basic Test Assembly properties:\rdata:\r​ default: \u0026quot;data\u0026quot;\r​ type: string\r​ description: 'parameter passed'\rnumOfServers:\r​ description: number of servers\r​ type: integer\r​ default: 1\routput:\r​ description: an example output parameter\r​ type: string\r​ read-only: true\r​ value: ${B.output}\rdeploymentLocation:\r​ type: string\r​ description: name of openstack project to deploy network\r​ default: admin@local\rresourceManager:\r​ type: string\r​ description: name of the resource manager\r​ default: test-rm\rscaleOut_threshold:\r​ type: integer ​ description: threshold that the load metric must breach to potentially trigger a scaleOut\r​ default: 90\rscaleIn_threshold:\r​ type: integer ​ description: threshold that the load metric must breach to potentially trigger a scaleOut\r​ default: 10\rscale_smoothing:\r​ type: integer ​ description: the number of sequential periods the load metric must be above threshold to trigger action\r​ default: 4\rcomposition:\rA:\r​ type: resource::h_simple::1.0\r​ quantity: '1'\r​ properties:\r​ referenced-internal-network:\r​ value: ${internal-network.id}\r​ reference-public-network:\r​ value: ${public-network.id}\r​ image:\r​ value: ${xenial-image.id}\r​ key_name:\r​ value: \u0026quot;ACCANTO_TEST_KEY\u0026quot;\r​ data:\r​ value: ${data}\r​ output:\r​ value: \u0026quot;A_output\u0026quot; ​ deploymentLocation:\r​ value: ${deploymentLocation}\r​ resourceManager:\r​ value: ${resourceManager}\rB:\r​ type: resource::t_simple::1.0\r​ cluster :\r​ initial-quantity: ${numOfServers}\r​ minimum-nodes: 1\r​ maximum-nodes: 4\r​ scaling-increment: 1\r​ properties:\r​ referenced-internal-network:\r​ value: ${internal-network.id}\r​ reference-public-network:\r​ value: ${public-network.id}\r​ image:\r​ value: ${xenial-image.id}\r​ key_name:\r​ value: \u0026quot;ACCANTO_TEST_KEY\u0026quot;\r​ data:\r​ value: ${data}\r​ output:\r​ value: ${A.output}\r​ deploymentLocation:\r​ value: ${deploymentLocation}\r​ resourceManager:\r​ value: ${resourceManager}\rpolicies: scaleStreamer: ​ type: policy::scale\r​ metric: A.load ​ target: B ​ properties:\r​ scaleOut_threshold: ${scaleOut_threshold}\r​ scaleIn_threshold: ${scaleIn_threshold}\r​ smoothing: ${scale_smoothing} references:\rinternal-network:\r​ type: resource::openstack_neutron_network::1.0\r​ properties:\r​ deploymentLocation:\r​ value: ${deploymentLocation}\r​ resourceManager:\r​ value: ${resourceManager}\r​ name:\r​ type: string\r​ value: VIDEO\rpublic-network:\r​ type: resource::openstack_neutron_network::1.0\r​ properties:\r​ deploymentLocation:\r​ value: ${deploymentLocation}\r​ resourceManager:\r​ value: ${resourceManager}\r​ name:\r​ type: string\r​ value: public\rxenial-image:\r​ type: resource::openstack_glance_image::1.0\r​ properties:\r​ deploymentLocation:\r​ value: ${deploymentLocation}\r​ resourceManager:\r​ value: ${resourceManager}\r​ name:\r​ type: string\r​ value: xenial\rrelationships:\rthird-relationship:\r​ source-capabilities:\r​ - A.capability-3\r​ target-requirements:\r​ - B.requirement-3\r​ source-state: Active\r​ target-state: Active\r​ properties:\r​ source:\r​ value: ${source.name}\r​ target:\r​ value: ${target.name}\r​ lifecycle:\r​ Cease:\r​ - target.CeaseRelationship3\r​ - source.CeaseRelationship3\r​ Create:\r​ - target.CreateRelationship3\r​ - source.CreateRelationship3\rThe following is an assembly that will create a set of video streamers and link them to a load balancer which is also created. It requires the name of a storage assembly to be provided so that it can share the video content between the streamers.\nname: assembly::Streamer_cluster::1.0\rdescription: An Assembly for a front end cluster comprising of a loadbalancer supported by an authorisation proxy and video streamers using a shared NFS based storage\rproperties:\rdeploymentLocation:\r​ type: string\r​ required: true\r​ description: The location as required by the resource manager. resourceManager:\r​ type: string\r​ required: true\r​ description: The name of the resource resource manager. numOfStreamers:\r​ type: string\r​ description: the number of streamers that should be created at install time\r​ default: 2\rtenant_key_name:\r​ type: string\r​ required: true\r​ description: The SSH key for the current tenant management-network-name:\r​ type: uuid\r​ required: true ​ description: the name of the management network in the tenant where the assembly is to be installed public-network-name:\r​ type: uuid\r​ required: true ​ description: the name of the public network associated with the tenant where the assembly is to be installed\rmax_connections:\r​ type: string\r​ description: Maximum connections for the balanced server\r​ default: '3' cluster_public_ip_address:\r​ type: string\r​ description: the public IP address for this cluster\r​ read-only: true\r​ value: '${balancer.publicIp}'\rscaleout-threshold:\r​ type: string\r​ description: the load value that when exceed will cause a scale out to be invoked\r​ default: 80\rscalein-threshold:\r​ type: string\r​ description: the level of load that will cause a scale in to be invoked\r​ default: 10\rcomposition:\rstreamer:\r​ type: resource::c_streamer::1.0\r​ cluster:\r​ initial-quantity: ${numOfStreamers} ​ minimum-nodes: 2\r​ maximum-nodes: 10\r​ scaling-increment: 2\r​ properties:\r​ deploymentLocation:\r​ value: '${deploymentLocation}'\r​ resourceManager:\r​ value: '${resourceManager}'\r​ key_name:\r​ value: '${tenant_key_name}'\r​ referenced-management-network:\r​ value: '${management-network.id}' ​ flavor:\r​ value: m1.small\r​ server_name:\r​ value: '${instance.name}'\r​ referenced-video-network:\r​ value: '${net_video.network-id}'\r​ availability_zone:\r​ value: DMZ\r​ integrity_publication_period:\r​ value: 120\r​ number-of-intervals:\r​ value: 4\rbalancer:\r​ type: 'resource::c_balancer::1.0'\r​ quantity: 1\r​ properties:\r​ deploymentLocation:\r​ value: '${deploymentLocation}'\r​ resourceManager:\r​ value: '${resourceManager}'\r​ key_name:\r​ value: '${tenant_key_name}'\r​ referenced-management-network:\r​ value: '${management-network.id}'\r​ referenced-internal-network:\r​ value: '${net_video.network-id}'\r​ referenced-public-network:\r​ value: '${public-network.id}'\r​ flavor:\r​ value: m1.large\r​ server_name:\r​ value: '${instance.name}'\r​ availability_zone:\r​ value: DMZ\r​ integrity_publication_period:\r​ value: 120\r​ number-of-intervals:\r​ value: 4\rnet_video:\r​ type: resource::net_video::1.0\r​ quantity: 1\r​ properties:\r​ deploymentLocation:\r​ value: '${deploymentLocation}'\r​ resourceManager:\r​ value: '${resourceManager}'\r​ subnetCIDR:\r​ type: string\r​ description: (Required)\r​ default: '10.0.1.0/24'\r​ networkName:\r​ type: string\r​ description: Network Name\r​ default: VIDEO\r​ subnetDefGwIp:\r​ type: string\r​ description: Default Gateway IP address\r​ default: '10.0.1.1' references:\rmanagement-network:\r​ type: resource::urbancode-network::1.0\r​ properties:\r​ deploymentLocation:\r​ value: '${deploymentLocation}'\r​ resourceManager:\r​ value: '${resourceManager}'\r​ name: ​ value: ${management-network-name} public-network:\r​ type: resource::urbancode-network::1.0\r​ properties:\r​ deploymentLocation:\r​ value: '${deploymentLocation}'\r​ resourceManager:\r​ value: '${resourceManager}'\r​ name: ​ value: ${public-network-name} capabilities: HttpStream:\r​ type: httpStream\rrelationships: uses-net_video:\r​ source-capabilities: ​ - net_video.Network\r​ target-requirements: ​ - streamer.VideoNetwork\r​ - storage.VideoNetwork\r​ - balancer.VideoNetwork\r​ source-state: Active\r​ target-state: Inactive\ruses-management-network:\r​ source-capabilities: ​ - management-network\r​ target-requirements: ​ - streamer.ManagementNetwork\r​ - storage.ManagementNetwork\r​ - balancer.ManagementNetwork\r​ source-state: Active\r​ target-state: Inactive\rbalancer-uses-public-network:\r​ source-capabilities: ​ - public-network\r​ target-requirements:\r​ - balancer.PublicNetwork\r​ source-state: Active\r​ target-state: Inactive\rbalanceStreamer:\r​ source-capabilities: ​ - streamer.VideoStream\r​ target-requirements: ​ - balancer.HttpServer\r​ source-state: Active\r​ target-state: Active\r​ properties:\r​ max_connections:\r​ value: '${max_connections}'\r​ server_ip:\r​ value: '${source.privateIp}'\r​ server_port:\r​ value: '8080' ​ lifecycle:\r​ Create:\r​ - balancer.AddBalancedHttpServer\r​ Cease:\r​ - balancer.RemoveBalancedHttpServer\r"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/reference/behaviour-testing/step-reference/assembly-events/","title":"Assembly Events","tags":[],"description":"","content":"Check for successful process Description Monitors Kafka for a message describing the success of a given process for a given Assembly. For example, this step could be used to check a scale occurred on an Assembly when simulating traffic that triggers a scale policy.\nThe step begins monitoring Kafka as the step is executed, so any processes completed before then will not be seen.\nPasses when:\n a message is seen which describes the success of the given process for the given Assembly  Fails when:\n no message is seen before a configurable amount of time has passed (configured with alm.doki.execution.lifecycleEventTimeout. Default = 900s)  Properties    Property Description     processType Type of process to check has succeeded: Heal Assembly, Scale Out Assembly, Scale In Assembly, Create Assembly, Delete Assembly, Upgrade Assembly, Change Assembly State   assemblyName The name of the Assembly the expected process is for. This Assembly may have been created by the scenario or existed previously. If using an \u0026ldquo;Existing Provided\u0026rdquo; assembly configuration then you may use the reference name you chose when adding it to the scenario    Check for in progress process Description Monitors Kafka for a message describing a given process is in progress for a given Assembly. For example, this step could be used to check a scale has started on an Assembly when simulating traffic that triggers a scale policy.\nThe step begins monitoring Kafka as the step is executed, so any processes started before then will not be seen.\nPasses when:\n a message is seen which specifies the given process has started for the given Assembly  Fails when:\n no message is seen before a configurable amount of time has passed (configured with alm.doki.execution.lifecycleEventTimeout. Default = 900s)  Properties    Property Description     processType Type of process to check has started: Heal Assembly, Scale Out Assembly, Scale In Assembly, Create Assembly, Delete Assembly, Upgrade Assembly, Change Assembly State   assemblyName The name of the Assembly the expected process is for. This Assembly may have been created by the scenario or existed previously. If using an \u0026ldquo;Existing Provided\u0026rdquo; assembly configuration then you may use the reference name you chose when adding it to the scenario    Check for change state process Description An extension of the \u0026ldquo;Check for successful process\u0026rdquo; step for the change state process, which allows you to specify the expected target state.\nMonitors Kafka for a message describing the success of a change state process for a given Assembly.\nThe step begins monitoring Kafka as the step is executed, so any processes completed before then will not be seen.\nPasses when:\n a message is seen which describes the success of the given change state for the given Assembly, which changed the state to the expected value.  Fails when:\n no message is seen before a configurable amount of time has passed (configured with alm.doki.execution.lifecycleEventTimeout. Default = 900s)  Properties    Property Description     assemblyName The name of the Assembly the expected process is for. This Assembly may have been created by the scenario or existed previously. If using an \u0026ldquo;Existing Provided\u0026rdquo; assembly configuration then you may use the reference name you chose when adding it to the scenario   targetState Expected target state the process should be moving the Assembly to: Installed, Inactive, Active    "},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/administration/security/getting-started/","title":"Getting Started","tags":[],"description":"","content":"The default installation of Agile Lifecycle Manager (ALM) includes a secure application with protected access and a set of preconfigured roles. It also includes predefined users who are able to perform a variety of the key system roles. These predefined users are suitable for either basic usage of the system in a demonstration capacity, or can act as a template for understanding how to configure users in a more permanent installation.\nSecurity User access in LM is based on 5 principles: Privilege, Roles, Groups, Users and Clients.\nPrivileges A privilege grants permissions to a user to perform an operation in the system. The set of allowed privileges are built into LM, read more information about them at: Available Privileges\nRoles A role is a collection of privileges that represent a responsibility in the system. For example, an \u0026ldquo;Administrator\u0026rdquo; role could be created with all possible privileges.\nEach role and it\u0026rsquo;s assigned groups or clients are managed in the configuration for the LM installation.\n Default Roles. Configure Roles  Groups A group is a collection of users with similar responsibilities. Every role configured on an LM installation is assigned a list of the groups that make use of it (a group may be assigned to more than one role). This results in assigning that role to every user in the group, granting them the privileges of that role.\nLM reads groups from a (potentially external) LDAP server.\n Default Groups Manage Groups  Users A user is a person with access to the system. Their access credentials may be used to make API requests from a valid client and login to the user interface of LM. The actions allowed by a user are dictated by the groups they are a part of.\nLM reads users from a (potentially external) LDAP server.\n Default Users Manage Users  Clients A client is a system allowed to make requests on LM. Each client is assigned a given grant type, which controls the type of authentication requests it may make and potential roles it is assigned.\nFor example a client may be created with roles, allowing it to authenticate with just it\u0026rsquo;s own credentials. Alternatively, a client may be assigned the password grant type, so it may only authenticate with it\u0026rsquo;s credentials in combination with a valid username/password, assuming the roles of the user (the LM user interface is a good example of this, as it allows users to login with their username/password, combines this with it\u0026rsquo;s own set of credentials to authenticate as the given user)\nEven with username/password credentials, the id and secret of a valid client must be included on all authentication requests.\n Default Client Credentials Manage Client Credentials Making authenticated requests  "},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/installation/installation/","title":"Installation","tags":[],"description":"","content":"Pre-requisites  Read What is Agile Lifecycle Manager (ALM) and watch ALM in action Understand the Key Concepts of ALM If installing the CI/CD Hub then be familiar with the concepts of a CI/CD in the context of ALM  Choose your install path Read the Deployment Model and determine which of the following install paths apply to you:\n  Install CI/CD Hub\n  Install ALM\n  Install Resource Manager\n  Demonstrations There are several pre-built demos that you can use to learn or demonstrate ALM. These can be found in the Demonstrations section.\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/installation/cicdhub/offline/","title":"Offline","tags":[],"description":"","content":"The following guide explains how to pre-prepare the artifacts required by the CI/CD Hub during installation so you may complete installation at a later date without internet access.\nPreparing Offline Install Helm Charts Pre-download the CI/CD Hub Helm chart from the releases page on GitHub.\nDocker Images Create a workspace mkdir cicdhub-docker-images\rIdentify requires images Below is a full list of the docker images used by the sub-charts in v2.0.5 of the CI/CD Hub helm chart:\n# Gogs\rdocker pull gogs/gogs:0.11.79\rdocker pull postgres:9.6.2\r# Jenkins\rdocker pull jenkins/jenkins:2.182\rdocker pull jenkins/jnlp-slave:3.27-1\r# Openldap\rdocker pull osixia/openldap:1.2.1\r# Nginx Ingress\rdocker pull quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.20.0\rdocker pull k8s.gcr.io/defaultbackend:1.4\r# Nexus\rdocker pull quay.io/travelaudience/docker-nexus:3.15.2\rThe following are included in the Helm charts but are for disabled features, so are only required if you intend to enable additional elements of the charts (at your discretion).\n# Jenkins - optional\rdocker pull nuvo/kube-tasks:0.1.2\rdocker pull shadwell/k8s-sidecar:0.0.2\rdocker pull alpine:3.7\r# Gogs - optional\rdocker pull wrouesnel/postgres_exporter:v0.1.1\r# Nexus - optional\rdocker pull quay.io/travelaudience/docker-nexus-backup:1.4.0\rdocker pull quay.io/travelaudience/docker-nexus-proxy:2.4.0_8u191\rThis list was obtained by:\n extracting the CI/CD Hub Helm chart to access the sub-charts: tar -xvzf \u0026lt;your-cicdhub-chart\u0026gt; using helm inspect values on each sub-chart found in extracted cicdhub/charts (be sure to extract each chart and check for sub-charts) looking for any docker image related settings such as image or imageName  Pull Images Pull each image, identified above, with docker:\ndocker pull \u0026lt;image\u0026gt;\rPull LMCTL Jnlp Image You will also need to pull the LMCTL JNLP slave docker image. Select the version from Docker Hub and pull the image:\ndocker pull accanto/lmctl-jnlp-slave:2.0.6\rSave Images Save each image into a tarball with docker:\ndocker save \u0026lt;image\u0026gt; -o cicdhub-docker-images/\u0026lt;image\u0026gt;.tar\rFull list of images pulled in previous steps for convenience:\n# Gogs\rdocker save gogs/gogs:0.11.79 -o cicdhub-docker-images/gogs-0.11.79.tar\rdocker save postgres:9.6.2 -o cicdhub-docker-images/postgres-9.6.2.tar\r# Jenkins\rdocker save jenkins/jenkins:2.182 -o cicdhub-docker-images/jenkins-2.182.tar\rdocker save jenkins/jnlp-slave:3.27-1 -o cicdhub-docker-images/jnlp-slave-3.27-1.tar\r# Openldap\rdocker save osixia/openldap:1.2.1 -o cicdhub-docker-images/openldap-1.2.1.tar\r# Nginx Ingress\rdocker save quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.20.0 -o cicdhub-docker-images/nginx-ingress-controller-0.20.0.tar\rdocker save k8s.gcr.io/defaultbackend:1.4 -o cicdhub-docker-images/defaultbackend-1.4.tar\r# Nexus\rdocker save quay.io/travelaudience/docker-nexus:3.15.2 -o cicdhub-docker-images/docker-nexus-3.15.2.tar\r# Lmctl\rdocker save accanto/lmctl-jnlp-slave:2.0.6 -o cicdhub-docker-images/lmctl-jnlp-slave-2.0.6.tar\r# Jenkins - optional\rdocker save nuvo/kube-tasks:0.1.2 -o cicdhub-docker-images/kube-tasks-0.1.2.tar\rdocker save shadwell/k8s-sidecar:0.0.2 -o cicdhub-docker-images/k8s-sidecar-0.0.2.tar\rdocker save alpine:3.7 -o cicdhub-docker-images/alpine-3.7.tar\r# Gogs - optional\rdocker save wrouesnel/postgres_exporter:v0.1.1 -o cicdhub-docker-images/postgres_exporter-v0.1.1.tar\r# Nexus - optional\rdocker save quay.io/travelaudience/docker-nexus-backup:1.4.0 -o cicdhub-docker-images/docker-nexus-backup-1.4.0.tar\rdocker save quay.io/travelaudience/docker-nexus-proxy:2.4.0_8u191 -o cicdhub-docker-images/docker-nexus-proxy-2.4.0_8u191.tar\rBuild a single archive Create a single archive of your cicdhub-docker-images directory:\ntar -cvzf cicdhub-docker-images.tgz cicdhub-docker-images/\rThis single archive can now be transferred to the target install environment to complete an offline installation.\nPerforming Offline Install Helm Charts Copy the previously downloaded CI/CD Hub Helm chart to the machine you intend to run the installation from.\nDocker Images Transfer Archive Copy the archive to the machine you intend to install the CI/CD Hub on\nExtract Archive Extract the archive to access the images inside:\ntar -xvzf cicdhub-docker-images.tgz\rLoad Images Load each image tarball with docker:\ndocker load \u0026lt;image\u0026gt; -i cicdhub-docker-images/\u0026lt;image\u0026gt;.tar\rFull list of images pulled in previous steps for convenience:\n# Gogs\rdocker load -i cicdhub-docker-images/gogs-0.11.79.tar\rdocker load -i cicdhub-docker-images/postgres-9.6.2.tar\r# Jenkins\rdocker load -i cicdhub-docker-images/jenkins-2.182.tar\rdocker load -i cicdhub-docker-images/jnlp-slave-3.27-1.tar\r# Openldap\rdocker load -i cicdhub-docker-images/openldap-1.2.1.tar\r# Nginx Ingress\rdocker load -i cicdhub-docker-images/nginx-ingress-controller-0.20.0.tar\rdocker load -i cicdhub-docker-images/defaultbackend-1.4.tar\r# Nexus\rdocker load -i cicdhub-docker-images/docker-nexus-3.15.2.tar\r# Lmctl\rdocker load -i cicdhub-docker-images/lmctl-jnlp-slave-2.0.6.tar\r# Jenkins - optional\rdocker load -i cicdhub-docker-images/kube-tasks-0.1.2.tar\rdocker load -i cicdhub-docker-images/k8s-sidecar-0.0.2.tar\rdocker load -i cicdhub-docker-images/alpine-3.7.tar\r# Gogs - optional\rdocker load -i cicdhub-docker-images/postgres_exporter-v0.1.1.tar\r# Nexus - optional\rdocker load -i cicdhub-docker-images/docker-nexus-backup-1.4.0.tar\rdocker load -i cicdhub-docker-images/docker-nexus-proxy-2.4.0_8u191.tar\rThe images are now available on the local docker system. As each service is installed, it will search the local system and find the image it needs, therefore it will not attempt to pull externally.\nPrepare Jenkins installation Open-source Jenkins chart used for CI/CD Hub would require access to the Internet for pulling Docker image and pre-installing plugins.\nTo move forward with offline Jenkins, the following needs to be added to values file for helm install command:\njenkins:\rmaster:\rimagePullPolicy: IfNotPresent\rinstallPlugins: []\rBe careful not to override your existing values for jenkins: in this file.\nNext, download from https://updates.jenkins.io/download/plugins/ the following plugins in .hpi format\n - kubernetes:1.18.2\r- workflow-job:2.33\r- workflow-aggregator:2.6\r- credentials-binding:1.19\r- git:3.11.0\r- gogs-webhook:1.0.15\rAfter your helm install command goes through (next steps) and Jenkins pod is running with accessible GUI, transfer the .hpi files to your offline machine and upload the plugins manually into Jenkins Manage Jenkins -\u0026gt; Manage Plugins -\u0026gt; Advanced: Next Steps Continue with the installation of the CI/CD Hub\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/behaviour-testing/assembly-configurations/","title":"Assembly Configurations","tags":[],"description":"","content":"Assembly configurations are pre-configured instances of a VNF/Network Service you will make use of in your scenarios. They hold a reference to an assembly descriptor you wish to create and the values to set on the properties of that descriptor. Essentially they are re-usable plans for creating an instance of an assembly at a later time.\nIn your scenarios you will select the assembly configurations to be used. As the scenario begins, the instances will be created based on the planned configuration, then as the scenario completes they may be optionally uninstalled to keep the environment clean for the next execution.\nCreate an Assembly Configuration To create a test assembly click the \u0026ldquo;Create Assembly Configuration\u0026rdquo; button. This opens up a new screen that allows you to select the assembly you want to use, and provide a name and description. Once you select the assembly descriptor, the properties for it are shown within the same popup window, giving you the chance to set values for them (or leave to make use of the defaults).\nClick \u0026ldquo;Save\u0026rdquo; to finish creating the configuration.\nModifying Assembly Configurations You may change the descriptor or properties on an assembly configuration by opening the actions menu and selecting \u0026ldquo;Edit\u0026rdquo;:\nRemoving Assembly Configurations You may remove an assembly configuration by opening the actions menu and selecting \u0026ldquo;Delete\u0026rdquo;:\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/resource-engineering/resource-packages/brent/basic-resource/","title":"Basic Resource Example","tags":[],"description":"","content":""},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/behaviour-testing/","title":"Behaviour Testing","tags":[],"description":"","content":""},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/business-case/","title":"Business Case","tags":[],"description":"","content":"The Agile Lifecycle Manager (ALM) Business case ALM helps service providers effectively streamline service delivery, minimize complexity, and deliver innovative new services to customers at significant speed and lower cost of operations.\nThe problem our product solves and our value proposition are explained as follows:\nService providers are undergoing a platform and business transformation at the same time. One in support of the other.\nFor business, Service providers must find new sources of revenue growth and strive for higher profit margins. Transforming their business from delivering a small number of very large services to mass market customers, to delivering a long pipeline of smaller and more niche services, targeting specific markets - quickly scaling services that are successful and retiring those that are not.\nIn support of this business model Service Providers are transforming their delivery platform to a Cloud-based model, enabling a virtualized or software driven automated and agile factory.\nTo achieve this business model, the cost and effort involved in bringing these niche services to market must be as close to zero as possible, and as these services need to scale, the cost model associated must also scale in a predictable fashion. Traditionally, with large mass market services, ongoing maintenance is performed by throwing people at any issues or maintenance tasks. With a long pipeline of new services, this will not be acceptable. These ongoing maintenance tasks must be automated as much as possible.\nBecause network appliances and applications are software-based, they can be brought to market quickly and scaled and maintained in a much more automated manner. Also, network applications can be blended or integrated with Enterprise Cloud applications to bring high value media products and services to market. Traditional appliances require a lot of manual activity to install, configure and maintain physical appliances.\nWhy is this business model required? NFV automation in particular is more complex than traditional physical network appliances. It forces the separation of network software from a new Cloud based hardware infrastructure management platform. NFV infrastructure in particular, and VNF management tools are, as a result much more aligned with Enterprise Cloud/IT than traditional telecoms environments. In fact, NFV network applications are much more aligned with software engineering practices than traditional networking.\nThis new model sees large numbers of interconnected software applications deployed across a highly distributed set of Cloud like data centers. Enterprise Cloud environments typically have a small number of huge data centers, conversely service providers have a large number of smaller data centers, with lots of connections or dependencies between the software applications deployed across them. This complexity raises some significant skills issues in the operational teams that today bring new products and service to market those that are responsible for their ongoing maintenance.\nEngineering must integrate and test many 3rd party software applications across a number of infrastructure platforms and operations must now understand how to identify resolutions to issues across a highly complex set of environments that they are not today skilled for.\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/installation/cicdhub/","title":"CI/CD Hub","tags":[],"description":"","content":""},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/reference/security/default-security-users/","title":"Default LM Users","tags":[],"description":"","content":"The default installation of Agile Lifecycle Manager (ALM) comes with the following default users, groups, roles and privileges.\nDefault users    Username Password Member of Group Suspended     Jack jack SLMAdmin No   Jill jill Portal No   John john SLMAdmin No   Jane jane SLMAdmin, RootSecAdmin No   Derek derek RootSecAdmin No   Lisa lisa - No   Kim kim ReadOnly No   Steve steve SLMAdmin Yes    Default groups    Group Name Roles Granted     SLMAdmin SLMAdmin   Portal Portal   RootSecAdmin RootSecAdmin   ReadOnly ReadOnly    Default roles    SLM Role Description Privileges     SLM Admin A user with the \u0026lsquo;SLMAdmin\u0026rsquo; role can perform the whole range of operations permissible with ALM Network Service instances - CRUDEVNF Instances - CRUDENetwork Service Designs - CRUDEVNF Designs - CRUDEDeployment Locations - CRUDEVDUs - CRUDEBehaviour Tests - CRUDIntent Requests - Read and ExecuteResource Driver - Read and Write   Portal As a user with \u0026lsquo;Portal\u0026rsquo; role can create Assembly Instances, but otherwise has read-only access Network Service instances - CRUDEVNF Instances - Read-OnlyNetwork Service Designs - Read-OnlyVNF Designs - NO ACCESSDeployment Locations - Read-OnlyVDUs - Read-OnlyBehaviour Tests - CRUDEIntent Requests - Read and ExecuteResource Driver - Read and Write   ReadOnly As a user with \u0026lsquo;ReadOnly\u0026rsquo; role has read-only access to all parts of the system Network Service instances - Read-OnlyVNF Instances - Read-OnlyNetwork Service Designs - Read-OnlyVNF Designs - Read-OnlyDeployment Locations - Read-OnlyVDUs - Read-OnlyBehaviour Tests - Read-OnlyResource Driver - Read-Only   RootSecAdmin \u0026lsquo;Root Security Admin\u0026rsquo;. Only an authenticated LM user with this role can create/update/delete role definitions within ALM and carry out other security admin tasks.A user with only this role cannot perform any other operations within ALM. User Credentials - CRUDE   BehaviourScenarioExecute \u0026lsquo;Root Security Admin\u0026rsquo;. Only an authenticated LM user with this role can create/update/delete role definitions within ALM and carry out other security admin tasks.A user with only this role cannot perform any other operations within ALM. Network Service instances - Read and WriteIntent Requests - Execute    Available privileges The list of available privileges which can be listed against a role to assign permissions are as follows:\n   Category Description Privilege Available actions     Security Administration Administer the security of the system, i.e. viewing or modifying credentials SECADMIN executewriteread   Intent Request Operations Perform intents relating to health operations on an assembly, i.e. scale or heal INTENTREQSOPS executewriteread   Intent Request Management Perform intents relating to management of assemblies, e.g. create, delete or upgrade INTENTREQSLMGT executeread   Network Service Instance Management Perform operations reliant on creating or viewing network service instances NSINSTSMG executewriteread   Network Service Design Create and view descriptors for network services NSDESMGT executewriteread   SLM Administration Manage deployment locations, resource managers and UI themes SLMADMIN executewriteread   Manage Deployment Locations Create and view deployment locations DEPLOYLOCMGT executewriteread   Behaviour Scenario Execute Execute behaviour scenarios and view execution results BEHVRSCENEXEC executewriteread   Behaviour Scenario Design Design behaviour scenarios BEHVRSCENDES writeread   Resource Driver Management Manage resource drivers in relations to a resource manager e.g onboard or get location RMDRVR writeread   Resource Driver Key Management Read (private and public) keys returned by a resource drivers RMDRVRKEYS read   Resource Package Management Create resource packages that work with resource drivers RESOURCEPKG write   VNF Instance Management View VNF instances VNFINSTSMGT executewriteread   VNF Instance Design Create and view descriptors for VNF instances VNFDESMGT executewriteread   VDU Instance Management Manage VDU instances VDUINSTSMGT executewriteread   VDU Group Management Manage VDU groups VDUGRPMGT executewriteread   VDU Instance Design Manage descriptors for VDU instances VDUDESMGT executewriteread    "},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/network-service-design/develop-a-network-service-package/design-a-network-service/","title":"Design a Network Service","tags":[],"description":"","content":"This section describes the steps to design and model a Network Service. A Network Service is typically designed prior to the implementation. This is part of a customer project in which requirements are gathered, a design is made including the Network Service Design, and then implemented.\nObjectives  Learn how to model a Network Service Create a Network Service graphical model that can be used during implementation  Pre-requisites  An understanding of the customer requirements for the Network Service in terms of composition, location, relationships, networking, testing, and underlying infrastructure requirements. Understanding of the Network Service components, use cases, and relationships. This information is usually gathered during the Requirements phase of a project. An understanding of Agile Lifecycle Manager (ALM) programming model.  Steps to Design a Network Service Once the pre-requisites are met, and you have a good understanding of the Network Service components and use cases, we can continue with the design, as follows:\nDesign Lifecycle artifacts The Network Service artifacts should be described in the table below. This includes the lifecycles that are applicable, their dependencies and their healing or scaling policies.\n   artifact Description Applicable lifecycles Dependencies Healing and scaling policies     Network Service e.g. Enterprise connectivity service      VNF/PNF e.g. vFirewall      VNFC e.g. 1 VNFC for vFirewall Install, configure, start, stop, heal, scale e.g. instantiated after VNF xyz, requires internal network xyz e.g. scale out if threshold exceeds x   VDU e.g. Docker image      Virtual Infrastructure e.g. Openstack in DC 1       You might also want to graphically represent this and describe the Network Service, VNF, VNFC, VDU, and Infrastructure artifacts in an image.\nModel Lifecycle artifacts Once there is visibility of the various artifacts, i.e. NS, VNF, VNFC, how they are managed through a lifecycle, and the rules that govern their composition, it is possible to go back to the requirements and map them into a working model of the ‘service’.\nWith the target modeling process in place the requirements above can be modeled to make up the VNFC, VNF and Network Services. Each artifact in a model demonstrates the interdependancies. This first step may create a large model that will be broken down into the artifacts that will need to be built and managed to create the network service in question.\nFor each noun in the details above create a rectangle in the model. Do not worry at this stage as to how these relate or whether something might be contained in something else. Put them all down. The next stage is to draw lines between these objects to indicate how they relate to each other. Typically we have the following types of lines between objects:\nThe first type of link ‘is a’ describes that the child is of the type of parent. For example, Ethernet Connection \u0026lsquo;is a\u0026rsquo; Connection. This type of link begins to gather objects that are the same type together, and to share some common details between each other. The ‘part of’ line indicates that the child is contained in the parent. Examples of this might be that Switch Port is part of a Switch. This link allows objects to be gathered into a natural hierarchy. ‘Related’ and ‘loosely related’ are links that allow you to describe how objects may be related informally. For example, the type of relationship might simply be that one object ‘uses’ another object.\nAs the subsequent steps of the modeling happen, these links may inform how objects are gathered together. One aspect of this initial model that is worth highlighting on each object, is the location type which will be deployed to and its VIM type. One way to do this is to color the different VIM objects in a different color and outline each object that is to be deployed in a similar location in the same color.\nGroup artifacts The next step of the process is to decompose the large model into smaller models that group artifacts and lifecycles together, by location, VIM type, shared lifecycles etc. Grouping can be done based on the following shared characteristics.\nGroup by location The first grouping to apply to your model is to group by location. Move the diagram around so that those items that are to be deployed in the same location or have dependent placement strategies are grouped together. Some objects may exist is more than one location – if this is the case replicate them so that they are shown in all locations.\nOther objects me be connections or service-chains that exist across locations. Put these in a group on their own. It is likely that these will be applied either to a VIM type that models these objects between the locations e.g. a SDN Controller or that the configuration of these will need to be applied in all the locations they span. This will become evident as the other types of grouping happen. There may be some network services that there is only one location. That is fine, continue onto the next section.\nGroup by VIM type Now for each location, group the objects into the VIM types that occur at those locations. This may be a simple exercise, however, for some locations there will be a less defined notion of a VIM. For example, in edge devices – the device itself may be in the model and they\u0026rsquo;ll be no defined VIM type associated with that. Group the items that are linked to the physical devices, together. Mark them as physical on the diagram.\nGroup by time of instantiation When you look at the model you will have some items that have to exist before others can exist. Rearrange your diagrams into layers with the items that need to be created first at the bottom. Try to layer the model so that objects that would be created together are shown side by side. Don’t worry if an object exists in a layer on its own. This is useful information.\nGroup by time of lifecycle This may not be as obvious as the other groupings. Some object may be so tightly linked to each other that they will always exist in a group and can only be created as a group. Group these objects together, put them in a box and see if your model still works if you considered this new single object without seeing the individuals of the group. If this looks and feels right, then this type of grouping can simplify the overall model, by abstracting these lower level objects. Promoting operations and metrics from the members of the group will also help with the abstraction. If a member of this group is a cluster a further question about whether details of the cluster will be required outside the grouping must be asked. Clusters handle properties differently to singleton objects. The set of properties are passed as arrays of values that must remain immutable. Passing them around is likely to cause confusion, so care should be taken before deciding to abstract clustered objects.\nIdentifying relationships In the model you need to identify those objects that need to pass information between each other and mark these relationships. It may be worth creating a new rectangle for each of these relationships and drawing the lines to it from the objects that will share the relationship. With a relationship it is often the case that one party is responsible for offering a capability to the other party. If this is obvious, then mark the object that offers the capability as the source of the relationship. When identifying relationships, it is worth noting that they do not necessarily fall tidily in one of your layers. Relationships may cross the layers, locations and/or VIM types. It is also likely that as you build the solution that new relationships will become obvious and this may need new operations to be exposed.\nIdentifying clusters For any object that can be scaled out and in, should be identified. In the background work to do with the objects it is worth finding out the details of any metrics that could be used to auto-scale the clusters. Some clusters are managed by an independent object outside of the cluster. Others use the cluster itself to decide this. If this is the case, you will need to consider how the metrics are gathered and decide if one of the cluster elements (often the first) makes the scaling decision based on shared metrics. This may influence how the cluster if managed.\nFinalizing your design In your model you now have groups of objects by location, VIM type, lifecycle, and time of instantiation. We now need to map these groups and objects into the appropriate lifecycle manager construct.\n The physical devices will need to be handled first. In some cases they need to be treated as a location into which configurations will be deployed. If this is the case the object should be marked as a location and will probably need to be implemented in two parts,  A location in the appropriate resource manager will need to be created. This can happen as part of the Install phase of the device, or as a manual step. The device configuration will then need to be installed onto the device. This should be treated as a NFC and one of these should be created and wrapped into an NF.   Each atomic object on the model will be packaged as a VNFC. These need to be wrapped in a VNF layer. This may seem like an extra layer that does not add much value. However, the VNFC may present many different offerings, this VNF layer is a way of specializing the general VNFC into what is needed for the solution being delivered. It also allows for service specific defaults to be applied that will simplify the information needed to be provided by the upper layers. Each of the Grouping shown on the model should be regarded as a network service. Each layer may depend on the layer below, i.e. the group that is created before it. The upper groups will then reference these lower level groups. The referenced groups will need to expose the required set of properties and promoted operations to allow property mappings and relationships to happen.  Now that all the groups are defined, an overall group needs to be defined for the top-level Network Service.\nResults Although there are lots of details that need to be gathered and collated for the design of the various groups, and components of the solution it is usually a good idea to create an overview diagram of how the various VNFC, VNF and NS relate to each other. Normally the details properties are not included in this diagram, but the details of operations is important as showing relationships on the model helps understand how the links will be established and what each component needs to contribute. There is no single modeling format for this overview diagram. UML class diagrams can be used.\nFrom the picture above you can see that a fairly standard UML class diagram can represent the model. The lines with black diamonds indicate composition items. The dotted lines with open diamonds indicate references (aggregation). The end with the diamond is doing the composition or referencing. The curved lines are the relationships. The open end of the line indicates the target of the relationship the filled end indicates the source. When a relationship is across a reference the referenced entity will always be the source of the relationship. Within each ‘class’ the method represent the operations exposed by the different levels.\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/network-service-design/develop-a-network-service-package/","title":"Develop a Network Service Package","tags":[],"description":"","content":""},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/cicd/getting-started/","title":"Getting Started","tags":[],"description":"","content":"Objective This section describes how to setup your own Agile Lifecycle Manager (ALM) CI/CD process and tools using the CI/CD Hub.\nPre-requisites You must have installed the CI/CD Hub software and attach one or more ALMs. A detailed description of the CI/CD Hub software can be found here.\nALM CI/CD Process and Tasks Once the CI/CD Hub is up and running, perform the following tasks to configure it to the needs of your team.\n Configure CI/CD Hub: Configure CI/CD Hub software components and create users etc. Install Local LMCTL: Install ALM command line tool for local development  The image below describes the common tasks required to run a CI/CD process for a Network Service or VNF project.\nThe first set of tasks to create a new project are typically performed in a local ALM \u0026ldquo;Development\u0026rdquo; environment.\n Create a new Project: Setup new local project structure and git repository. Upload images: Load any VNF software images to CI/CD Hub repository. Develop Package: Implement the package artifacts. Add tests: Add tests to ensure expected operational behaviour.  Pipeline tasks below are required to automate the population of pre-production ALM slaves, running behaviour tests and generating versions of binary packages.\n Create CI Pipeline: Trigger build and deployment process from your project source. Create CD Pipeline: Automate release process from pre-production to production environments  Package versions need to be continually updated, a typical update task is described below.\n Update Package Version: Manage new versions of released packages.  "},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/best-practices/how-to-guides/","title":"How-to Guides","tags":[],"description":"","content":"This section discusses common Agile Lifecycle Manager (ALM) use cases and provides examples to help illustrate how to best apply to what you are working on.\nVNFs  Upgrade a VNF with no service downtime Move a VNF from one location to another  Network Services  Model an EPC Network Service Create service chains and auto test their behaviour Model an Enterprise Connectivity Network Service  Data Center Automation  Configure and tune hardware devices Configure network fabric controllers Auto-test your NFVI data center Configure physical network appliances  Northbound Integration  Integrate with external policy engines  Southbound Integration  Build your own Resource Manager Integrate with ETSI SOL003 VNFMs  DevOps  Manage CI/CD inter package dependencies Self certify VNFs Manage lifecycle of VNF packages that are managed by external VNFMs  Cloud native  Create a cloud native \u0026ldquo;Network Service Mesh\u0026rdquo;  "},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/reference/behaviour-testing/step-reference/intent-engine/","title":"Intent Engine","tags":[],"description":"","content":"Create Assembly Description Requests the creation of an Assembly with the given descriptor and initial state. The step then waits for the process to complete successfully.\nPasses when:\n the create was accepted and completed successfully  Fails when:\n the create was rejected (not a valid request) the create was accepted but failed to complete successfully the create was accepted but has not finished before a configurable amount of time has passed (configured with alm.doki.execution.lifecycleEventTimeout. Default = 900s)  Properties    Property Description     assemblyName The chosen name of the Assembly to be created   descriptorName name of the Assembly Descriptor to create an instance from   initialState the state to create the Assembly in: Installed, Inactive or Active    Create Assembly with properties Description Requests the creation of an Assembly with the given descriptor and initial state. The step then waits for the process to complete successfully. This step includes a parameter to set the property values from the descriptor this instance is being created from.\nPasses when:\n the create was accepted and completed successfully  Fails when:\n the create was rejected (not a valid request) the create was accepted but failed to complete successfully the create was accepted but has not finished before a configurable amount of time has passed (configured with alm.doki.execution.lifecycleEventTimeout. Default = 900s)  Properties    Property Description     assemblyName The chosen name of the Assembly to be created   descriptorName Name of the Assembly Descriptor to create an instance from   initialState The state to create the Assembly in: Installed, Inactive or Active   assemblyProperties Properties to use in the creation of the Assembly instance    Uninstall Assembly Description Requests the uninstall of a given Assembly. The step then waits for the process to complete successfully.\nPasses when:\n the uninstall was accepted and completed successfully  Fails when:\n the uninstall was rejected (not a valid request) the uninstall was accepted but failed to complete successfully the uninstall was accepted but has not finished before a configurable amount of time has passed (configured with alm.doki.execution.lifecycleEventTimeout. Default = 900s)  Properties    Property Description     assemblyName The name of the Assembly to uninstall. This Assembly may have been created by the scenario or existed previously. If using an \u0026ldquo;Existing Provided\u0026rdquo; assembly configuration then you may use the reference name you chose when adding it to the scenario    Change Assembly State Description Requests to start a process to change the state of a given Assembly. The step then waits for the process to complete successfully.\nPasses when:\n the change state request was accepted and completed successfully  Fails when:\n the change state request was rejected (not a valid request) the change state request was accepted but failed to complete successfully the change state request was accepted but has not finished before a configurable amount of time has passed (configured with alm.doki.execution.lifecycleEventTimeout. Default = 900s)  Properties    Property Description     assemblyName The name of the Assembly to change state. This Assembly may have been created by the scenario or existed previously. If using an \u0026ldquo;Existing Provided\u0026rdquo; assembly configuration then you may use the reference name you chose when adding it to the scenario   newState The target state of the Assembly    Scale Out Cluster Description Requests to start a process to scale out a cluster belonging to the given Assembly. The step then waits for the process to complete successfully.\nPasses when:\n the scale out request was accepted and completed successfully  Fails when:\n the scale out request was rejected (not a valid request) the scale out request was accepted but failed to complete successfully the scale out request was accepted but has not finished before a configurable amount of time has passed (configured with alm.doki.execution.lifecycleEventTimeout. Default = 900s)  Properties    Property Description     clusterName Name of the Cluster instance in the Assembly to scale   assemblyName The name of the Assembly instance. This Assembly may have been created by the scenario or existed previously. If using an \u0026ldquo;Existing Provided\u0026rdquo; assembly configuration then you may use the reference name you chose when adding it to the scenario    Scale In Cluster Description Requests to start a process to scale in a cluster belonging to the given Assembly. The step then waits for the process to complete successfully.\nPasses when:\n the scale in request was accepted and completed successfully  Fails when:\n the scale in request was rejected (not a valid request) the scale in request was accepted but failed to complete successfully the scale in request was accepted but has not finished before a configurable amount of time has passed (configured with alm.doki.execution.lifecycleEventTimeout. Default = 900s)  Properties    Property Description     clusterName Name of the Cluster instance in the Assembly to scale   assemblyName The name of the Assembly instance. This Assembly may have been created by the scenario or existed previously. If using an \u0026ldquo;Existing Provided\u0026rdquo; assembly configuration then you may use the reference name you chose when adding it to the scenario    Heal Resource Description Requests to start a process to heal a Resource belonging to the given Assembly. The step then waits for the process to complete successfully.\nPasses when:\n the heal request was accepted and completed successfully  Fails when:\n the heal request was rejected (not a valid request) the heal request was accepted but failed to complete successfully the heal request was accepted but has not finished before a configurable amount of time has passed (configured with alm.doki.execution.lifecycleEventTimeout. Default = 900s)  Properties    Property Description     brokenComponentName Name of the Component instance in the Assembly to heal   assemblyName The name of the Assembly instance. This Assembly may have been created by the scenario or existed previously. If using an \u0026ldquo;Existing Provided\u0026rdquo; assembly configuration then you may use the reference name you chose when adding it to the scenario    Upgrade Assembly Description Requests to start a process to upgrade the given Assembly. The step then waits for the process to complete successfully. You may upgrade the descriptor and/or property values.\nPasses when:\n the upgrade request was accepted and completed successfully  Fails when:\n the upgrade request was rejected (not a valid request) the upgrade request was accepted but failed to complete successfully the upgrade request was accepted but has not finished before a configurable amount of time has passed (configured with alm.doki.execution.lifecycleEventTimeout. Default = 900s)  Properties    Property Description     assemblyName The name of the Assembly instance to be upgraded. This Assembly may have been created by the scenario or existed previously. If using an \u0026ldquo;Existing Provided\u0026rdquo; assembly configuration then you may use the reference name you chose when adding it to the scenario   descriptorName Name of the Assembly Descriptor to upgrade to (set to the existing descriptor name to only make property updates)   properties Properties to use in the upgrade of the Assembly instance    Upgrade Assembly Descriptor Description Requests to start a process to upgrade the given Assembly to a new descriptor. The step then waits for the process to complete successfully.\nPasses when:\n the upgrade request was accepted and completed successfully  Fails when:\n the upgrade request was rejected (not a valid request) the upgrade request was accepted but failed to complete successfully the upgrade request was accepted but has not finished before a configurable amount of time has passed (configured with alm.doki.execution.lifecycleEventTimeout. Default = 900s)  Properties    Property Description     assemblyName The name of the Assembly instance to be upgraded. This Assembly may have been created by the scenario or existed previously. If using an \u0026ldquo;Existing Provided\u0026rdquo; assembly configuration then you may use the reference name you chose when adding it to the scenario   descriptorName Name of the Assembly Descriptor to upgrade to    "},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/administration/configuration/languages/language-installation/","title":"Language Installation","tags":[],"description":"","content":"The following steps explain how to alter the installation process to change the language files used by the Agile Lifecycle Manager (ALM) User Interface.\nInstalling a Locale   Follow the steps in language overview before doing the following.\n  When installing the lm-configurator helm chart you must include the following values:\n   configurator:\rlmConfigImport:\rnimrod:\ralm.nimrod.localeLocation: /var/lm/locales\rWhen installing the lm-helm helm chart you must include the following values so the ConfigMap with locales.tar can be loaded by LM:   nimrod:\rapp:\rconfig:\rlocalesConfigMap: lm-locales\rOnce LM has started you will be able to view the UI with the updated locales.  "},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/administration/security/manage-roles/","title":"Manage Roles","tags":[],"description":"","content":"Pre-requisites To complete this guide you will need:\n An existing installation of the Agile Lifecycle Manager (ALM) An understanding of how to configure LM using the Vault UI kubectl client with access to the Kubernetes cluster LM is installed on  Find Role Configuration   Login to the Vault UI for your LM system and navigate to the secrets engine named lm\n  Navigate to the secret named ishtar\n  Find the existing roles configuration in the JSON at:\nalm:\rroles:\r...\r  Add a role Add a role by adding a new entry under the roles key:\nalm:\rroles:\rMyNewRole:\rldapGroups:\r- MyRole\rprivileges:\rNsinstsMgt: read,write\r MyNewRole - the unique name for this role ldapGroups - the LDAP group a user should be assigned in order to acquire this role privileges - the privileges given to any user assigned this role and the granted actions (execute, write, read).  You must restart the ishtar service for the changes to take affect.\nFor a list of available privileges, see Available Privileges\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/installation/lm/production/offline/","title":"Offline","tags":[],"description":"","content":"The following guide explains how to pre-prepare the artifacts required by Agile Lifecycle Manager (ALM) during installation so you may complete installation at a later date without internet access.\nPreparing Offline Install Helm Charts The Helm charts should have already been gathered and copied onto the installation machine.\nDocker Images  Create a workspace  mkdir lm-docker-images\rIdentifying Images  Below is a full list of the docker images used by the sub-charts in v2.0.3 of all the ALM helm charts:\n# Docker Registry\rdocker pull registry:2.6.2\rdocker pull cassandra:3\rdocker pull openjdk:8u181-jre\rdocker pull quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.20.0\rdocker pull docker.elastic.co/beats/filebeat-oss:6.4.0\rdocker pull confluentinc/cp-kafka:4.1.1-2\rdocker pull osixia/openldap:1.2.1\rdocker pull vault:0.10.1\rdocker pull docker.elastic.co/elasticsearch/elasticsearch-oss:6.1.1\rdocker pull gcr.io/google_samples/k8szk:v3\rdocker pull k8s.gcr.io/defaultbackend:1.4\rdocker pull lwolf/kubectl_deployer:0.4\rdocker pull frapsoft/openssl:latest\rThe following are included in the Helm charts but are for disabled features, so are only required if you intend to enable additional elements of the charts (at your discretion).\n# Jenkins - optional\rdocker pull confluentinc/cp-ksql-server:\rThis list was obtained by:\n extracting the Helm chart to access the sub-charts: tar -xvzf \u0026lt;your-helm-chart\u0026gt; using helm inspect values on each sub-chart found in extracted cicdhub/charts (be sure to extract each chart and check for sub-charts) looking for any docker image related settings such as image or imageName  These commands should be executed for these 3 Helm charts:\n helm-foundation Helm chart lm-configurator Helm chart lm-helm - Helm chart  Pull Images  Pull each image, identified above, with docker:\ndocker pull \u0026lt;image\u0026gt;\rBuild the ALM Images  You will need to build the ALM images offline. This should already have been covered in Getting Started\nFor each image that was built, it will be necessary to pull it, e.g.\ndocker pull nimrod:2.0.3-307\rdocker pull apollo:2.0.3-294\rdocker pull watchtower:2.0.3-283\rdocker pull galileo:2.0.3-299\rdocker pull ishtar:2.0.3-290\rdocker pull talledega:2.0.3-82\rdocker pull lm-configurator:2.0.3-50\rdocker pull doki:2.0.3-87\rdocker pull relay:2.0.3-284\rdocker pull daytona:2.0.3-309\rdocker pull conductor:2.0.3-284\rSave Images  Save each image into a tarball with docker:\ndocker save \u0026lt;image\u0026gt; -o lm-docker-images/\u0026lt;image\u0026gt;.tar\rBuild single archive  Create a single archive of your lm-docker-images directory:\ntar -cvzf lm-2.0.3.tgz lm-docker-images/\rThis single archive can now be transferred to the target install environment to complete an offline installation.\nPerforming Offline Install Helm Charts Copy the previously downloaded LM Helm charts to the machine you intend to run the installation from.\nDocker Images  Transfer single archive to target  Copy the archive to the machine you intend to install LM on\nExtract Archive  Extract the archive to access the images inside:\ntar -xvzf lm-2.0.3.tgz\rLoad Images  Load each image tarball with docker:\ndocker load \u0026lt;image\u0026gt; -i lm-docker-images/\u0026lt;image\u0026gt;.tar\rThe images are now available on the local docker system. As each service is installed, it will search the local system and find the image it needs, therefore it will not attempt to pull externally.\nNext Steps Continue with the configuration of Access Configuration\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/installation/lm/openshift/","title":"OpenShift Origin","tags":[],"description":"","content":""},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/installation/lm/production/","title":"Production","tags":[],"description":"","content":""},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/reference/descriptor-specification/resource-descriptor/","title":"Resource Descriptor","tags":[],"description":"","content":"Introduction This document describes the descriptors that are used by the Agile Lifecycle Manager (ALM). ALM needs to have descriptions of the building blocks of applications that it is going to manage. The basic building blocks are described in resource descriptors. Sets of these are composed into assembly descriptors to allow designers to describe a complete application/service that they need LM to manage.\nNaming Each resource must have a name and a version - it is advisable to name the resource assuming other people might use the same name i.e. companyname.resourcename. Any change to any part of the resource should result in creating a new resource with an updated version number.\nThe resource descriptor name field will contain the following string:\nresource::uniqueName::1.0\nThis consists of three components, each separated by a :: (double colon).\n Descriptor Type: LM supports two descriptor types assembly and resource identifying the two core descriptor types. Name: Each descriptor is uniquely identified by its name for a given descriptor type. The name must match the following rules  Must start with a letter (either case) Can include letters, digits, non-consecutive underscores (single) and hyphens (minus sign). The name must not contain spaces A maximum length of 50 characters.   Version: The version is a field consisting of one or more numeric values separated by single point . no alpha characters are permitted thus 1.0 and 1.0.0 are valid while v1.0 is not.  Descriptor Sections The following sections of the descriptors can occur in resource descriptors. These can be required or optional. Default values can be supplied. The table below describes the resource descriptor sections:\n   Name Required Comment, Function or Description     name N Unique name and version name of the resource.   description N Description of the resource.   properties N Properties and values for the resource instance such as image location, networks, information for metrics and ID.   private-properties N Private properties and values for the resource instance. These properties are not exposed outside of Brent.   infrastructure N Infrastructure that applies for this resource.   lifecycle N Lifecycles that apply for this resource.   default-driver N The default Resource driver to use if not explicitly defined in either \u0026ldquo;operations\u0026rdquo; or \u0026ldquo;lifecycle\u0026rdquo;.   metrics N Metrics such as integrity and publication period.   policies N Policies using these metrics such as heal based in the integrity metric.   operations N Transitions that are invoked when establishing relationships.    Header The header of a resource descriptor includes the name and textual description of the descriptor and associated resource manager type. The contents of the name field must be a globally unique string.\nname: resource::c_streamer::1.0\rdescription: Component package for c_streamer Properties and Private Properties The properties and private-properties sections defines the properties of a resource instance; that is, the set of properties that are required to orchestrate a resource through to the Active state. These can be understood as the context for the management of the resource during its lifecycle. The properties section comprises the set of properties that are exposed outside of Brent. The private-properties section defines the private properties of a resource instance. These properties are not exposed through the north-bound resource manager API of Brent, but they are sent to drivers along with the \u0026ldquo;properties\u0026rdquo; through the south-bound driver APIs when handling transition requests.\nEach property is identified by a name and a number of attributes.\n   Attribute M/O Type Restrictions Comment     name M string No spaces permitted, minimum length of one character (alphanumeric), no dot characters, must conform to yaml. The chosen name for the property. There are few restrictions placed on it so as not to restrict its use however the limited identified restrictions in addition to scope uniqueness must be met.   type M value string | key | resourceKeyId Currently only the value string or key is supported. This attribute exists to support product expansion.   required O bool  Indicates that a value for this property must be provided at Resource instantiation time.   description O string  A textual description of the role of the property.   default O string  An optional default value can be provided which will be supplied to the resource in the event that an override is not provided at instantiation time.   read-only O bool  Indicates that a value cannot be provided at instantion time (note a default or a mapped property may be provided) and that the resource may provide a (updated) value once instantated.   volatile O bool  Identifies that changes to this property will not, in and of itself, trigger a reinstall of the resource. Additional information can be found here.   Value O string / Reference  It is possible to provide a static value for the resource property which cannot be overridden at instantiation time. Alternatively a value can be identified as being mapped from one or more properties of the parent Assembly or from those of any peer component using a ${\u0026lt;sourceProperty\u0026gt;} notation.    properties:\rdeploymentLocation: # the name of the property ​ type: string\r​ required: true\r​ description: The name of the OpenStack project (tenant) to deploy this resource to. numOfStreamers:\r​ type: string\r​ description: The number of streamers that should be created at install time\r​ default: 2\rtenant_key_name:\r​ type: string\r​ required: true\r​ description: The SSH key for the current tenant\rflavor:\r​ value: m1.small\rcluster_public_ip_address:\r​ type: string\r​ description: the public IP address for this cluster\r​ read-only: true\r​ value: '${balancer.publicIp}'\rlicanceKey:\r​ type: string\r​ description: A runtime modifiable licanceKey\rvolatile: true\rvmKeyId:\r​ type: resourceKeyId\r​ description: The identity of the Key which should be used when communicating with the VM\rrequired: true\r  name - each property name must be unique within its property section (scope). The name cannot contain dot (period) character but has limited additional restrictions.\n  type - currently, the type field has restricted use. As of ALM 2.1 two additional supported types of resourceKeyId and key are supported over the previously supported string. The default type is string.\n  type will in future allow active handling of different types of data (for example: dates, IP addresses, etc…). It is recommended users omit this field or use the supported values of “string” and \u0026ldquo;key\u0026rdquo; to avoid compatibility issues in the future.\nProperties are optional unless explicitly defined as required by the inclusion of a required: true flag. This only has material affect on a top-level assembly and means that a value must be present (for example: not null) for a property. However it is recommended that where a value is mandatory it be present in a resource descriptor as an aid to Assembly descriptor designers to ensure that it is reflected in assemblies as the property is mapped/promoted. This can be evaluated from the “value” field, a “default” or passed in from the intent request.\nProperties marked as read-only: true will not be overridden by values mapped in from an enclosing assembly or from the intent request. This is typically used for properties that are calculated from or returned by the resource itself.\nProperties may be declared with a default value or a specific value or neither. Where the value field is used it may either be an explicit value or it may reference to another property within the descriptor. When referencing another property, the reference will look as follows: value: ‘${max_connections}’\nThe LM will assign an internal name and identifier for each resource instance it creates. It also supplies the index number of a resource in a cluster. These values can be useful to give unique names for servers etc. To access them a property may have its value set to ${instance.name}, ${instance.id} or ${instance.index}. These should be placed in the value field of a property. The LM will then replace the placeholders with the appropriate value.\nKey Properties (v2.1 onwards) Properties can have a type key. For non-read-only properties, the value of a key property is the name of a shared infrastructure key managed by Brent. The actual key will be substituted at runtime before being sent to Brent drivers. For read-only properties, the value of a key property is the name of a shared infrastructure key.\nVolatile Properties New to LM 2.1 is an additonal property attribute \u0026lsquo;volatile\u0026rsquo; which identifies a property as being a runtime modifiable property.\nA volatile property is one which the designer has actively identified as being modifiable for an instantiated, active assembly/resource without the need for ALM to \u0026lsquo;reinstall\u0026rsquo; the assembly/resource. Rather, where the set of property changes to individual resources within an assembly, wholly consists of changes to the values of volatile properties, LM will execute a Reconfigure operation on those resources rather than follow the default resource \u0026lsquo;reinstall\u0026rsquo; procedure which would otherwise be followed.\nIt is important to note that while it can be identified as an attribute of a property at both the assembly and resource level it only meaning at the resource level. However it is good practice to flag properties as being volatile through the assembly hierarchy\nIdentifying a property as Volatile A property is identified as being volatile by adding the following attribute to its descriptor. This is achieved through the LM UI by selecting the Volatile tick box on the property.\n volatile: true\rFor example:\n licenceKey:\r​ type: string\r​ description: A runtime modifiable licenceKey\rvolatile: true\rReconfigure \u0026amp; Volatile properties The reconfigure action for a Resource is triggered only when certain criteria are met. If they are not all met then a standard change procedure if Uninstall and reinstall is followed.\n The only changes made to a resource are volatile property changes No other changes to the assembly cascade through property dependencies to the resource The resource is in an active state  With all three of these preconditions met the Reconfigure operation will be executed.\nProperty attribute conflicts The volatile attribute can be added to any property subject to the following restrictions;\n A volatile property cannot be readonly as the combination represents conflicting directives A volatile property cannot have a static Value as a static Value makes the value immutable  Capabilities and Requirements These two sections allow designers to explain what functions the resources are implementing or need before they can work successfully. These might be expressing that networks or various types must be available for the resource instances to work or it may be describing that a resource supports, for example, incoming http requests.\nThe type is a string that expresses the capability or requirement. The values in these string will have to be agreed across an organization and where possible they should be agreed by the industry. Resource capabilities should use common industry terms. In the examples below the idea is that httpStreamOutput indicates that the capability is using the http protocol in a stream form and in an output direction. The OS::Neutron:Net is the resource type from OpenStack associated with a network instantiated within neutron.\ncapabilities These are used to enable service designers to understand what function a resource provides.\ncapabilities:\r​ VideoStream:\r​ type: httpStreamOutput\rcapabilities:\r​ Network: ​ type: neutronNetwork\rrequirements Similar to the capabilities the requirements contain the list of capabilities that the resource needs to be provided for them to work.\nrequirements:\r​ VideoNetwork:\r​ type: neutronNetwork\r​ ManagementNetwork:\r​ type: neutronNetwork\r​ RemoteNFSMountPoint:\r​ type: nfsExportMountpoint\roperations This section defines sets operations that can be called to enable relationships to be created between resources. Operations definitions in the resource have a name and a set of properties. Where a resource descriptor describes an operation. As a convention the name of the operation should be linked to the capability that is being enabled through the creation of the relationship.\noperations:\rRemoveHttpStreamOutput:\r​ description: removes the http server from being managed by the balancer\r​ properties:\r​ server_ip:\r​ type: string\r​ description: Http Server Ip Address\r​ default: the ip address\r​ server_port:\r​ type: string\r​ description: http server port number ​ default: '8080'\rAddHttpStreamOutput:\r​ description: adds an http server to the balancer's pool\r​ properties:\r​ max_connections:\r​ type: string\r​ description: Maximum connections for the balanced server\r​ default: 3\r​ server_ip:\r​ type: string\r​ description: Ip Address of the server to be balanced\r​ server_port:\r​ type: string\r​ description: Port on balanced server\r​ default: '8080'\rInfrastructure The \u0026ldquo;infrastructure\u0026rdquo; section defines the resource\u0026rsquo;s infrastructure types as empty maps objects. Mappings from infrastructure type to template types and template files will be set under each lifecycle section (e.g Create or Delete).\ninfrastructure:\rOpenstack: {}\rDefault Driver The default-driver section matches any driver selection that is not explicitly defined in lifecycle or operations. It defines the Resource Driver to use if no explicit match is made.\ndefault-driver:\ransible:\rselector:\rinfrastructure-type:\r- '*'\rLifecycle The lifecycle section defines the lifecycle transitions that the resource supports.\nResource descriptors must support the Create lifecycle transition. However, they may implement the other lifecycle transitions which are: Install, Configure, Reconfigure, Start, Stop, Integrity, Delete and Uninstall. Where the transition is not provided by the resource, LM will be free to change the state of the associated component instances without calling any underlying transition.\nFor each lifecycle transition one can configure in the \u0026ldquo;drivers\u0026rdquo; section which Resource Driver to use based on property values. The key is the type of a Resource Driver.\nThrough Resource Drivers, Create and Delete handles the existence of infrastructure objects while other lifecycles execute lifecycle transitions against the objects that are part of the resource\u0026rsquo;s associated topology. An example: \u0026ldquo;Create\u0026rdquo; lifecycle uses the Kubernetes Driver to create a Kubernetes pod, then \u0026ldquo;Install\u0026rdquo; lifecycle runs an Ansible playbook in that pod.\nSelector criteria defines property values to match when selecting a resource driver to handle a lifecycle transition request. At present only infrastructure-type (the deployment location \u0026ldquo;type\u0026rdquo;) is supported, allowing the selection of a resource driver based on one or more infrastructure types.\ninfrastructure:\rOpenstack: {}\rlifecycle:\rCreate:\rdrivers:\ropenstack:\rselector:\rinfrastructure-type:\r- Openstack\rtemplate:\rfile: heat.yaml\rtemplate_type: HEAT\rInstall:\rdrivers:\ransible:\rselector:\rinfrastructure-type:\r- '*'\rDelete:\rdrivers:\ropenstack:\rselector:\rinfrastructure-type:\r- '*'\rMetrics and Policies A resource descriptor may indicate that the underlying resource will emit one or more metrics.\nA metric is defined as having a name, type and an optional publication-period.\nIf no publication period is given at all, a default of 60 seconds is assumed. The publication period is in seconds. A value of 0 means no metrics will be published. The value must be +integer\nThere are two reserved types that are used by ALM to monitor the health of the associated resource instances.:\n metric::integrity metric::load  Example resource metrics.\nmetrics:\rh_integrity:\r​ type: \u0026quot;metric::integrity\u0026quot;\r​ publication-period: \u0026quot;${integrity_publication_period}\u0026quot;\rload:\r​ type: \u0026quot;metric::load\u0026quot;\rWithin the resource descriptor the policies section contains details of the heal policy. This allows the smoothing interval to be defined for the resource.\nEach policy has a name, the associated metric, an action (heal) and a properties section. Example policies section.\npolicies:\rheal:\r​ metric: \u0026quot;h_integrity\u0026quot;\r​ type: \u0026quot;policy::heal\u0026quot;\r​ properties:\r​ smoothing:\r​ value: \u0026quot;${number-of-intervals}\u0026quot;\rThe above example shows the policy associated with the Integrity metric. The smoothing value is used to prevent ‘snap’ changes happening due to unusual short-term conditions.\nExample of smoothing:\nExample Integrity Metric (on Kafka) LM expects integrity metrics on the alm__integrity Kafka topic to have the following JSON form:\n{\r\u0026quot;metricKey\u0026quot; : \u0026quot;142971c5-a84b-4d34-af15-435ba8640aec\u0026quot;,\r\u0026quot;metricName\u0026quot; : \u0026quot;h_integrity\u0026quot;,\r“metricId” : \u0026quot;142971c5-a84b-4d34-af15-435ba8640aec.h_integrity\u0026quot;,\r\u0026quot;integrity\u0026quot; : \u0026quot;OK\u0026quot;,\r\u0026quot;message\u0026quot; : \u0026quot;Everything is working\u0026quot;\r}\r   Field Description Mandatory     metricKey The key provided when the resource was created Yes   metricName The name of the metric as defined in the resource descriptor No   metricId The unique ID for this metric value. Made from concatenating the metricKey and the metricName. This is used as the Kafka Key so all Resources must send this value so that multiple metrics with the same name can be supported correctly. Yes   integrity Allowed values: OK | BROKEN Equating to OK when the resource is healthy and passing its Integrity checks and BROKEN when the checks fail Yes   message An optional message to add value to the metric – useful in the event of BROKEN No    Example Load Metric (on Kafka) LM expects load metrics on the alm__load Kafka topic to have the following JSON form:\n{\r\u0026quot;metricKey\u0026quot; : \u0026quot;818127b3-1904-4737-a60c-8c7bab73532d\u0026quot;,\r\u0026quot;metricName\u0026quot; : \u0026quot;h_load\u0026quot;,\r“metricId” : \u0026quot;818127b3-1904-4737-a60c-8c7bab73532d.h_load\u0026quot;,\r\u0026quot;load\u0026quot; : 76,\r\u0026quot;message\u0026quot; : \u0026quot;Load is high\u0026quot;\r}\r   Field Description Mandatory     metricKey The key provided when the resource was created Yes   metricName The name of the metric as defined in the resource descriptor No   metricId The unique ID for this metric value. Made from concatenating the metricKey and the metricName. This is used as the Kafka Key so all Resources must send this value so that multiple metrics with the same name can be supported correctly. Yes   load A value between 0 and 100 (i.e. a percentage) indicating the level of the load a resource is experiencing. A higher value indicates a higher load Yes   message An optional test string to include information about the integrity of the resource. for example, it may include an error code No    Yaml Examples The examples included below show the c_balancer,c_streamer and the net_video resources.\nresource::net_video:1.0 This is a resource that creates a neutron network\nname: resource::net_video::1.0 description: resource to create an internal neutron network that includes a subnet\rproperties:\rsubnetCIDR:\r​ type: string ​ description: The subnet classless inter-domain routing\r​ default: '10.0.1.0/24'\rnetworkName:\r​ type: string\r​ description: Network Name\r​ value: VIDEO\rsubnetDefGwIp:\r​ type: string\r​ description: Default Gateway IP address\r​ default: '10.0.1.1'\rnetwork-id:\r​ type: string\r​ description: the id of the network just created\r​ read-only: true\rcapabilities:\r​ Network: ​ type: OS::Neutron::Net\rlifecycle:\r- Install\r- Uninstall\rA simple component with metrics and policies name: \u0026quot;resource::h_simple::1.0\u0026quot;\rdescription: \u0026quot;resource for t_simple\u0026quot;\rproperties:\rserver_name:\r​ type: \u0026quot;string\u0026quot;\r​ value: \u0026quot;${instance.name}\u0026quot;\rreferenced-internal-network:\r​ type: \u0026quot;string\u0026quot;\r​ description: \u0026quot;Generated to reference a network\u0026quot;\rreference-public-network:\r​ type: \u0026quot;string\u0026quot;\r​ description: \u0026quot;Generated to reference public network\u0026quot;\rimage:\r​ type: \u0026quot;string\u0026quot;\r​ description: \u0026quot;The Image reference\u0026quot;\rkey_name:\r​ type: \u0026quot;string\u0026quot;\r​ description: \u0026quot;SSH key\u0026quot;\rdata:\r​ type: \u0026quot;string\u0026quot;\r​ description: \u0026quot;parameter passed\u0026quot;\r​ default: \u0026quot;data\u0026quot;\rintegrity_publication_period:\r​ type: \u0026quot;string\u0026quot;\r​ description: \u0026quot;the period that should be used to publish the metrics\u0026quot;\r​ default: \u0026quot;60\u0026quot;\rpublication_period:\r​ type: \u0026quot;string\u0026quot;\r​ description: \u0026quot;the period that should be used to publish the metrics\u0026quot;\r​ default: \u0026quot;60\u0026quot;\rnumber-of-intervals:\r​ type: \u0026quot;string\u0026quot;\r​ description: \u0026quot;The intervals before calling a Heal\u0026quot;\r​ default: \u0026quot;3\u0026quot;\routput:\r​ type: \u0026quot;string\u0026quot;\r​ description: \u0026quot;an example output parameter\u0026quot;\r​ read-only: true\roperations:\rCreateRelationship1:\r​ description: \u0026quot;Create a new relationship\u0026quot;\r​ properties:\r​ source:\r​ type: \u0026quot;string\u0026quot;\r​ description: \u0026quot;that name of the source\u0026quot;\r​ target:\r​ type: \u0026quot;string\u0026quot;\r​ description: \u0026quot;that name of the target\u0026quot;\rCeaseRelationship1:\r​ description: \u0026quot;Cease an existing relationship\u0026quot;\r​ properties:\r​ source:\r​ type: \u0026quot;string\u0026quot;\r​ description: \u0026quot;that name of the source\u0026quot;\r​ target:\r​ type: \u0026quot;string\u0026quot;\r​ description: \u0026quot;that name of the target\u0026quot;\rCreateRelationshipr2:\r​ description: \u0026quot;Create a new relationship\u0026quot;\r​ properties:\r​ source:\r​ type: \u0026quot;string\u0026quot;\r​ description: \u0026quot;that name of the source\u0026quot;\r​ target:\r​ type: \u0026quot;string\u0026quot;\r​ description: \u0026quot;that name of the target\u0026quot;\rCeaseRelationship2:\r​ description: \u0026quot;Cease an existing relationship\u0026quot;\r​ properties:\r​ source:\r​ type: \u0026quot;string\u0026quot;\r​ description: \u0026quot;that name of the source\u0026quot;\r​ target:\r​ type: \u0026quot;string\u0026quot;\r​ description: \u0026quot;that name of the target\u0026quot;\rCreateRelationship3:\r​ description: \u0026quot;Create a new relationship\u0026quot;\r​ properties:\r​ source:\r​ type: \u0026quot;string\u0026quot;\r​ description: \u0026quot;that name of the source\u0026quot;\r​ target:\r​ type: \u0026quot;string\u0026quot;\r​ description: \u0026quot;that name of the target\u0026quot;\rCeaseRelationship3:\r​ description: \u0026quot;Cease an existing relationship\u0026quot;\r​ properties:\r​ source:\r​ type: \u0026quot;string\u0026quot;\r​ description: \u0026quot;that name of the source\u0026quot;\r​ target:\r​ type: \u0026quot;string\u0026quot;\r​ description: \u0026quot;that name of the target\u0026quot;\rmetrics:\rh_integrity:\r​ type: \u0026quot;metric::integrity\u0026quot;\r​ publication-period: \u0026quot;${integrity_publication_period}\u0026quot;\rload:\r​ type: \u0026quot;metric::load\u0026quot;\rpolicies:\rheal:\r​ metric: \u0026quot;h_integrity\u0026quot;\r​ type: \u0026quot;policy::heal\u0026quot;\r​ properties:\r​ smoothing:\r​ value: \u0026quot;${number-of-intervals}\u0026quot;\rlifecycle\r- \u0026quot;Configure\u0026quot;\r- \u0026quot;Install\u0026quot;\r- \u0026quot;Integrity\u0026quot;\r- \u0026quot;Start\u0026quot;\r- \u0026quot;Stop\u0026quot;\r- \u0026quot;Reconfigure\u0026quot;\r- \u0026quot;Uninstall\u0026quot;\rresource::c_streamer::1.0 This descriptor will create a virtual server that streams video traffic using the http protocol.\nname: resource::c_streamer::1.0\rdescription: resource descriptor for c_streamer\rproperties:\rkey_name:\r​ type: string\r​ required: true\r​ description: the SSH key-pair name to be used by OpenStack with the associated VM instances\rreferenced-management-network:\r​ type: string\r​ required: true\r​ description: The id of the network that will act in the role of a management network\rflavor:\r​ type: string\r​ required: true\r​ description: Flavor to be used for compute instance\rserver_name:\r​ type: string\r​ required: true\r​ description: the name of the server to be created\rreferenced-video-network:\r​ type: string\r​ description: The id of the network that will act in the role of an internal network\ravailability_zone:\r​ type: string\r​ description: Name of availability zone in which to create the instance\r​ default: DMZ\rprivateIp:\r​ type: string\r​ description: IpAddress of server on the internal network\r​ read-only: true mgmtIp:\r​ type: string\r​ description: IpAddress of server on the management network\r​ read-only: true integrity_publication_period:\r​ type: string\r​ description: the number of seconds between publishing integrity metric\r​ default: 60\rnumber-of-intervals:\r​ type: string\r​ description: the number of intervals for smoothing\r​ default: 3\rcapabilities:\r​ VideoStream:\r​ type: httpStreamOutput\rrequirements:\r​ VideoNetwork:\r​ type: neutronNetwork\r​ ManagementNetwork:\r​ type: neutronNetwork\rRemoteNFSMountPoint:\r​ type: nfsExportMountpoint\rlifecycle:\r- Install\r- Uninstall\r- Configure\r- Start\r- Stop\r- Integrity\roperations:\rMountStorage:\r​ description: An operation to enable the streamer to mount a remote NFS mount point\r​ properties:\r​ remote_nfs_port:\r​ type: string\r​ description: Port for the NFS\r​ default: '2049'\r​ remote_nfs_server_ip:\r​ type: string\r​ description: Ip Address of remote nfs server\r​ remote_mount_point:\r​ type: string\r​ description: Location of NFS Exported Mount Point\r​ default: /\r​ local_mount_point:\r​ type: string\r​ description: The location where the remote nfs mount will be mounted in the local machine\r​ default: /mnt\rUnmountStorage:\r​ description: An operation to unmount a remote NFS mount point\r​ properties:\r​ local_mount_point:\r​ type: string\r​ description: The location where the remote nfs mount will be mounted in the local machine\r​ default: /mnt\rresource::c_balancer::1.0 name: resource::c_balancer::1.0\rdescription: component package for a http loadbalancer\rproperties:\rkey_name:\r​ type: string\r​ description: SSH key_name.\rreferenced-management-network:\r​ type: string\r​ description: Generated to reference a network\rreferenced-internal-network:\r​ type: string\r​ description: Generated to reference a network referenced-public-network:\r​ type: string\r​ description: Generated to reference a network\rflavor:\r​ type: string\r​ description: Flavor to be used for compute instance\rserver_name:\r​ type: string\r​ description: server name of the balancer\ravailability_zone:\r​ type: string\r​ description: Name of availability zone in which to create the instance\r​ default: DMZ\rmgmtIp:\r​ type: string\r​ description: IpAddress of server in management network\r​ readOnly: true\rinternalIp:\r​ type: string\r​ description: IpAddress of server on internal network\r​ readOnly: true publicIp:\r​ type: string\r​ description: Public IpAddress of server\r​ readOnly: true integrity_publication_period:\r​ type: string\r​ description: the number of seconds between publishing integrity metric\r​ default: 60\rnumber-of-intervals:\r​ type: string\r​ description: the number of intervals for smoothing\r​ default: 3 capabilities:\r​ HttpLoadBalancer:\r​ type: loadbalancerHttp\rrequirements:\r​ PublicNetwork:\r​ type: neutronNetwork\r​ ManagementNetwork:\r​ type: neutronNetwork\r​ HttpServer:\r​ type: http\rlifecycle:\r- Install - Uninstall - Start\r- Stop operations:\rRemoveBalancedHttpServer:\r​ description: removes the http server from being managed by the balancer\r​ properties:\r​ server_ip:\r​ type: string\r​ description: Http Server Ip Address\r​ default: the ip address\r​ server_port:\r​ type: string\r​ description: http server port number ​ default: '8080'\rAddBalancedHttpServer:\r​ description: adds an http server to the balancer's pool\r​ properties:\r​ max_connections:\r​ type: string\r​ description: Maximum connections for the balanced server\r​ default: 3\r​ server_ip:\r​ type: string\r​ description: Ip Address of the server to be balanced\r​ server_port:\r​ type: string\r​ description: Port on balanced server\r​ default: '8080'\rMetrics in the Resource Descriptor Each resource may emit metric information to help in its management. ALM is expecting all collected metrics from resources to be made available on Kafka. ALM listens to specific Kafka topics for events containing metrics. Metrics should be associated with necessary identifiers including timestamps, names of the metric and “Metric Identifiers” specifying the source resource of the metrics.\nMetrics for Integrity and Load are defined in the resource Descriptor. The Integrity metric is used to heal a broken resource. The Load metric is used in the VNF or Network Service to scale a VNF:\nmetrics:\rlb_integrity:\rtype: metric::integrity\rpublication-period: ${integrity_publication_period}\rlb_load:\rtype: metric::load\rpublication-period: ${load_publication_period}\rHeal Policy in the Resource Descriptor The Integrity or Heal policy in the resource Descriptor can be seen in the example below:\nmetrics:\rh_integrity:\rtype: \u0026quot;metric::integrity\u0026quot;\rpublication-period: \u0026quot;10\u0026quot;\rpolicies:\rheal:\rmetric: \u0026quot;h_integrity\u0026quot;\rtype: \u0026quot;policy::heal\u0026quot;\rproperties:\rsmoothing:\rvalue: ${integrity_smoothing}\rThis shows the metrics section in the resource Descriptor that defines the metric called h_integrity. For the Integrity metric a policy is also included that defines the number of smoothing periods that will pass before the resource will be either marked as BROKEN when the BROKEN integrity messages have sent, or if the resource is missing.\nAn Example of a Resource Descriptor Below is an example of an LM resource descriptor:\nname: resource::simple_resource::1.0.0\rdescription: An example resource\rproperties:\rdevice_id:\rdescription: The ID of the Device\rtype: string\rvalue: ${instance.id}\rdevice_name:\rdescription: The name of the Device\rtype: string\rrequired: true\rmgmt_network:\rdescription: The management network used to manage the device\rtype: string\rdefault: mgmt\rmgmt_address:\rdescription: The management network ip address of the the device\rtype: string\rread-only: true\rinstance_name:\rvalue: ${instance.name}\rlifecycle:\r- Install\r- Configure\r- Start\r- Integrity\r- Stop\r- Uninstall\roperations:\raddRule:\rproperties:\rmgmt_address:\rtype: string\rdescription: the ip_address of the instance\rvalue: ${mgmt_address}\rrule_name:\rtype: string\rdescription: The name of the rule\rrequired: true\rrule_type:\rtype: string\rdescription: The type of rule\rdefault: Other\rrequired: true\rdeleteRule:\rproperties:\rmgmt_address:\rtype: string\rdescription: the ip_address of the instance\rvalue: ${mgmt_address}\rrule_name:\rtype: string\rdescription: The name of the rule\rrequired: true\rrule_type:\rtype: string\rdescription: The type of rule\rdefault: Other\rrequired: true\raddNetworkInterface:\rproperties:\rnetwork_name:\rtype: string\rdescription: The name of the network\rrequired: true\rinterface_name:\rtype: string\rdescription: the name of the vnic to be used\rdefault: vnic0\rdevice_name:\rtype: string\rdescription: The name of this device\rrequired: true deleteNetworkInterface:\rproperties:\rnetwork_name:\rtype: string\rdescription: The name of the network\rrequired: true\rinterface_name:\rtype: string\rdescription: the name of the vnic to be used\rdefault: vnic0\rdevice_name:\rtype: string\rdescription: The name of this device\rrequired: true\rmetrics:\rh_load:\rtype: metric::load\rpublication_period: 60\rh_integrity:\rtype: \u0026quot;metric::integrity\u0026quot;\rpublication-period: \u0026quot;10\u0026quot;\rpolicies:\rheal:\rmetric: \u0026quot;h_integrity\u0026quot;\rtype: \u0026quot;policy::heal\u0026quot;\rproperties:\rsmoothing:\rvalue: ${integrity_smoothing}\r"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/resource-engineering/","title":"Resource Engineering","tags":[],"description":"","content":""},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/administration/configuration/themes/theme-installation/","title":"Theme Installation","tags":[],"description":"","content":"The following steps explain how to alter the installation process to change the theme used by the Agile Lifecycle Manager (ALM) User Interface.\nInstalling a Theme   Follow the steps in theme overview before doing the following.\n  When installing the lm-configurator helm chart you must include the following values, changing the value of theme.name to be the name of theme to be used:\n   configurator:\rlmConfigImport:\rnimrod:\ralm.nimrod.theme.name: mytheme\rWhen installing the lm-helm helm chart you must include the following values so the ConfigMap with theme.tar can be loaded by LM:   nimrod:\rapp:\rconfig:\rthemesConfigMap: lm-themes\rOnce LM has started you will be able to view the UI with your chosen theme.  "},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/","title":"User Guides","tags":[],"description":"","content":""},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/installation/cicdhub/sizing/","title":"Sizing","tags":[],"description":"","content":"CPU and Memory The CPU and memory limits of each service may be configured by adding resource requests and limit values to the custom values file. See Kubernetes - Manage Compute Resources Container for more information.\nThe default values for each service are shown below. Override any defaults by adding them to your custom values. Blank fields represent deployments without a default value, which will result in no limits being imposed on their usage.\ndockerregistry:\rresources:\rlimits:\rcpu:\rmemory:\rrequests:\rcpu:\rmemory:\rgogs:\rresources:\rrequests:\rcpu:\rmemory:\rlimits:\rcpu:\rmemory:\rpostgresql:\rresources:\rrequests:\rcpu: 100m\rmemory: 256Mi\rlimits:\rcpu:\rmemory: jenkins:\rmaster:\rresources:\rrequests:\rcpu:\rmemory:\rlimits:\rcpu:\rmemory:\rnginx-ingress:\rcontroller:\rresources:\rrequests:\rcpu:\rmemory:\rlimits:\rcpu:\rmemory:\rnexus:\rresources:\rrequests:\rcpu:\rmemory:\rlimits:\rcpu:\rmemory:\ropenldap:\rresources:\rrequests:\rcpu:\rmemory:\rlimits:\rcpu:\rmemory:\rNext Steps Continue configuring your installation with Storage\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/installation/lm/production/configuration/access-config/","title":"Access Configuration","tags":[],"description":"","content":"Configuring Access to Agile Lifecycle Manager (ALM) The installation of ALM includes an Ingress Controller which exposes the key services externally. This needs to be configured so that access to these services will be on the required hostnames and ports.\nIf your environment already has an Ingress Controller available, then disable the one that comes with the LM installation. You can check the status of your ingress controller with:\nkubectl get service --all-namespaces | grep ingress\nOtherwise proceed to Configuring the Ingress Controller\nDisabling the Ingress Controller To use an existing Ingress Controller, disable the one that comes with the LM installation by adding a custom Helm value as follows:\nnginx-ingress: enabled: false Now proceed to Configuring the Ingress Hostnames\nConfiguring the Ingress Controller By default, the installation of the controller is configured as follows:\n   Protocol Port     HTTPS 32443   HTTP 32080    Configuring the Access Port The Ingress port value, on which LM is exposed externally, can be set via the Helm values for the helm-foundation chart. The value should be set as follows:\nnginx-ingress: controller: service: nodePorts: http: 32080 https: 32443 Configuring SSL Certificate By default, a \u0026ldquo;dummy\u0026rdquo; certificate is generated by the Ingress controller. It is recommended that you generate and provide your own SSL certificate, with a valid common name (CN) value matching the host you will access the controller with.\nYou can do this by first generating a certificate:\nopenssl req -newkey rsa:2048 -nodes -keyout ingress.key -x509 -days 3650 -out ingress.cer -subj \u0026quot;/CN=\u0026lt;your-host-ip-or-host-name\u0026gt;\u0026quot;\rCreate a secret from the .key and .cer file produced:\nkubectl create secret tls ingress-tls --key ingress.key --cert ingress.cer -n \u0026lt;target-install-namespace\u0026gt;\rNow add the following to your Helm values file:\nnginx-ingress: controller: extraArgs: default-ssl-certificate: \u0026#34;\u0026lt;target-install-namespace\u0026gt;/ingress-tls\u0026#34; For example, if installing LM into the default namespace, you would add:\nnginx-ingress: controller: extraArgs: default-ssl-certificate: \u0026#34;default/ingress-tls\u0026#34; Configuring the Ingress Rules The rules applied to the Ingress controller decide how the LM services can be accessed.\nHostname rule By default, a rule is created to access the UI and API through configurable hostnames. The default values for these rules are shown below:\nglobal: lm: extAccess: ui: host: ## Enable for an Ingress rule to be created which supports access through http(s)://\u0026lt;hostname\u0026gt;:\u0026lt;ingress-port\u0026gt;/ui ingressEnabled: true ## The Hostname used on the Ingress rule name: ui.lm ## Set to true if lm-configurator should generate the SSL certificate generateCert: true ## The Common Name used when generating the SSL certificate (if security is enabled)  commonName: ui.lm ## Name of the secret to create containing the SSL certificate for the host based access certSecretName: nimrod-host-tls api: host: ## Enable for an Ingress rule to be created which supports access through http(s)://\u0026lt;hostname\u0026gt;:\u0026lt;ingress-port\u0026gt; ingressEnabled: true ## The Hostname used on the Ingress rule name: app.lm ## Set to true if lm-configurator should generate the SSL certificate generateCert: true ## The Common Name used when generating the SSL certificate (if security is enabled)  commonName: app.lm ## Name of the secret to create containing the SSL certificate for the host based access certSecretName: ishtar-host-tls This will allow access to LM at:\n   Service Secure Endpoint Insecure Endpoint     UI https://ui.lm:32443/ui http://ui.lm:32080/ui   API https://app.lm:32443 http://app.lm:32080    Disable this rule for each service by setting the ingressEnabled and generateCert properties to false.\nNo Host Rule By default, an additional rule is created to access the UI and API through the IP address of the Kubernetes environment. The default values for these rules are shown below:\nglobal: lm: extAccess: ui: noHost: ## Enable for an Ingress rule to be created without a host specified, allowing IP address access http(s)://\u0026lt;Kubernetes-IP\u0026gt;:\u0026lt;ingress-port\u0026gt;/ui ingressEnabled: true ## Set to true if lm-configurator should generate the SSL certificate generateCert: true ## The Common Name used when generating the SSL certificate (if security is enabled)  ## The default value here is acceptable but will not produce a completely valid certificate ## You should update this value to the IP address used to access the UI on this rule commonName: ui.lm ## Name of the secret to create containing the SSL certificate for non-host based access certSecretName: nimrod-nohost-tls api: noHost: ## Enable for an Ingress rule to be created without a host specified, allowing IP address access through http(s)://\u0026lt;Kubernetes-IP\u0026gt;:\u0026lt;ingress-port\u0026gt;/\u0026lt;path\u0026gt; ingressEnabled: true ## Subpath to access the API on. Leave empty to access the API on the root path of the Ingress host (http(s)://\u0026lt;Kubernetes-IP\u0026gt;:\u0026lt;ingress-port\u0026gt;) path: ## Set to true if lm-configurator should generate the SSL certificate generateCert: true ## The Common Name used when generating the SSL certificate (if security is enabled)  ## The default value here is acceptable but will not produce a completely valid certificate ## You should update this value to the IP address used to access the API on this rule commonName: app.lm ## Name of the secret to create containing the SSL certificate for non-host based access certSecretName: ishtar-nohost-tls This will allow access to LM at:\n   Service Secure Endpoint Insecure Endpoint     UI https://:32443/ui http://:32080/ui   API https://:32443 http://:32080    You should configure the commonName value to the Kubernetes IP to produce a valid SSL certificate.\nVault An Ingress rule is created to allow access to Vault (configuration repository) UI.\nBy default, Vault is accessible at https://vault.lm:32443. To configure the Vault hostname you should include the following configuration values and modify the vault.lm:\nvault: ingress: hosts: - vault.lm tls: - secretName: vault-tls hosts: - vault.lm Kibana An Ingress rule is created to allow access to Kibana, the logging dashboard service provided with LM.\nBy default, Kibana is accessible at https://kibana.lm:32080 or via NodePort at http://\u0026lt;Kubernetes IP\u0026gt;:31001. To configure the Kibana hostname or NodePort you should include the following configuration values and modify them:\nkibana: service: type: NodePort nodePort: 31001 ingress: hosts: - kibana.lm If you update the values above then you also need to update the logging-dashboard configuration, so the LM UI may include hyperlinks to the dashboard when an error has occurred. Set the endpoint to your new hostname or set it the Kubernetes IP (and change the port to the NodePort) if preferred.\nconfigurator: ## Logging dashboard config loggingDashboard: ## Enable logging dashboard feature in UI enabled: true ## Endpoint of the chosen logging dashboard endpoint: http://kibana.lm:32080 NodePort access:\nconfigurator: ## Logging dashboard config loggingDashboard: ## Enable logging dashboard feature in UI enabled: true ## Endpoint of the chosen logging dashboard endpoint: http://\u0026lt;Kubernetes IP\u0026gt;:31001 Next steps You can now proceed to review the configuration for Sizing\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/installation/resource-manager/ansible-rm/","title":"Ansible-RM","tags":[],"description":"","content":""},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/resource-engineering/resource-packages/brent/","title":"Brent Resource Packages","tags":[],"description":"","content":""},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/cicd/","title":"CI/CD","tags":[],"description":"","content":""},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/installation/lm/production/configuration/","title":"Configuration","tags":[],"description":"","content":""},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/cicd/configure-cicd-hub/","title":"Configure the Hub","tags":[],"description":"","content":"Objectives A newly installed CI/CD Hub will require some additional configuration to setup users and cross component integrations.\nPre-requisites  Newly installed CI/CD Hub See initial CI/CD Hub login details  Add users to OpenLDAP To add users to the to the OpenLDAP installed with the CI/CD Hub. You can follow the instructions to add users to a standalone Agile Lifecycle Manager (ALM).\nSlave ALMs can be configured at installation time to use the OpenLDAP instance deployed with the CI/CD Hub. Users configured in the CI/CD OpenLDAP will then have access to any slave LMs.\nDocumentation for Jenkins, Gogs and Nexus can show how to configure those functions to also use the shared OpenLDAP instance. These instructions are outside the scope of this learning center.\nConfigure Jenkins The next step is to configure Jenkins and its required plugins.\nInstall Plugins The following plugins are required:\n Pipeline Gogs  To install these plugins follow the steps below:\n Log into Jenkins On Jenkins dashboard select \u0026lsquo;Manage Jenkins\u0026rsquo; Select \u0026lsquo;Manage Plugins\u0026rsquo; Check each plugin is listed under the \u0026lsquo;installed\u0026rsquo; tab. If a plugin is not installed, search for it under the \u0026lsquo;Available\u0026rsquo; tab and select it to install it. Check the \u0026lsquo;Restart Jenkins when installation is complete and no jobs are running\u0026rsquo; link Wait for Jenkins to restart  Add Jenkins credentials Jenkins credentials are required to add tags to git and upload files to Nexus. Perform the following steps to setup the credentials.\n Log into the Jenkins UI On the Jenkins dashboard select \u0026lsquo;Credentials\u0026rsquo; In the row for \u0026lsquo;Jenkins\u0026rsquo; hover over (global) and in the drop list select \u0026lsquo;Add Credentials\u0026rsquo; Add the username \u0026lsquo;jenkins\u0026rsquo; and a password to log into Gogs and add a note in the description that these are the gogs credentials. Enter \u0026lsquo;gogs-id\u0026rsquo; in the \u0026lsquo;ID\u0026rsquo; field Press OK Repeat for nexus, using \u0026lsquo;nexus-id\u0026rsquo; as the \u0026lsquo;ID\u0026rsquo; value and a valid username and password that has permission to upload (for demo use the admin credentials)  Configuring an LMCTL Jenkins slave Choose LMCTL JNLP Image Select a version of the lmctl-jnlp-slave image from Docker Hub.\nCreate LMCTL Config Map  Create a new configmap in Kubernetes with kubectl, providing the the LMCTL configuration file as data (you will need to remember the file name for your LMCTL configuration file when configuring the slave):  kubectl create configmap lmctl-slave-config -n \u0026lt;cicdhub-namespace\u0026gt; --from-file \u0026lt;your-lmctl-config-file\u0026gt;\rAdd Template to Jenkins   Navigate to the cloud settings in the Jenkins UI (Manage Jenkins-\u0026gt;Configure System-\u0026gt;Kubernetes-\u0026gt;Images)\n  Find the existing Pod Template named default.\n  Modify the name of the template to lmctl-slave.\n  Add lmctl as a label (leave a space after any existing labels)\n  Modify the Docker image value of the Contain template to the name of the LMCTL JNLP image you selected e.g. accanto/lmctl-jnlp-slave:2.0.6\n  Under EnvVars use Add Environment Variable to add a new environment variable with:\n key: LMCONFIG value: /lmctl-data/\u0026lt;name of the LMCTL configuration file used earlier\u0026gt;    Under Volumes use Add Volume to add a new Config Map Volume with:\n Config Map name: lmctl-slave-config Mount path: /lmctl-data    Save the template\n  An example of this configuration can be seen below: The template is now configured to create a new pod for any Jenkins job which requests an agent with the lmctl label.\nChanging configuration of an existing LMCTL Jenkins slave To update the LMCTL configuration used on any slave pods you need to re-create the configmap with the updated LMCTL configuration file:\nkubectl create configmap lmctl-slave-config -n lm --from-file \u0026lt;your-updated-lmctl-config-file\u0026gt; -n lm\rConfigure Additional Slaves There are very few use cases for configuring additional LMCTL slave templates to Jenkins as the existing template will be used any time a slave with LMCTL is required.\nIf you need to use different Agile Lifecycle Manager (ALM) environments in each Jenkins job, you can update the configmap with an updated LMCTL configuration file featuring all of the potential environments.\nIt may be reasonable to configure an additional slave if you need an alternative version of LMCTL (or want to try out a new version without losing your old slave template). If this is the case, follow the Configure Slave instructions for the additional template, changing the names of the configmap and JNLP template name to a unique value.\nConfigure Gogs The following Gogs configuration should be performed.\nRegister users  Log into Gogs as admin From the top right drop down list titled \u0026lsquo;SIGNED IN AS CICDHUB-ADMIN\u0026rsquo; click and select \u0026lsquo;Admin Panel\u0026rsquo; Go to Users and add user \u0026lsquo;jenkins\u0026rsquo; (with same password as set in the Jenkins credentials step above) . This account is used for Jenkins to push changes to Gogs projects. Give the jenkins user admin permissions Add any additional Gogs users who will be creating and maintaining Network Services and VNFs. New users can change their password later. Register on the home page (before you sign in)  Create an Organization The following steps creates an organization which will contain Network Service and VNF projects for your team.\n Log into Gogs as admin Create (+) New Organization name \u0026lsquo;marketplace\u0026rsquo;. This is where Network Services and VNF projects will be created. If you use a different name, then you will have to edit your Jenkinsfile for each of the pipeline automation\u0026rsquo;s you create later. \u0026lsquo;Invite Someone\u0026rsquo; to join the organization for all the users you created. You can add/delete users later and add/remove them from the organization. Add \u0026lsquo;jenkins\u0026rsquo; to the \u0026lsquo;marketplace\u0026rsquo; organizations.  Configure Nexus The following Nexus configuration should be performed to create an initial repository for storing NS/VNF packages built through the CI/CD Hub pipeline:\n Log in as Nexus admin Click settings (the cog icon in the top navigation bar) Select repositories from the left side menu Click Create repository button at the top of main pane Select raw (hosted) from the list of recipes Enter a name e.g. raw Click Create repository at the bottom of the screen to finish creating the repository  "},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/resource-engineering/resource-packages/brent/infrastructure-keys-resource/","title":"Infrastructure Keys Example","tags":[],"description":"","content":""},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/reference/behaviour-testing/step-reference/intent-requests/","title":"Intent Requests","tags":[],"description":"","content":"The steps in this group can be used to request processes to be executed on Assemblies. The difference between these steps and those in the Intent Engine group is the asynchronous execution. These steps only request the process to start, they do not wait for it to complete. The successful completion of the process can be checked for later in the scenario with the provided \u0026ldquo;Expect Intent Success\u0026rdquo; step.\nCreate Assembly Description Requests the creation of an Assembly with the given descriptor and initial state, progressing when the request is accepted.\nPasses when:\n the create was accepted  Fails when:\n the create was rejected (not a valid request)  Properties    Property Description     assemblyName The chosen name of the Assembly to be created   descriptorName name of the Assembly Descriptor to create an instance from   initialState the state to create the Assembly in: Installed, Inactive or Active    Create Assembly with properties Description Requests the creation of an Assembly with the given descriptor and initial state, progressing when the request is accepted. This step includes a parameter to set the property values from the descriptor this instance is being created from.\nPasses when:\n the create was accepted  Fails when:\n the create was rejected (not a valid request)  Properties    Property Description     assemblyName The chosen name of the Assembly to be created   descriptorName Name of the Assembly Descriptor to create an instance from   initialState The state to create the Assembly in: Installed, Inactive or Active   assemblyProperties Properties to use in the creation of the Assembly instance    Uninstall Assembly Description Requests the uninstall of a given Assembly, progressing when the request is accepted.\nPasses when:\n the uninstall was accepted  Fails when:\n the uninstall was rejected (not a valid request)  Properties    Property Description     assemblyName The name of the Assembly to uninstall. This Assembly may have been created by the scenario or existed previously. If using an \u0026ldquo;Existing Provided\u0026rdquo; assembly configuration then you may use the reference name you chose when adding it to the scenario    Change Assembly State Description Requests to start a process to change the state of a given Assembly, progressing when the request is accepted.\nPasses when:\n the change state request was accepted  Fails when:\n the change state request was rejected (not a valid request)  Properties    Property Description     assemblyName The name of the Assembly to change state. This Assembly may have been created by the scenario or existed previously. If using an \u0026ldquo;Existing Provided\u0026rdquo; assembly configuration then you may use the reference name you chose when adding it to the scenario   newState The target state of the Assembly    Scale Out Cluster Description Requests to start a process to scale out a cluster belonging to the given Assembly, progressing when the request is accepted.\nPasses when:\n the scale out request was accepted  Fails when:\n the scale out request was rejected (not a valid request)  Properties    Property Description     clusterName Name of the Cluster instance in the Assembly to scale   assemblyName The name of the Assembly instance. This Assembly may have been created by the scenario or existed previously. If using an \u0026ldquo;Existing Provided\u0026rdquo; assembly configuration then you may use the reference name you chose when adding it to the scenario    Scale In Cluster Description Requests to start a process to scale in a cluster belonging to the given Assembly, progressing when the request is accepted.\nPasses when:\n the scale in request was accepted  Fails when:\n the scale in request was rejected (not a valid request)  Properties    Property Description     clusterName Name of the Cluster instance in the Assembly to scale   assemblyName The name of the Assembly instance. This Assembly may have been created by the scenario or existed previously. If using an \u0026ldquo;Existing Provided\u0026rdquo; assembly configuration then you may use the reference name you chose when adding it to the scenario    Heal Resource Description Requests to start a process to heal a Resource belonging to the given Assembly, progressing when the request is accepted.\nPasses when:\n the heal request was accepted  Fails when:\n the heal request was rejected (not a valid request)  Properties    Property Description     brokenComponentName Name of the Component instance in the Assembly to heal   assemblyName The name of the Assembly instance. This Assembly may have been created by the scenario or existed previously. If using an \u0026ldquo;Existing Provided\u0026rdquo; assembly configuration then you may use the reference name you chose when adding it to the scenario    Upgrade Assembly Description Requests to start a process to upgrade the given Assembly, progressing when the request is accepted. You may upgrade the descriptor and/or property values.\nPasses when:\n  the upgrade request was accepted Fails when:\n  the upgrade request was rejected (not a valid request)\n  Properties    Property Description     assemblyName The name of the Assembly instance to be upgraded. This Assembly may have been created by the scenario or existed previously. If using an \u0026ldquo;Existing Provided\u0026rdquo; assembly configuration then you may use the reference name you chose when adding it to the scenario   descriptorName Name of the Assembly Descriptor to upgrade to (set to the existing descriptor name to only make property updates)   properties Properties to use in the upgrade of the Assembly instance    Upgrade Assembly Descriptor Description Requests to start a process to upgrade the given Assembly to a new descriptor, progressing when the request is accepted.\nPasses when:\n the upgrade request was accepted  Fails when:\n the upgrade request was rejected (not a valid request)  Properties    Property Description     assemblyName The name of the Assembly instance to be upgraded. This Assembly may have been created by the scenario or existed previously. If using an \u0026ldquo;Existing Provided\u0026rdquo; assembly configuration then you may use the reference name you chose when adding it to the scenario   descriptorName Name of the Assembly Descriptor to upgrade to    Expect Intent Success Description Check the last process (intent), requested by this scenario, executed on a given Assembly has completed successfully.\nPasses when:\n the Assembly is known by this scenario and has a process known by this scenario. The process completed successfully  Fails when:\n the Assembly is not known by this scenario the Assembly has no process known by this scenario the process completed unsuccessfully  Properties    Property Description     assemblyName The name of the Assembly instance. If using an \u0026ldquo;Existing Provided\u0026rdquo; assembly configuration then you may use the reference name you chose when adding it to the scenario    "},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/key-concepts/","title":"Key Concepts","tags":[],"description":"","content":"Objectives This section introduces key concepts that are useful to understand how Agile Lifecycle Manager (ALM) works and the key drivers behind of the design.\nPrerequisites It is assumed that the reader has basic understanding of concepts around virtualization, NFV, and DevOps.\nTo learn about NFV basic concepts read: NFV Basics guide (Coming Soon)\nUnified Lifecycle Model Standard lifecycle model is a key enabling factor for intent based automation. In order to drive automation, ALM adopted a standardized lifecycle model which mandates all the underlying artifacts (Network Services and VNFs) look the same to ALM. Each artifact of a Network Service will be expected to go through the same lifecycle with defined lifecycle transitions as shown below.\nEach artifact in a Network Service is optimally stepped through this lifecycle concurrently with others accounting for the declaratively identified dependencies. For example, on installation each artifact will be moved to the ‘Installed’ state together before they are moved to the Inactive state. Only the lowest level artifacts (VNFC) have transition steps defined that will be performed as the Network Service progresses through the lifecycle.\nIntent Based Automation Intent Based Automation is comparable to a satellite navigation system. In order to go from A to B, you only need to enter the destination and the Sat Nav will figure out the best way to get there. You don’t need to manually program each turn. If there is a roadblock or a traffic jam, the engine will figure out the best alternative route without you having to manually re-program the route.\nALM works in a similar way. Rather than programming all individual lifecycles for all possible lifecycle scenarios such as upgrade and migrate, the engine automatically generates and executes all lifecycles for VNF and Network Services.\nThe Intent Engine (one of the ALM core microservices) in the heart of ALM has the responsibility for driving Network Service (NS) instances through their lifecycles according to intent requests it receives. With support from other ALM services, like catalog and topology, intent engine resolves the necessary network service level actions, breaks them down to VNF and eventually to VNFC level lifecycle transitions, and figures out the correct order to execute the steps. The exact sequence of execution steps depends on the model of the network service, the current state of the service, and the requested target state and shape of the service. Finally, the Intent Engine pushes the corresponding requests to responsible resource managers to complete through LM\u0026rsquo;s southbound APIs.\nAlso, the detailed execution plans are stored by ALM to enable real-time monitoring of the progress of currently active intents as well as browsing the intent history for a specific network service instance. The following is a screenshot illustrating an intent execution on the ALM GUI.\nIntents can be requested through a set of API endpoints on the ALM northbound API. Requests can be triggered by an external northbound system, by LM itself for policy driven intents (such as auto-healing or scaling for example) or they can be manually requested through the LM User Interface which then calls the corresponding API end point.\nBehaviour Driven Testing ALM has in-built Behaviour Driven Testing capability functioning as an automated test framework for VNF and Network Service (NS) testing. While Intent Based Automation automates the production lifecycle of your Network Services and associated resources, Behaviour Driven Testing, together with CI/CD Hub, helps you to automate your service development processes. Automated Behaviour Driven Testing reduces manual effort and errors in production.\nIn development and pre-production phase, LM\u0026rsquo;s Behaviour Driven Testing can automatically spin up test environments, Network Service under test, and additional test VNFs such as probes and traffic generators, and run through full test scenario of all lifecycle states.\nWhile services are deployed in production environment, Ready for Service testing can be invoked automatically so that the Network Service is first tested in full, before being released to the customer.\nBelow screenshot shows a sample view of behaviour driven test scenario designer where testing sequence can defined by adding and configuring the individual steps.\nOnes designed, the test scenarios will be part of the VNF- or Network Service Package. Test scenarios can be either run directly from ALM’s user interface or triggered through its APIs enabling full automation and integration to CI/CD pipelines. Setting up CI/CD Process.\nCI/CD Hub Cloud DevOps best practices and principles are at the heart of the ALM solution. To scale any Cloud based networking program, a unified operations and engineering model should be combined with a set of automation tools that can simplify and automate the complexities of an end-to-end VNF or Network Service lifecycle.\nThe ALM CI/CD tools and processes are designed to simplify and automate the following DevOps tasks:\n Onboard VNFs and design Network Services: Quickly integrate and package the lifecycle actions required to operate a VNF or any virtual or physical network appliance. Behaviour Testing in pre-production: Deploy VNFs to pre-production environments and easily script complex operational behaviour tests to ensure the onboarded VNF behaves as expected in all \u0026ldquo;day 1\u0026rdquo; or \u0026ldquo;day 2\u0026rdquo; lifecycle tasks. Deploy to production: Once fully tested, auto deploy to production environments. Monitor and Change: Monitor and sense environmental or VNF state and auto scale, heal or move components of the network service. Report and Resolve issues: Errors in lifecycle actions or VNF software found in production are reported and trigger an upgrade process that rebuilds a new version of the VNF.  To learn more about CI/CD Hub and applying DevOps processes on Network Service and VNF lifecycle management read: Introduction to CI/CD Hub.\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/administration/configuration/languages/language-running/","title":"Language Updates","tags":[],"description":"","content":"The following steps explain how to change the language files used by the Agile Lifecycle Manager (ALM) User Interface when it is running.\nUpdating a Locale   Follow the steps in translation overview before doing the following.\n  Add the following configuration to vault for lm/nimrod, changing the value of alm.nimrod.localeLocation to be the name of the theme to be used:\n  \u0026quot;alm.nimrod.localeLocation\u0026quot;: \u0026quot;/var/lm/locales\u0026quot;\rCreate a values file with the following configuration:   nimrod:\rapp:\rconfig:\rlocalesConfigMap: lm-locales\rRun a helm upgrade of the ALM helm chart with the custom values file above.  "},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/installation/lm/","title":"Lifecycle Manager","tags":[],"description":"","content":""},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/administration/configuration/manage-rm-timeout-values/","title":"Manage RM Timeout values","tags":[],"description":"","content":"Introduction Agile Lifecycle Manager (ALM) can communicate with a number of Resource Managers (RM) when executing the set of tasks of an intent. This communication is through the OSSLM API. There are three timers and three associated timer configurations values which govern this relationship.\n alm.http.clientReadTimeout alm.daytona.resource-manager.default-timeout-duration alm.daytona.resource-manager.polling-interval     Setting Default Setting Description     alm.http.clientReadTimeout 30s Defaulting to 30 seconds, clientReadTimeout represents the time that the Intent Engine will wait for a response to the task execution request from the RM   alm.daytona.resource-manager.default-timeout-duration 15m The time, once the request has been accepted by the resource manager, that the Intent Engine will give the RM to complete the task before failing it   alm.daytona.resource-manager.polling-interval 15s Once an intent execution request has been positively acknowledged by the resource manager, the Intent Engine will poll the RM at this interval for polling-interval to determine if the task has completed    IntentEng-ResourceMgr-timers\nOnce a request has been accepted by the RM, ALM gives it a finite amount of time to complete the task. During this time, the Intent Engine will poll the RM at intervals to determine if the task has been completed. Once the RM responds to the poll indicating that the Task has completed (be it successfully or unsuccessfully), the Intent Engine will cease polling and cancel The default-timeout-duration timer.\nAll three of these timer values can be set to specific values in Vault following the same procedure should the default values not meet the needs of a specific deployment. They can be set collectively or individually as required. There is no requirement to set all three. In the following example we look at setting clientReadTimeout but this can be applied to all three timers.\nProcedure to set a custom RM Response timeout value Setting the value requires two steps\n Setting the alm.http.clientReadTimeout property in Vault Restarting the daytona microservice instances  These are achieved as follows;\n  Setting the alm.http.clientReadTimeout property in Vault\nThe general details of how to make a configuration change in vault are detailed here but the specific details for this setting are as follows\na. Log in to Vault UI using the appropriate vault security token for your environment.\nb. Navigate to the LM secrets ( ui/vault/secrets/lm/list ).\nc. If an existing ‘secret’ does not exist for daytona then one must be created. By default a daytona secret does not exist. A new one can be created using the ‘Create Secret’ button on the top right-hand side of the UI. Should a daytona secret already exist then it should be edited.\nd. Add a key/value to the secret with the new timeout period. The provided value is in milliseconds.\nKey: alm.http.clientReadTimeout value: \u0026lt;newclientReadTimeoutValue\u0026gt; For example;\nalm.http.clientReadTimeout: 900000 This will alter the desired clientReadTimeout to 900000ms (15 minutes) from its default of 300000 (5 minutes). The default is held in the code and clientReadTimeout only needs to be present in Vault if this is to be overridden.\nThe values for alm.daytona.resource-manager.default-timeout-duration \u0026amp; alm.daytona.resource-manager.polling-interval can be supplied with units thus a one hour timer can, for example, be set using:\n 1h 60m 360s ( default thus 360 without a unit is 360s ) 360000ms    Restarting the daytona microservice instances\n  With the new setting made, the individual daytona microservice instances must be restarted to adopt the new setting\ra. With kubectl configured for the correct cluster perform the following query to obtain a list of daytona microservice instances\r```\rkubectl get pods | grep -i daytona | awk '{print $1}';\r```\rb. 'Bounce' each *daytona* microservice pod in turn making sure that each pod comes back into service before bouncing the next. This will ensure that the system does not lose engineered capacity\r```\rkubectl delete pod \u0026lt;output\u0026gt;\r```\r "},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/administration/security/manage-users/","title":"Manage Users and Groups","tags":[],"description":"","content":"The following guide details how to configure users in the default OpenLDAP installed as part of Agile Lifecycle Manager (ALM).\nALM provides no built-in mechanism for managing users. There are many available LDAP clients which can be used for such purposes which makes the management much easier and a lot more visual.\nIf your environment is connecting to an LDAP managed outside of ALM, then the system administrator is expected to already understand the connection and user model in use.\nIf your LDAP server is installed along with LM helm-foundation and you are a first-time user of any LDAP Client, skip to our example instructions for LDAP Admin.\nPre-requisites To complete this guide you will need:\n An existing installation of ALM kubectl configured with access to your Kubernetes cluster  Connecting to OpenLDAP Identify NodePort As part of the helm-foundation install, OpenLDAP should have been installed and should be exposed externally via a NodePort. This NodePort is randomly assigned so it will need to be discovered from the installation environment. The following command should reveal this:\nkubectl get service foundation-openldap | awk '{print $5}' | grep TCP | cut -d ':' -f 2 | cut -d'/' -f 1\rConnect with LDAP client Having established the port, it should be possible to connect to the environment using the IP address or hostname and this port with your chosen LDAP Client.\nAssuming the default installation values were used, the connection details will be as follows:\n   Username Password     URL ldap://\u0026lt;kubernetes-host\u0026gt;:\u0026lt;port\u0026gt;   Base dc=lm,dc=com   Username cn=admin,dc=lm,dc=com   Password lmadmin    Add a user Create a new entry for a user in OpenLDAP with the person and uidObject classes. The entry should have the following properties set:\ndn: uid=\u0026lt;username\u0026gt;,ou=people,dc=\u0026lt;domain\u0026gt;\robjectClass: person\robjectClass: uidObject\rcn: \u0026lt;username\u0026gt;\rsn: \u0026lt;username\u0026gt;\ruid: \u0026lt;username\u0026gt;\ruserPassword: \u0026lt;password\u0026gt;\rdn is the directory path in OpenLDAP this user should be created in. Replace \u0026lt;username\u0026gt; with a username of your choice and replace the \u0026lt;domain\u0026gt; with the domain set at installation (the default is: lm.com which results in dc=lm,dc=com)\nBelow is an example for adding a new user named TestUser and password of secret (note the password will be encrypted on write by OpenLDAP):\ndn: uid=TestUser,ou=people,dc=lm,dc=com\robjectClass: person\robjectClass: uidObject\rcn: TestUser\rsn: TestUser\ruid: TestUser\ruserPassword: secret\rAdd a group Create a new entry for a group in OpenLDAP with the existing groupOfNames. The entry should have the following properties set:\ndn: cn=\u0026lt;group-name\u0026gt;,ou=groups,dc=\u0026lt;domain\u0026gt;\robjectclass: groupOfNames\rcn: \u0026lt;group-name\u0026gt;\rdn is the directory path in OpenLDAP this group should be created in. Replace \u0026lt;group-name\u0026gt; with a name of your choice (this will be needed when managing roles). Replace the \u0026lt;domain\u0026gt; with the domain set at installation (the default is: lm.com which results in dc=lm,dc=com)\nBelow is an example for adding a new group named TestGroup:\ndn: cn=TestGroup,ou=groups,dc=lm,dc=com\robjectclass: groupOfNames\rcn: TestGroup\rAdd user to group In order to assign a user with roles, the user must be made a member of an appropriate LDAP group. If the LDAP group doesn\u0026rsquo;t already exist, then it should be created. If it does exist, the user should be added as a group member to grant them the role.\nThe member list of a group should contain the users who are part of it. To add a member to a group, add the full identifier of the user entry to the member list:\ndn: cn=\u0026lt;group-name\u0026gt;,ou=groups,dc=\u0026lt;domain\u0026gt;\robjectclass: groupOfNames\rcn: \u0026lt;group-name\u0026gt;\rmember: \u0026lt;member1-id\u0026gt;\rmember: \u0026lt;member2-id\u0026gt;\r...\rBelow is an example a group named TestGroup with 2 members; TestUserA and TestUserB:\ndn: cn=TestGroup,ou=groups,dc=lm,dc=com\robjectclass: groupOfNames\rcn: TestGroup\rmember: uid=TestUserA,ou=people,dc=lm,dc=com\rmember: uid=TestUserB,ou=people,dc=lm,dc=com\rRevoke user access An existing user may be suspended in order to disable their access. Suspension can be performed by making a user a members of a group called Suspended:\ndn: cn=Suspended,ou=groups,dc=lm,dc=com\robjectclass: groupOfNames\rcn: Suspended\rmember: uid=TestUserA,ou=people,dc=lm,dc=com\rThis avoids having to make permanent changes to the LDAP configuration (e.g. removing them from group membership). To restore access for this user, simply remove them from this group.\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/reference/behaviour-testing/step-reference/metric-definitions/","title":"Metric Definitions","tags":[],"description":"","content":"Specify Metric Definition Description Specify the format of messages on Kafka for a metric type that will be recorded later.\nPasses when:\n the format is valid  Fails when:\n the format is invalid  Properties    Property Description     metricDefinitionName Name given to the metric definition (referenced in steps to record a metric)   metricDefinition A table specifying the 4 fields to be found on a Kafka message    Metric Definition Table:\n   Definition Parameter Description     resourceIdField The field on the Kafka message which holds the identifier to group metrics on. This field is used to determine the entity the metric message is for; typically this is the name/id of the resource instance however, depending on the metrics produced by your service, it could be a higher level of abstraction such as the service itself   timestampField The field on the Kafka message which holds the timestamp of the message   metricTypeField The field on the Kafka message which holds the type of metric. This field is used to determine what this metric is measuring e.g. CPU, memory, failed_calls   valueField The field on the Kafka message which holds the value to record. The value can hold a single numeric value or a list, however in the second case only the first value is recorded    Specify Metric Definition Inline Description Specify the format of messages on Kafka for a metric type that will be recorded later.\nPasses when:\n the format is valid  Fails when:\n the format is invalid  Properties    Property Description     metricDefinitionName Name given to the metric definition (referenced in steps to record a metric)   resourceIdField The field on the Kafka message which holds the resourceId. This field is used to determine which Resource the metric message is for   timestampField The field on the Kafka message which holds the timestamp of the message   metricTypeField The field on the Kafka message which holds the type of metric. This field is used to determine what this metric is measuring e.g. CPU, memory, failed_calls   valueField The field on the Kafka message which holds the value to record    "},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/network-service-design/","title":"Network Service Design","tags":[],"description":"","content":""},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/resource-engineering/drivers/overview/","title":"Overview","tags":[],"description":"","content":"Resource Driver Overview Brent handles Resource Manager transition and operation requests for Resources using Resource Drivers.\nThe following diagram shows the interactions between Brent and a resource driver Resource Driver is a combination of VIM and Lifecycle drivers of LM version 2.1 into a single driver concept.\nUnlike in v2.1.0 where a VIM Driver handles infrastructure creation/deletion while Lifecycle Driver executes resource transitions once the resource has been created; a Resource Manager handles both infrastructure and lifecycles by serving Create/Delete as lifecycle requests.\nVersion compability between LM and drivers    LM Version lmctl Ansible Lifecycle Driver SOL 003 Lifecycle Driver Openstack VIM Driver Kubernetes Resource Driver     2.0.3 2.0.7.1 none none none none   2.1.0 2.4.1 0.5.0 - 1.1.0 0.0.3 - 0.0.5 0.4.0 - 1.0.0 none   2.2.x \u0026gt;= 2.5.0 \u0026gt;= 2.0.0 0.1.0 \u0026gt;= 2.0.0 \u0026gt;= 0.0.3a1     Ansible Lifecycle Driver SOL 003 Lifecycle Driver Openstack VIM Driver Kubernetes Resource Driver  If you want to create your own Resource Driver, check out how to create custom driver.\nNext Steps Once you understand the basics of the drivers and have installed some for your own testing, you should continue to onboard the drivers in LM.\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/resource-engineering/resource-packages/brent/basic-resource/get-started/","title":"Prerequisites","tags":[],"description":"","content":"Objectives This guide aims to show you how to:\n Create a working Resource with Openstack infrastructure Include an external IP address as part of the infrastructure so the Resource service can be accessed publicly Add lifecycle scripts to configure a software function for the Resource Create an instance of the Resource  Prerequisites To follow this guide you will need:\n LMCTL v2.5.0+ A ready-to-use Agile Lifecycle Manager (ALM) environment An Openstack environment (for testing we recommend installing DevStack. Playbooks to setup a DevStack VM can be found on GitHub)  Openstack Your Openstack environment will need:\n Compute instance image with SSH password access support: xenial-server-cloudimg-amd64-disk1 (this should be included with a standard DevStack install or manually loaded from here) A public network (should be included with a standard DevStack install) A key-pair named default of type SSH Key to be used by compute instances (create one through the Openstack dashboard)  Agile Lifecycle Manager (ALM) Your ALM environment must have the following:\n Openstack VIM Driver installed and onboarded Ansible Lifecycle Driver installed and onboarded A valid Deployment Location onboarded to ALM with the necessary properties required by the Openstack VIM Driver and Ansible Lifecycle Driver  Get Started Once you have fulfilled the prerequisites you may start creating the Resource\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/resource-engineering/resource-packages/brent/infrastructure-keys-resource/get-started/","title":"Prerequisites","tags":[],"description":"","content":"Objectives This guide aims to show you how to:\n Create a working Resource with Openstack infrastructure Include an external IP address as part of the infrastructure so the Resource service can be accessed publicly Create Compute instances accessible with SSH keys that are stored securely in LM (Brent), and provided to lifecycle scripts and inventory to be able to connect to the instance. Add lifecycle scripts to configure a software function for the Resource Create an instance of the Resource  Prerequisites To follow this guide you will need:\n LMCTL v2.5.0+ A ready-to-use Agile Lifecycle Manager (ALM) environment An Openstack environment (for testing we recommend installing DevStack. Playbooks to setup a DevStack VM can be found on GitHub)  Openstack Your Openstack environment will need:\n Compute instance image with SSH password access support: xenial-server-cloudimg-amd64-disk1 (this should be included with a standard DevStack install or manually loaded from here) A public network (should be included with a standard DevStack install) A key-pair named apache1_server_key of type SSH Key to be used by compute instances (create one through the Openstack UI, and save the private key that is downloaded when creating the key)  Agile Lifecycle Manager (ALM) Your ALM environment must have the following:\n Openstack VIM Driver installed and onboarded Ansible Lifecycle Driver installed and onboarded A valid Deployment Location onboarded to ALM with the necessary properties required by the Openstack VIM Driver and Ansible Lifecycle Driver  Get Started Once you have fulfilled the prerequisites you may start creating the resource\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/reference/descriptor-specification/relationship-state/","title":"Relationship State","tags":[],"description":"","content":"Relationship State When defining a relationship we must also define when this relationship should be created and deleted (ceased) within the context of the assembly lifecycle (and the lifecycles of its components)\n This is achieved by specifying the minimum state each end of the relationship must occupy for the relationship to exist The target component is the driving end of the relationship, meaning that the relationship will be created when the target component will reach its minimum state in this transition  In addition to specifying the minimum state, we can also define a modifier for the state which identifies whether the relationship should be created before (pre) or after (post) the relevant end transitions\n By default, source components have the post modifier and target components have the pre modifier If the target component has the post modifier, the relationship will be created once both the source and target are at their minimum states (or higher) If the target component has the pre modifier, the relationship will be created prior to the target component transitioning to its minimum state  LM will ensure that the source component will also be in its minimum state This can result in the source component having its state artificially raised to a higher state than that of the parent assembly In this way, relationships can be used (in addition to properties) to introduce dependencies into how the processes are built and executed    Relationship State example The following example represents the lifecycle of a relationship as the source and target resources transition through their lifecycles\nSource State: post-Active\nTarget State: pre-Inactive\nThe relationship will be created once both components have successfully transitioned to the Active state\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/resource-engineering/resource-packages/","title":"Resource Packages","tags":[],"description":"","content":""},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/behaviour-testing/scenarios/","title":"Scenarios","tags":[],"description":"","content":"A scenario outlines the list of steps to execute in order to simulate actions or assert expectations of lifecycle behaviour or metric values. It can be thought of as a script, that will execute commands one-by-one till completion. Each step represents a command which must pass in order for the scenario to continue execution. If a step fails, the scenario is stopped and marked as failed. Any steps scheduled after the failing step are not executed.\nScenarios are created by first outlining the \u0026ldquo;actors\u0026rdquo; involved. Each actor is just an assembly instance to be created from an assembly configuration previously created by the user in the current project. You will then select the steps to be executed, potentially referencing the assembly actors to simulate behaviour.\nCreating Scenarios To create a scenario click on \u0026ldquo;Create Scenario\u0026rdquo;. Provide a name and a description to outline the purpose of the scenario.\nClick \u0026ldquo;Save\u0026rdquo; to finish creating the scenario. Once it appears in the scenario list you may use the \u0026ldquo;Open\u0026rdquo; button to navigate to the scenario designer where you may begin adding assembly configurations and steps.\nScenario Designer The following sections show you how to make changes to the scenario using the designer. It\u0026rsquo;s important to note that any changes made are not saved until you click the \u0026ldquo;Save\u0026rdquo; button at the top of the designer.\nManage Assembly Actors In the designer for your chosen scenario you will see the \u0026ldquo;Assemblies\u0026rdquo; panel at the top and the \u0026ldquo;Scenario Palette\u0026rdquo; on the right hand side. If you expand the \u0026ldquo;Assembly Configurations\u0026rdquo; section of the palette you will see a list of the configurations you have created in this project. You will also see an additional configuration called \u0026ldquo;Existing Provided Assembly\u0026rdquo; which will be covered later.\nAdd an Assembly Actor To include a new actor in your scenario, drag an assembly configuration from the right hand panel into the dotted \u0026ldquo;Assemblies\u0026rdquo; area to the left.\nDragging it across will create an entry in the scenario:\nThe configuration is now an actor in the scenario. When the scenario is executed an assembly instance will be created based on the descriptor and properties set in this configuration.\nYou may use the input fields on the new entry to decide the name of the future assembly instance and also decide the state it should be created in. The final input lets you choose what should happen to this assembly when the scenario completes successfully: uninstall or keep it. When choosing uninstall, it is important to know that this takes place at the end of the scenario, after all the steps have completed. This means if the scenario fails then the assembly will not be uninstalled, as the step to uninstall it will never be reached.\nMultiple assembly actors can be included on the scenario by dragging in further assembly configurations or by re-using the same configuration, just be sure to give each assembly a unique name.\nThe order of the assembly actor entries is important, as this is the order each instance will be created in. You can re-order the assemblies by dragging each entry up or down.\nRemove Assembly Actor An assembly actor may also be removed by clicking on the trash icon in the top right of the entry.\nManage Stages Below the \u0026ldquo;Assemblies\u0026rdquo; panel is the \u0026ldquo;Stages\u0026rdquo; section. You will need at least one stage in the scenario before you can add any steps (there should already be a \u0026ldquo;Default Stage\u0026rdquo; included on any new scenario).\nA stage is a logical group of steps, allowing you to break the scenario down into smaller sections for readability. The use of stages has no impact on the scenario execution.\nAdd a Stage Add a new stage by clicking the \u0026ldquo;Add Stage\u0026rdquo; button below the current list of stages.\nThe stage will be immediately added to the bottom of the list, with a default name of \u0026ldquo;Stage\u0026rdquo;. You can modify the name by clicking the \u0026ldquo;pencil\u0026rdquo; button next to the current name. This will open an input box for you to enter an alternative name. Save your changes with the \u0026ldquo;tick\u0026rdquo; button.\nThe order of the stages is important, as the steps in each stage are executed in linear order, starting with the first stage in the list. You can re-order the stages by dragging each entry up or down.\nRemove a Stage A stage may be removed by clicking on the trash icon in the top right of the entry. You will be prompted to confirm your intention to remove the stage, as this will remove all of the steps included on it.\nManage Steps The \u0026ldquo;Scenario Palette\u0026rdquo; to the right of the designer contains the set of steps which you may add to your scenario. View the steps by clicking on one of the group titles, this will expand the group to reveal the steps it contains. You can close the group by clicking the title again.\nEach step features a sentence which declares the action or assertion it will take. Some parts of this sentence are configurable, indicated by a square with a variable name inside curly brackets e.g. ${assemblyName}.\nAdd a Step Add a step by dragging it from the palette into one of your stages.\nThis will create an entry in the stage. You can configure the properties by filling out the input boxes.\nYou may add as many steps as you desire to each stage. The order of the steps is important as this is the order they will be executed. The first step in the first stage will be executed, following by the next step in the stage. After all the steps in a stage are complete, the first step in the next stage will be executed and so on until there are no more stages.\nYou can re-order the steps by dragging each entry up or down. You may also move steps from one stage to another by dragging it across to the new stage.\nRemove a Step A step may be removed by clicking on the trash icon in the top right of the entry. You will be prompted to confirm your intention to remove the step.\nStep Reference For a complete guide on the behaviour of each step offered in the palette please read the Step Reference\nExecuting Scenarios A scenario may be executed from the designer by clicking the \u0026ldquo;Run Scenario\u0026rdquo; button at the top of the screen. Be sure to save any changes first, by clicking the \u0026ldquo;Save\u0026rdquo; button.\nYou will be re-directed to the scenario execution screen which shows it\u0026rsquo;s progress. The status of the steps are shown on the left side whilst the right hand panel will show metric charts if a record metric step has been executed.\nYou will notice some additional stages added before and after the stages you added in your design. The \u0026ldquo;Configure Execution Environment\u0026rdquo; initiates the start of an execution. The \u0026ldquo;Prepare Assemblies\u0026rdquo; stage will create the assembly instances for any actors you specified on the scenario (by dragging in assembly configurations). The \u0026ldquo;Cleanup Assemblies\u0026rdquo; stage, added to the end of the scenario, will uninstall any assembly instances with the \u0026ldquo;uninstall\u0026rdquo; option set on their actor entry.\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/network-service-design/develop-a-network-service-package/setting-up-a-project/","title":"Setting up a project","tags":[],"description":"","content":"The first step in developing a Network Service (NS) is to create a NS project.\nCreate NS Project On your local environment, you create a NS project by running the following LMCTL command in the folder you want the NS to be created in:\nlmctl project create --name myvns --version 0.1 --servicetype NS ./myns\rThis creates a NS directory structure for the NS. The result of this is a NS directory structure that is created that should look like this:\n├── Descriptor\r│ └── assembly.yml\r├── Behaviour\r│ └── Runtime\r│ └── Templates\r│ └── Tests\r└── lmproject.yml\rProject structure explanation Directory structure A top level project structure can have the following artifacts:\n   Directory Required Description     Descriptor Yes This directory must contain a descriptor file called \u0026ldquo;assembly.yml\u0026rdquo;, representing the top level Network Service. The descriptor is pushed to ALM during onboarding   Behaviour optional This optional directory contains the Assembly Templates and Scenarios used by the test scenarios   Behaviour/Tests optional This optional directory contains the Test Scenarios. These are pushed to ALM automatically during on boarding   Behaviour/Templates optional This optional directory contains the Assembly Templates. These are pushed to ALM automatically during onboarding   Behaviour/Runtime optional This optional directory contains the Runtime Scenarios, such as Diagnostic tests. These are pushed to ALM automatically during on boarding    lmproject File Every project should include a lmproject.yml file at root that is automatically created when you create the project using LMCTL. This file will be automatically updated once you add elements and VNFs to the NS in the Designer, so there is no manual action needed for the NS lmproject.yml file.\nCommit, Create and Push Project to GIT Commit the project  $ cd ./myvns\r$ git init\r$ echo \u0026quot;_lmctl\u0026quot; \u0026gt; .gitignore\r$ git add .\r$ git commit -m 'initial project'\rCreate the project on Git Server (Gogs) Go to Gogs in the CI/CD hub.\n log in as admin create \u0026ldquo;+ New Repository\u0026rdquo; named the same as your local project (e.g. \u0026ldquo;myvns\u0026rdquo;) select the organization (e.g. \u0026lsquo;marketplace\u0026rsquo;) Note the commands that Gogs gives you for \u0026lsquo;Push an existing repository from the command line\u0026rsquo; and use them in the next step.  Push project to Gogs On your local machine:\n $ git remote add origin http://\u0026lt;gogs ip address\u0026gt;:8001/cicdhub-admin/myvns.git\r$ git push -u origin master\rCreate git develop branch  $ git checkout -b develop\r On Gogs, go to the project, select settings and collaboration \u0026lsquo;Add New Collaborator\u0026rsquo; for jenkins and any user you wish to be able the make changes. You are now ready to start developing and testing your new project.  Push Project to Dev Environment The NS project needs to be pushed to the development environment. Run the following command in the project directory (assuming \u0026ldquo;dev\u0026rdquo; is the name of the environment you want to push the NS to):\n $ cd \u0026lt;myvns dir\u0026gt;\r$ lmctl project push dev\rWhen you login to ALM and go to the Assembly Designer section, you should see an assembly with the project name you created.\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/installation/cicdhub/storage/","title":"Storage","tags":[],"description":"","content":"Storage Class By default, any service requiring persistence is configured to use the default provisioner of your Kubernetes cluster. You can check your default with kubectl:\nkubectl get storageclass\rThe default storage class will be shown with (default) alongside it\u0026rsquo;s name. If you have no default, you can mark an existing class as the default with:\nkubectl patch storageclass \u0026lt;your-class-name\u0026gt; -p '{\u0026quot;metadata\u0026quot;: {\u0026quot;annotations\u0026quot;:{\u0026quot;storageclass.kubernetes.io/is-default-class\u0026quot;:\u0026quot;true\u0026quot;}}}'\rAlternatively, you may explicitly set the storage class for each service by adding storageClass fields in your custom values:\ndockerregistry:\rpersistence:\rstorageClass: \u0026lt;override-class-name\u0026gt;\rgogs:\rpersistence:\rstorageClass: \u0026lt;override-class-name\u0026gt;\rpostgresql:\rpersistence:\rstorageClass: \u0026lt;override-class-name\u0026gt;\rjenkins:\rpersistence:\rstorageClass: \u0026lt;override-class-name\u0026gt;\ropenldap:\rpersistence:\rstorageClass: \u0026lt;override-class-name\u0026gt;\rnexus:\rpersistence:\rstorageClass: \u0026lt;override-class-name\u0026gt;\rStorage Size The size of persistent volume created for each service may be configured by adding resource size settings to the custom values file. See Kubernetes - Persistent Volumes for more information about how the size of a volume is managed.\nThe default values for each service are shown below. Override any defaults by adding them to your custom values.\ndockerregistry:\rpersistence:\rsize: 100Gi\rgogs:\rpersistence:\rsize: 1Gi\rpostgresql:\rpersistence:\rsize: 5Gi\rjenkins:\rpersistence:\rsize: 1Gi\ropenldap:\rpersistence:\rsize: 5Gi\rnexus:\rpersistence:\rstorageSize: 100Gi\rNext Steps Continue configuring your installation with Access Configuration\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/administration/configuration/themes/theme-running/","title":"Theme Updates","tags":[],"description":"","content":"The following steps explain how to change the theme used by the Agile Lifecycle Manager (ALM) User Interface when it is running.\nUpdating a Theme   Follow the steps in theme overview before doing the following.\n  Add the following configuration to vault for lm/nimrod, changing the value of theme.name to be the name of the theme to be used:\n  \u0026quot;alm.nimrod.theme.name\u0026quot;: \u0026quot;mytheme\u0026quot;\rCreate a values file with the following configuration:   nimrod:\rapp:\rconfig:\rthemesConfigMap: lm-themes\rRun a helm upgrade of the ALM helm chart with the custom values file above.  "},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/installation/resource-manager/ansible-rm/storage/","title":"Storage","tags":[],"description":"","content":"Existing Storage Services Ansible RM requires a connection to Cassandra and Kafka. By default the Helm chart is configured to locate existing persisted services using labels:\n Cassandra - expects a Cassandra service in the same namespace with labels app=cassandra and release=foundation Kafka - expects a Kafka service in the same namespace with labels app=kafka and release=foundation  Check your existing Cassandra and Kafka have the given labels. If you did not install foundation services from Agile Lifecycle Manager (ALM) with the Helm release name of foundation, then the release label may have an alternative value. If this is the case, update the labels by adding the following to your custom values:\napp:\rconfig:\rkafka:\rexistingSelectors:\rapp: kafka\rrelease: foundation\rcassandra\rexistingSelectors:\rapp: cassandra\rrelease: foundation\rNext Steps Continue configuring your installation with Access Configuration\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/architecture/","title":"Architecture","tags":[],"description":"","content":"Objectives Introduce Agile Lifecycle Manager (ALM) software architecture\nSoftware Architecture ALM software is deployed as separated distinct component sets which can be considered in two groups;\n Core ALM consisting of a number of microservices Dependencies: Foundation functionality consisting of industry standard third party implementations  Message Bus Data Persistence Authentication Server    ALM Core application is realized by a number of microservices deployed on Kubernetes environment. Each microservice has a specific role/function within LM application and communicate with each other via RESTful APIs and through a Kafka message bus.\nEach LM microservice is realized as a Kubernetes service and implemented as a set of service instances which are reachable via a load balancer. Every service operates in a fully active instance set. That is, all service instances within the set are fully operational and capable of servicing any request made of it concurrent to its peers within the service.\nSolution Architecture The main responsibilities of ALM is to enable modeling of network services (NS) and drive them through their lifecycles according to requests it receives. ALM integrates on its northbound interface with Order management tools to take in orders to deploy, maintain or decommission service instances.\nFigure below illustrates the positioning of ALM with other main components involved in service development and orchestration.\nMore detailed description of ALM architecture can be found in ALM Architecture\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/resource-engineering/resource-packages/brent/basic-resource/creating-resource/","title":"Create Resource","tags":[],"description":"","content":"Create the Resource Project Start by creating a directory for the Resource project and navigating to it\nmkdir hw-apache2 \u0026amp;\u0026amp; cd hw-apache2\rUse LMCTL to create the basis of a Resource project\nlmctl project create --type Resource --rm brent\rEach resource requires a resource descriptor in YAML format that is located in the \u0026lsquo;Definitions/lm/resource.yaml\u0026rsquo; file of the resource package. The format of this file is defined by the ALM Resource Descriptor YAML Specification.\nAdd Resource Descriptor Update the Definitions/lm/resource.yaml file with the following content:\ndescription: descriptor for hw-apache2\rproperties:\rkey_name:\rdefault: default\rdescription: Name of key-pair to be used for compute instance\rimage_id:\rdefault: xenial-server-cloudimg-amd64-disk1\rdescription: Image to be used for compute instance\rflavor:\rdefault: m1.small\rdescription: Type of instance (flavor) to be used\rinternal_ip:\rread-only: true\rdescription: IP assigned to the server on the internal network\rpublic_ip: read-only: true\rdescription: IP assigned to the server to access from the external network\rinfrastructure:\rOpenstack: {}\rlifecycle:\rCreate:\rdrivers:\ropenstack:\rselector:\rinfrastructure-type:\r- '*'\rDelete:\rdrivers:\ropenstack:\rselector:\rinfrastructure-type:\r- Openstack\rThe descriptor includes 3 input properties, each correspond to an input in our HEAT template: key_name, image_id, flavor. The descriptor also includes 2 read-only properties, which correspond to outputs in our HEAT template: internal_ip and public_ip.\ninfrastructure defines the infrastructure type as that of Openstack. infrastructure-type property for drivers of lifecycles can then use a wildcard value '*' and the default resource infrastructure will be automatically registered by LM.\nUnder lifecycle section are 2 basic lifecycle transitions for the resource: Create and Delete. These will be handled by the Openstack VIM Driver to manage infrastructure (e.g create VMs with compute, storage and network infrastructure required on Openstack) to fulfill the resource\u0026rsquo;s function.\nNote that \u0026ldquo;Openstack\u0026rdquo; under infrastructure is the infrastructure type by default for all lifecycles, while \u0026ldquo;openstack\u0026rdquo; under lifecycle.Create.drivers is the Resource Driver type used by a particular lifecycle.\nFor \u0026ldquo;Create\u0026rdquo; lifecycle, the resource driver will define the format of the templates to be used and automatically pick up suitable infrastructure template under Lifecycle/openstack. In the next step we will create a HEAT template for this lifecycle.\nAdd Infrastructure template For this example, we will create a new file called openstack-heat.yaml at Lifecycle/openstack with the following content:\nheat_template_version: \u0026quot;2018-08-31\u0026quot;\rdescription: \u0026quot;Base infrastructure for an Apache2 example\u0026quot;\rparameters:\rkey_name:\rtype: string\rlabel: Key Name\rdescription: Name of key-pair to be used for compute instance\rimage_id:\rtype: string\rlabel: Image ID\rdescription: Image to be used for compute instance\rflavor:\rtype: string\rlabel: Instance Type\rdescription: Type of instance (flavor) to be used\rresources:\rap_security_group: type: \u0026quot;OS::Neutron::SecurityGroup\u0026quot;\rproperties: rules: - port_range_min: 1\rport_range_max: 100\rprotocol: tcp\r- remote_ip_prefix: 0.0.0.0/0\rprotocol: icmp\rname: ap_security_group\rap_net: type: \u0026quot;OS::Neutron::Net\u0026quot;\rproperties: admin_state_up: true\rname: ap_net\rap_subnet: type: \u0026quot;OS::Neutron::Subnet\u0026quot;\rproperties: network: { get_resource: ap_net }\rname: ap_subnet\renable_dhcp: true\rcidr: \u0026quot;10.10.10.0/24\u0026quot;\rap_port: type: \u0026quot;OS::Neutron::Port\u0026quot;\rproperties: admin_state_up: true\rfixed_ips: - subnet: { get_resource: ap_subnet }\rsecurity_groups: - { get_resource: ap_security_group }\rname: ap_port\rnetwork: { get_resource: ap_net }\rap_router: type: \u0026quot;OS::Neutron::Router\u0026quot;\rproperties: admin_state_up: true\rname: ap_router\rexternal_gateway_info:\rnetwork: \u0026quot;public\u0026quot;\rap_routerinterface: type: \u0026quot;OS::Neutron::RouterInterface\u0026quot;\rproperties: router: { get_resource: ap_router }\rsubnet: { get_resource: ap_subnet }\rapache_server_floating_ip:\rtype: OS::Neutron::FloatingIP\rproperties:\rfloating_network: \u0026quot;public\u0026quot;\rport_id: { get_resource: ap_port }\rapache_server: type: \u0026quot;OS::Nova::Server\u0026quot;\rproperties: networks: - port: { get_resource: ap_port }\rname: apache_server\rflavor: { get_param: flavor }\rkey_name: { get_param: key_name }\rimage: { get_param: image_id }\rconfig_drive: true\ruser_data_format: RAW\ruser_data: |\r#cloud-config\rmanage_etc_hosts: true\rbootcmd: - [ sysctl, net.ipv4.ip_forward=1 ]\r- [ sh, -c, echo 'nameserver 8.8.8.8' \u0026gt; /etc/resolv.conf ]\rssh_pwauth: True\rpassword: ubuntu\rchpasswd:\rlist: |\rubuntu:ubuntu\rexpire: False\rpackages:\r- \u0026quot;python\u0026quot;\routputs:\rinternal_ip: value: { get_attr: [ apache_server , first_address ] }\rpublic_ip: value: { get_attr: [ apache_server_floating_ip , floating_ip_address ] }\rThis will create an Apache Server running in an Openstack VM.\nNext Steps You have now created the basic files needed for a simple Resource with OpenStack infrastructure. Move on to onboard and instantiate the Resource\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/resource-engineering/resource-packages/brent/infrastructure-keys-resource/creating-resource/","title":"Create Resource","tags":[],"description":"","content":"Create the Resource Project The resource comprises 2 Apache servers defined in a HEAT template (one that uses an existing SSH key pair pre-created in Openstack, the other uses a new SSH key pair created by the HEAT template) and configured by Ansible playbooks.\nStart by creating a directory for the Resource project and navigating to it\nmkdir hw-apache2 \u0026amp;\u0026amp; cd hw-apache2\rUse LMCTL to create the basis of a Resource project\nlmctl project create --type Resource --rm brent\rEach resource requires a resource descriptor in YAML format that is located in the \u0026lsquo;Definitions/lm/resource.yaml\u0026rsquo; file of the resource package. The format of this file is defined by the ALM Resource Descriptor YAML Specification.\nAdd Resource Descriptor Update the Definitions/lm/resource.yaml file with the following content:\ndescription: descriptor for hw-apache2\rproperties:\rserver1_ssh_key:\rtype: key\rdefault: apache1_server_key\rdescription: This is the name of an existing SSH key pair in Openstack, used by apache_server1 server\rserver2_ssh_key_name:\rdescription: This is the name given to a new SSH key pair generated by the HEAT template, and used by apache_server2 server\rdefault: apache2_server_key\rtype: string\rimage_id:\rdefault: xenial-server-cloudimg-amd64-disk1\rdescription: Image to be used for compute instance\rflavor:\rdefault: m1.small\rdescription: Type of instance (flavor) to be used\rserver1_internal_ip:\rread-only: true\rdescription: IP assigned to the apache1 server on the internal network\rserver1_public_ip: read-only: true\rdescription: IP assigned to the apache1 server to access from the external network\rserver2_internal_ip:\rread-only: true\rdescription: IP assigned to the apache2 server on the internal network\rserver2_public_ip: read-only: true\rdescription: IP assigned to the apache2 server to access from the external network\rprivate-properties:\rserver2_ssh_key:\rdescription: An SSH key pair created by the HEAT template, used by apache_server2 server. This will be stored in LM (both the public and private key portions, the private key portion will be stored securely).\rtype: key\rinfrastructure:\rOpenstack: {}\rlifecycle:\rCreate:\rdrivers:\ropenstack:\rselector:\rinfrastructure-type:\r- Openstack\rDelete:\rdrivers:\ropenstack:\rselector:\rinfrastructure-type:\r- Openstack\rThe descriptor includes 4 input properties, each correspond to an input in our HEAT template:\n server1_ssh_key: the name of an existing SSH key (created in Openstack and stored as a shared infrastructure key in LM), used when provisioning apache_server1 server2_ssh_key_name: the name to give the SSH key generated by the HEAT template and used when provisioning apache_server2 image_id: the Openstack VM image id flavor: the Openstack VM flavor  The descriptor also includes 4 read-only properties, which correspond to outputs in our HEAT template: server1_internal_ip, server1_public_ip, server2_internal_ip, server2_public_ip. Note that server2_ssh_key is a private property because we don\u0026rsquo;t wish to expose it outside of the resource manager (Brent).\ninfrastructure defines the infrastructure type as that of Openstack. infrastructure-type property for drivers of lifecycles can then use a wildcard value '*' and the default resource infrastructure will be automatically registered by LM.\nUnder lifecycle section are 2 basic lifecycle transitions for the resource: Create and Delete. These will be handled by the Openstack VIM Driver to manage infrastructure (e.g create VMs with compute, storage and network infrastructure required on Openstack) to fulfill the resource\u0026rsquo;s function.\nNote that \u0026ldquo;Openstack\u0026rdquo; under infrastructure is the infrastructure type by default for all lifecycles, while \u0026ldquo;openstack\u0026rdquo; under lifecycle.Create.drivers is the Resource Driver type used by a particular lifecycle.\nFor \u0026ldquo;Create\u0026rdquo; lifecycle, the resource driver will define the format of the templates to be used and automatically pick up suitable infrastructure template under Lifecycle/openstack. In the next step we will create a HEAT template for this lifecycle.\nAdd Infrastructure For this example, we will create a new file called openstack-heat.yaml at Lifecycle/openstack with the following content:\nheat_template_version: \u0026quot;2018-08-31\u0026quot;\rdescription: \u0026quot;Base infrastructure for an Apache2 example\u0026quot;\rparameters:\rserver1_ssh_key:\rtype: string\rlabel: Key Name\rdescription: Name of an existing Openstack SSH key pair\rserver2_ssh_key_name:\rtype: string\rdescription: Name of new Openstack SSH key pair created by the HEAT template\rimage_id:\rtype: string\rlabel: Image ID\rdescription: Image to be used for compute instance\rflavor:\rtype: string\rlabel: Instance Type\rdescription: Type of instance (flavor) to be used\rresources:\rap_security_group: type: \u0026quot;OS::Neutron::SecurityGroup\u0026quot;\rproperties: rules: - port_range_min: 1\rport_range_max: 100\rprotocol: tcp\r- remote_ip_prefix: 0.0.0.0/0\rprotocol: icmp\rname: ap_security_group\rap_net: type: \u0026quot;OS::Neutron::Net\u0026quot;\rproperties: admin_state_up: true\rname: ap_net\rap_subnet: type: \u0026quot;OS::Neutron::Subnet\u0026quot;\rproperties: network: { get_resource: ap_net }\rname: ap_subnet\renable_dhcp: true\rcidr: \u0026quot;10.10.10.0/24\u0026quot;\rap_port1:\rtype: \u0026quot;OS::Neutron::Port\u0026quot;\rproperties: admin_state_up: true\rfixed_ips: - subnet: { get_resource: ap_subnet }\rsecurity_groups: - { get_resource: ap_security_group }\rname: ap_port\rnetwork: { get_resource: ap_net }\rap_port2:\rtype: \u0026quot;OS::Neutron::Port\u0026quot;\rproperties: admin_state_up: true\rfixed_ips: - subnet: { get_resource: ap_subnet }\rsecurity_groups: - { get_resource: ap_security_group }\rname: ap_port\rnetwork: { get_resource: ap_net }\rap_router: type: \u0026quot;OS::Neutron::Router\u0026quot;\rproperties: admin_state_up: true\rname: ap_router\rexternal_gateway_info:\rnetwork: \u0026quot;public\u0026quot;\rap_routerinterface: type: \u0026quot;OS::Neutron::RouterInterface\u0026quot;\rproperties: router: { get_resource: ap_router }\rsubnet: { get_resource: ap_subnet }\rapache_server1_floating_ip:\rtype: OS::Neutron::FloatingIP\rproperties:\rfloating_network: \u0026quot;public\u0026quot;\rport_id: { get_resource: ap_port1 }\rapache_server2_floating_ip:\rtype: OS::Neutron::FloatingIP\rproperties:\rfloating_network: \u0026quot;public\u0026quot;\rport_id: { get_resource: ap_port2 }\rapache_server1: type: \u0026quot;OS::Nova::Server\u0026quot;\rproperties: networks: - port: { get_resource: ap_port1 }\rname: apache_server\rflavor: { get_param: flavor }\rkey_name: { get_param: server1_ssh_key }\rimage: { get_param: image_id }\rconfig_drive: true\ruser_data_format: RAW\ruser_data: |\r#cloud-config\rmanage_etc_hosts: true\rbootcmd: - [ sysctl, net.ipv4.ip_forward=1 ]\r- [ sh, -c, echo 'nameserver 8.8.8.8' \u0026gt; /etc/resolv.conf ]\rpackages:\r- \u0026quot;python\u0026quot;\rapache_server2: type: \u0026quot;OS::Nova::Server\u0026quot;\rproperties: networks: - port: { get_resource: ap_port2 }\rname: apache_server\rflavor: { get_param: flavor }\rkey_name: { get_resource: server2_ssh_key }\rimage: { get_param: image_id }\rconfig_drive: true\ruser_data_format: RAW\ruser_data: |\r#cloud-config\rmanage_etc_hosts: true\rbootcmd: - [ sysctl, net.ipv4.ip_forward=1 ]\r- [ sh, -c, echo 'nameserver 8.8.8.8' \u0026gt; /etc/resolv.conf ]\rpackages:\r- \u0026quot;python\u0026quot;\rserver2_ssh_key:\rtype: OS::Nova::KeyPair\rproperties:\rname: { get_param: server2_ssh_key_name }\rsave_private_key: True\routputs:\rserver1_internal_ip: value: { get_attr: [ apache_server1 , first_address ] }\rserver1_public_ip: value: { get_attr: [ apache_server1_floating_ip , floating_ip_address ] }\rserver2_internal_ip: value: { get_attr: [ apache_server2 , first_address ] }\rserver2_public_ip: value: { get_attr: [ apache_server2_floating_ip , floating_ip_address ] }\rserver2_ssh_key:\rvalue:\rstr_replace:\rtemplate: $key_name#$private_key#$public_key\rparams:\r$key_name: { get_param: [ server2_ssh_key_name ] }\r$private_key: { get_attr: [ server2_ssh_key, private_key ] }\r$public_key: { get_attr: [ server2_ssh_key, public_key ] }\rThis will create two Apache Servers running in an Openstack VM.\n apache_server1 will use an existing Openstack SSH key pair, the name of which is given by the value of the server1_ssh_key property (defaulted to apache1_server_key). Note that this is defined as a key property in the resource descriptor. LM will substitute this with the real key from the shared infrastructure key pool. The value of this input property will become the LM property value (in this case, the name of the infrastructure key). Two further synthetic properties will be constructed; server1_ssh_key_public and server1_ssh_key_private will contain the values of the public and private keys, respectively. That is, the LM key property will result in three properties being available to the driver:  [lm property]: the name of the infrastructure key [lm property]_public: the public portion of the infrastructure key (if present) [lm property]_private: the private portion of the infrastructure key (if present)   apache_server2 will use a new SSH key pair (server2_key) generated by the HEAT template, the name of which will be given by the value of the server2_ssh_key_name property.  It will output the IP addresses of the Apache servers, along with the combined private and public portions of the apache_server2\u0026rsquo;s key as an output key property called server2_ssh_key. Note the structure of the server2_ssh_key output property value; it consists of a single line containing the key name, private and public key portions (in PEM format), separated by hashes. Note also the definition of server2_ssh_key as property of type key in the private-properties section of the resource descriptor. This defines the property as private to Brent, with a type of key. Note that private string properties can be ommitted from this section (the \u0026ldquo;string\u0026rdquo; type is the default).\nNext Steps You have now created the basic files needed for a simple Resource with OpenStack infrastructure. Move on to creating a shared infrastructure key.\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/behaviour-testing/designing-scenarios/","title":"Designing Scenarios","tags":[],"description":"","content":""},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/resource-engineering/drivers/","title":"Drivers","tags":[],"description":"","content":""},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/administration/configuration/error-links-running/","title":"Enable Error Links","tags":[],"description":"","content":"Enabling the dashboard links for a running ALM  Add the following to the nimrod secret in vault:   \u0026quot;alm.nimrod.loggingDashboard.application\u0026quot;: \u0026quot;kibana\u0026quot;,\r\u0026quot;alm.nimrod.loggingDashboard.enabled\u0026quot;: \u0026quot;true\u0026quot;,\r\u0026quot;alm.nimrod.loggingDashboard.endpoint\u0026quot;: \u0026quot;http://kibana.lm:31001\u0026quot;,\r\u0026quot;alm.nimrod.loggingDashboard.kibana.index\u0026quot;: \u0026quot;lm-logs\u0026quot;\r Create an index in Kibana named lm-logs\n  Delete the Nimrod pod\n  kubectl delete pod \u0026lt;nimrod pod id\u0026gt;\u0026gt;\r"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/network-service-design/develop-a-network-service-package/implement-a-network-service-descriptor/","title":"Implement a Network Service","tags":[],"description":"","content":"Objectives  Learn how to access Service Designer tool in Agile Lifecycle Manager (ALM) Create a new Assembly Descriptor working as a starting point for a new Network Service (NS) design Design the Network Service using the Designer  Pre-requisites Before you begin following pre-requisites should be fulfilled:\n Access to ALM environment with applicable user role assigned to the user account (see Installation and Config of ALM for more details on user roles) Necessary Virtual Network Function (VNF) packages developed and onboarded to the ALM environment (see Develop a VNF package for more details) Used the LMCTL command to create the NS project and pushed that into the environment  Create a new Assembly Descriptor This step is only relevant if you have not created a NS project with LMCTL yet and you start one from scratch in the ALM Designer UI. If you have already created and onboarded the NS project with LMCTL, you can skip to the next section.\n  To create a new Assembly Descriptor to model the network service, first you need to navigate to Service Designer tool. The Service Designer is accessible by selecting Designer -\u0026gt; Assembly Designer option from the Navigation Panel on the left. Ones the Assembly Designer option is selected the entry page of the Service Designer called Assembly Descriptors is opened. Assembly Descriptors page is shown in the below screenshot.   The Assembly Descriptors page lists all the Assembly Descriptors already available in ALM. This includes both Network Service Descriptors and VNF Descriptors.\n  A new Assembly Descriptor is created by clicking the Create button on the top right corner of the page. Clicking the Create button opens a dialog box (see screenshot below) allowing to enter necessary identification information associated to the Assembly Descriptor.   Define header information for the Assembly Descriptor according to following rules\n Name  The name must start with a letter and must not contain spaces or colons. The name must be between 2 and 50 characters.   Version  The version should be numeric and can be dot separated. The version must be max 10 characters.   Description  The Description is a short textual description of the Assembly Descriptor. Description is an optional field and can be left empty.      After filling in the above listed fields, the creation of the new descriptor can be confirmed by clicking the Create button in the lower left corner of the dialog box.\n  As a result, a new Assembly Descriptor created in LM environment representing the implemented network service and accessible through Assembly Descriptors page within the Service Designer.\nAdd elements To design a Network Service, open the created Assembly instance. In order to add an element to the NS you are designing, such as a VNF, press the “Create element” button. This opens up a new dialog where you need to provide a name for the element. You also need to select the Resource Descriptor you want to use for that element, and whether it is a component or a reference. A component is embedded and part of the Assembly you are creating. A Reference is a reference to a component or Assembly outside of the NS you are creating that is required for the NS to function (e.g. a network that needs to be in place for this NS). When you press \u0026ldquo;create\u0026rdquo;, you will see the element being created in your Assembly. The picture below shows the Create Element dialog.\nAdd Properties You now see the element that you created appear in the designer indicated by a hexagon. On the left you should have your top-level NS (in this example the voice-service), and on the right the element that you created. You can click on the element to see its properties.\nThe next thing to do is to add the Properties that are needed for the NS to be able to run. Click the top-level Network Service element and click Add property (from the right panel that appears). The picture below shows the Create Property dialog.\nIn the Create New Property dialog you give the property a name, a value, a default value, and you check whether the property is required and/or locked. You can also provide a short description of the property. Once you click Create, the property you just created will appear in the top-level NS element in the Designer.\nPassing Properties The next step is to pass in properties between the top-level NS and the VNF Element. For each property you will notice a grayed out ball on the front and at the back of that. You can pass in a property from one element to the other by simply dragging and dropping. A ball icon on the left side of the property means it is an INPUT, the ball icon on the right side means it is OUTPUT.\nThe picture below shows the NS and in this case the gateway element that was added with the added Resource Manager property passed between the top-level NS and the gateway and indicated by the blue highlighted line.\nMetrics and Policies ALM supports Load and Integrity as the standard metrics that are defined in the VNFC Descriptor. The Integrity metric is used in the Healing policy at the VNFC level to heal the VNF if it is broken. The Heal policy is designed at the VNFC level so no need to design this on the NS level.\nThe Load metric is used for the Scaling policy of a VNF, and can be defined in the VNF or in the NS Descriptor. In our example of a voice-service, we had already Promoted the Load metric when we designed the VNF so it becomes available on the NS level, and can be used for the Scaling policy in the next section. In a NS, you can also promote a metric if it needs to be used in a higher-level NS that uses this NS as an element.\nClustering and auto-scaling A cluster and scaling policy can be defined in a Network Service. The Load metric is used to automatically trigger a scale in or out action. The scale policy can be based on the (promoted) metric of the VNFC/VNF itself, or from another element in the Assembly (for example scale out VNF1 based on the Load metric of VNF2).\nIn order to design a cluster and an auto-scaling policy, you click the \u0026ldquo;\u0026hellip;\u0026rdquo; button on the right hand panel in the Clustering section of the Properties, and select \u0026ldquo;Edit\u0026rdquo;. A new popup window appears. You first select what the cluster looks like. You define the Initial quantity that is instantiated once the VNF is instantiated, the scaling increments, and the minimum and maximum number of nodes.\nYou can then select the auto-scaling policy and provide a name for the policy. You select the Load metric that is to be used, and the scale-out and scale-in thresholds and smoothing periods (to avoid scaling-in and -out actions happening too frequently if thresholds are violated only once for example). The below image shows the Clustering Property window.\nOperations In the same way as with Metrics, the Operations that were defined in the VNFC Descriptor that need to be used in a higher level Network Service, need to be promoted first.\nYou can do this by clicking the \u0026ldquo;\u0026hellip;\u0026rdquo; button in the Operations Property section in the right hand panel and clicking \u0026ldquo;Promote Operation\u0026rdquo;. See the image below for an example of where to promote the Operation for the Kamailio VNFC) and making that available for use in the VNF/Network Service.\nYou would Promote the Operations inn the Network Service if you want to use these for Relationships in a higher-level NS.\nRelationships Relationships are very important for the Intent Engine as it uses them to determine in what order and what tasks/elements need to be established, re-established, or torn down when an Intent is carried out.\nA NS can for example consist of multiple VNFs. You can add multiple VNFs by adding new elements in the NS Design and repeating the steps above. It is possible to establish relationships between the VNF elements, for example if one VNF needs to be installed before the other, or if an operation needs to be performed.\nIn order to establish a relationship, you can drag and drop the orange ball on the element in the design between the two VNF elements. Using the orange ball on the left of the element means a FROM relation that is then dragged to the output orange ball on the right side of the other element. Once the line is drag-and-dropped, a new screen pops up where you define the relation.\nYou can enter the name of the relationship, the from and to elements, whether it needs to be a pre- or post- condition, and select the source and target state of the element.\nYou can also select the operations that need to be carried out once the relationship is created and when it is removed. You can also provide additional properties and values. When you have filled in all the required information, press \u0026ldquo;Done\u0026rdquo; and an orange line will appear between the VNFCs that represents the relationship. On the right-hand panel the Relationships section is then also populated.\nTesting a Network Service See Behaviour Testing\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/installation/lm/production/install-lm/","title":"Installing ALM","tags":[],"description":"","content":"This section assumes you have followed the configuration steps and now have:\n a helm-foundation Helm chart an lm-configurator Helm chart an lm-helm - Helm chart a custom values YAML file  Installing with Helm You can now install the various LM Helm charts in the order listed above, using the helm install command\nhelm install \u0026lt;your-helm-chart\u0026gt; --name \u0026lt;your-release-name\u0026gt; --namespace \u0026lt;your-namespace\u0026gt; -f \u0026lt;your-custom-values-file\u0026gt; -f \u0026lt;your-additional-flavour-files\u0026gt;\r cicdhub-helm-chart - path to the Helm chart your-release-name - unique name used to identify this installation in Helm your-namespace - Kubernetes namespace to install into (leave out to use default). This namespace must be repeated for all 3 helm installs. your-custom-values-file - path to the YAML file created with any configuration overrides (if you have any) your-additional-flavour-files - path to the YAML file created with any configuration overrides (if you have any). The same flavour must be repeated for all 3 helm installs.  This should be done as follows:\nFoundation Install Install helm-foundation first.\nFor example, to install with a namespace of lm and a flavour of Basic HA:\nhelm install helm-foundation-2.0.3-45.tgz --name foundation --namespace lm -f custom-values.yaml -f ./flavours/ha/ha-values.yaml\rNow check the status of the pods. If you have kubectl installed you can use the get pods commands:\nkubectl get pods\rNAME READY STATUS RESTARTS AGE\rfoundation-cassandra-0 1/1 Running 0 11m\rfoundation-elasticsearch-client-7bf56fd98-f5th8 1/1 Running 0 11m\rfoundation-elasticsearch-data-0 1/1 Running 0 11m\rfoundation-elasticsearch-master-0 1/1 Running 0 11m\rfoundation-elasticsearch-master-1 1/1 Running 0 10m\rfoundation-filebeat-xbxh2 1/1 Running 0 11m\rfoundation-kafka-0 1/1 Running 0 11m\rfoundation-openldap-585489f595-4gf28 1/1 Running 1 11m\rfoundation-vault-7c96bfdf6d-7nthc 1/1 Running 0 11m\rfoundation-vault-init-rsh7c 0/1 Completed 0 11m\rfoundation-zookeeper-0 1/1 Running 0 11m\rOnce the installation of Foundation is complete, you can proceed to the installation of lm-configurator.\nLM Configurator Install Next, install LM Configurator.\nFor example, to install with a namespace of lm and again using the flavour of Basic HA:\nhelm install lm-configurator-2.0.3-50.tgz --name lm-configurator --namespace lm -f custom-values.yaml -f ./flavours/ha/ha-values.yaml\rNow check the status of the running lm-configurator job and wait for it to show as completed.\nNAME READY STATUS RESTARTS AGE\rlm-configurator-qw5wd 0/1 Completed 0 10m\rOnce this is complete, proceed to the installation of LM\nLM Install Finally, the LM Helm chart can be installed.\nFor example, to install with a namespace of lm and again using the flavour of Basic HA:\nhelm install lm-helm-2.0.3-208.tgz --name lm --namespace lm -f custom-values.yaml -f ./flavours/ha/ha-values.yaml\rMonitor the status of the pods, waiting for the ishtar and nimrod pods to show as ready:\nNAME READY STATUS RESTARTS AGE\rapollo-66485f5f6b-mknbw 1/1 Running 0 21m\rconductor-0 1/1 Running 0 21m\rdaytona-844fcdd94f-zxhp7 1/1 Running 0 21m\rgalileo-0 1/1 Running 0 21m\rishtar-659b695bbf-xxlxf 1/1 Running 0 21m\rnimrod-5f5f5cfb44-hd7hn 1/1 Running 0 21m\rrelay-545cccc7c7-2z8qp 1/1 Running 0 21m\rsimple-rm-74dfd46cb9-wcgl6 1/1 Running 0 11m\rtalledega-7c69b75c7b-dq8df 1/1 Running 0 21m\rwatchtower-5f6b7f7fb7-2mb4b 1/1 Running 0 21m\rOnce all pods are shown with at least one ready instance, you may continue to Accessing Services\nAccessing Services Be sure to update any ports, hostnames, usernames and passwords shown if you provided overrides in your custom values file.\nLM No Host If you kept the noHost Ingress rules enabled for LM, then the UI and API will be accessible at:\n UI - https://:32443/ui (non-secure http://:32080/ui) API - https://:32443 (non-secure http://:32080)  The ports shown above are the default for the Ingress controller installed as part of LM, so adjust them if you configured alternative values.\nHost Based If you have kept the host Ingress rule enabled, then the UI and API will also be accessible at:\n UI - https://ui.lm:32443/ui (non-secure http://ui.lm:32080/ui) API - https://app.lm:32443 (non-secure http://app.lm:32080)  As Ingress uses the hostname to route traffic to the desired service, you will need to either:\n add the hostname(s) to your hosts file configure a proxy (such as Apache2) to route traffic to the target Kubernetes ingress controller configure a nameserver to route traffic targeting the hostname(s) to your Kubernetes cluster  Update Hosts File Adding hostnames to your hosts file is an easy way to get started but is not suitable long term as this will need to be repeated on every machine that will access the Hub.\nIf you\u0026rsquo;d like to configure access to Ingress routes this way, then add the following to your hosts file:\n\u0026lt;your-host-ip-address\u0026gt; ui.lm app.lm vault.lm\rYou may now access the UI at https://ui.lm:32443 and Gogs at https://app.lm:32443.\nProxy with Apache2 If you have an available Apache2 server (or can install one) then it\u0026rsquo;s possible to make the Ingress hosts available through an IP address.\nYou will need to add the Ingress hostnames to the hosts file of the machine running Apache2, see Hostfile.\nCreate a site configuration file:\ntouch /etc/apache2/sites-available/lm.conf\rAdd the following configuration to the file, updating the Ingress hostnames and ports if overridden. Replace \u0026lt;your-lm-ui-proxy-port\u0026gt;, \u0026lt;your-lm-api-proxy-port\u0026gt; with desired ports for your instances.\nListen \u0026lt;your-lm-ui-proxy-port\u0026gt;\r\u0026lt;VirtualHost *:\u0026lt;your-lm-ui-proxy-port\u0026gt;\u0026gt;\rSSLProxyEngine On\rSSLEngine On\r# Set the path to SSL certificate\rSSLCertificateFile /etc/apache2/ssl/ca.crt\rSSLCertificateKeyFile /etc/apache2/ssl/ca.key\rProxyPass / https://ui.lm:32443/\rProxyPassReverse / https://ui.lm:32443/\r\u0026lt;/VirtualHost\u0026gt;\rListen \u0026lt;your-lm-api-proxy-port\u0026gt;\r\u0026lt;VirtualHost *:\u0026lt;your-lm-api-proxy-port\u0026gt;\u0026gt;\rSSLProxyEngine On\rSSLEngine On\r# Set the path to SSL certificate\rSSLCertificateFile /etc/apache2/ssl/ca.crt\rSSLCertificateKeyFile /etc/apache2/ssl/ca.key\rProxyPass / https://app.lm:32443/\rProxyPassReverse / https://app.lm:32443/\r\u0026lt;/VirtualHost\u0026gt;\rAdd the site to Apache2 (you may need to reload the Apache2 service for the changes to take affect):\na2ensite lm.conf\rYou may now access the UI at http://\u0026lt;apache2-server-ip\u0026gt;:\u0026lt;your-lm-ui-proxy-port\u0026gt; and the API at http://\u0026lt;apache2-server-ip\u0026gt;:\u0026lt;your-lm-api-proxy-port\u0026gt;.\nFirst login After the hostnames have been registered visit http://\u0026lt;your-host-ip\u0026gt;:32443/ui in a supported browser to access the UI.\nThe default username and password for UI access is:\n Username: Jack Password: jack  The default client credentials for API access is:\n ClientId: LmClient Secret: pass123  Vault To modify or inspect system configuration, access the Vault UI at https://vault.lm:32443. As mentioned in accessing LM, Ingress uses the hostname to route traffic to the desired service, so you will need to either add the hostname to your hosts file or configure a proxy/nameserver to route traffic to it.\nThe configured access token will be needed to login to Vault. The value will be output within the logs of the vault-int job, and can be found by inspecting them, for example (replace with correct name of vault-init pod):\nkubectl logs foundation-vault-init-8fkvg\nThis will reveal the configured access token at the end of the log statements, e.g.\nVault access token is: \u0026lt;vault token shown here\u0026gt;\nOpenldap To view or configure available system users, access OpenLDAP by determining the randomly allocated NodePort and connecting via your chosen client software as described in this guide Connecting to OpenLDAP\nKibana Kibana can be accessed at http://kibana.lm:32080 (or http://\u0026lt;your-host-ip\u0026gt;:31001 to use NodePort access).\nAs mentioned in accessing LM, Ingress uses the hostname to route traffic to the desired service, so you will need to either add the hostname to your hosts file or configure a proxy/nameserver to route traffic to it.\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/administration/security/users-ldap-admin/","title":"Manage Users with LDAP Admin","tags":[],"description":"","content":"The following guide is an extension of User Configuration with OpenLDAP with more detailed instructions for using LDAP Admin client. This guide is tested with version 1.8.3 of the client.\nPre-requisites To complete this guide you will need:\n An existing installation of the Agile Lifecycle Manager (ALM) OpenLDAP accessible to your client LDAP Admin Client installed on your environment  Connecting to OpenLDAP  To get LDAP connection details follow this guide. Open the LDAP Admin window, click Start -\u0026gt; Connect -\u0026gt; New Connection and fill in details collected earlier.  Click \u0026ldquo;Test Connection\u0026rdquo; to make sure it is working before confirming with \u0026ldquo;OK\u0026rdquo;. An LDAP server that is configured for ALM will have a layout of organization units groups and people on the left-hand panel.   Adding new user with access to ALM To add a new user  Right click on ou=people -\u0026gt; New -\u0026gt; Entry Add the fields extendedPerson and uidObject in the left-hand column for Objectclass The right-hand panel will be filled with a list of attributes for you to fill in, with cn sn and uid being compulsory. It is recommended that the 3 fields should have the same value - the username of your new user. For userPassword, you need to convert plain-text passwords to BCrypt. Use bcrypt-cli or browser-based generator. Check that password hashes have the $2a prefix. Choose uid=... in field Rdn: in the upper left-hand corner then click the \u0026ldquo;Save and exit\u0026rdquo; icon.   To grant access to a new user  Read about default groups and decide the group to which you want to add your user. After adding a new user, return to the LDAP Admin main panel and right click on wanted group e.g cn=Portal -\u0026gt; Edit Entry. Add a new row, choose member as attribute and add as value uid=\u0026lt;created-user-uid\u0026gt;,ou=people,dc=lm,dc=com. Save and exit   Test access with ALM  Go to the ALM UI and log in with created username and password. Verify that your new user has correct access according to their roles.  "},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/administration/configuration/configuring-lm/","title":"Managing Configuration","tags":[],"description":"","content":"Pre-requisites To follow this guide you will need:\n an existing installation of the Agile Lifecycle Manager (ALM)  Vault Many of the applications installed as part of the Lifecycle Manager load configuration from a configuration repository service. Vault is the chosen storage mechanism for any secure config required by the microservices. It is an industry standard approach to storing sensitive information and stores all secure config in a highly encrypted format.\nConfiguration may be loaded into Vault during the installation through the lm-configurator Helm chart but it is also possible to update the configuration post-install by visiting the Vault UI.\nApplication Configuration Application configuration is persisted using a secrets engine named lm in Vault.\nA secret will exist for each LM service inside the lm engine, with configuration properties stored as JSON content. Any configuration stored under the secret named application will be shared among all services.\nConfiguration is stored within the secret as key/value pairs, where the name is the dot-separated name of the configuration item.\nClick the ‘Edit’ toggle switch to enter edit mode\nNew property values should be added as dot separated keys.\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/reference/behaviour-testing/step-reference/metric-recording/","title":"Metric Recording","tags":[],"description":"","content":"Start Recording Description Start recording metric values by monitoring Kafka for messages detailing a measurement for a given Resource and metric type.\nYou must create a metric definition before recording metrics, as this informs the monitor how to identify the Resource and metric type for each message.\nRecording stops when explicitly requested with the Stop Recording step or when the scenario finishes.\nPasses when:\n the recording has started  Fails when:\n the recording could not start  Properties    Property Description     metricName Chosen name to be given to this metric. This name can be used in later steps to assert the value of the metric or to stop recording   metricDefinitionName Name of a previously created metric definition in this scenario. This informs the monitor how to identifiy the Resource and metric type   metricType Type of metric to record e.g. failed_calls, CPU, memory. The monitor will check Kafka for messages with this type, recording the values for any that match (and also match the given resourceId). Messages for other types of metric, even those for the same Resource, are ignored   topicName The name of the Kafka topic to be monitored for metric messages   resourceId The ID of the Resource the metric should be recorded for. The monitor will check Kafka for messages for this Resource, recording the values for any that match (and also match the given metricType). Messages for other Resources are ignored    Start Recording Multiple Description Start recording metric values by monitoring Kafka for messages detailing a measurement for a given Resource and multiple metric types. Essentially this step allows you to record multiple metrics for the same Resource in one line, rather than repeating the Start Recording Metric step.\nYou must create a metric definition before recording metrics, as this informs the monitor how to identify the Resource and metric type for each message.\nPasses when:\n the recording has started  Fails when:\n the recording could not start  Properties    Property Description     metricDefinitionName Name of a previously created metric definition in this scenario. This informs the monitor how to identifiy the Resource and metric type   topicName The name of the Kafka topic to be monitored for metric messages   resourceId The ID of the Resource the metric should be recorded for. The monitor will check Kafka for messages for this Resource, recording the values for any that match (and also match one of the given metrics.metricId). Messages for other Resources are ignored   metrics Table of metric types (metricId) to record and the chosen name for each, used in later steps to assert the value of the metric or to stop recording    Metrics Table:\n   Parameter Description     Metric Id Type of metric to record e.g. failed_calls, CPU, memory. The monitor will check Kafka for messages with this type, recording the values for any that match (and also match the given resourceId)   Metric Name Chosen name to be given to this metric. This name can be used in later steps to assert the value of the metric or to stop recording    Stop Recording Description Stop recording a metric by name.\nPasses when:\n the recording has stopped  Fails when:\n the given metric name is not known the recording could not be stopped  Properties    Property Description     metricName Name given to the metric when recording started    Start Recording Load Description Start recording the load metric for a given Resource. The message of a load metric is standard to LM, so has a pre-determined message format which means there is not requirement for a metric definition.\nPasses when:\n the recording has started  Fails when:\n the recording could not be started  Properties    Property Description     metricName Chosen name to be given to this metric. This name can be used in later steps to assert the value of the metric or to stop recording   metricType Type of metric to record e.g. load. The monitor will check Kafka for messages with this type, recording the values for any that match (and also match the given resourceId). Messages for other types of metric, even those for the same Resource, are ignored   topicName The name of the Kafka topic to be monitored for metric messages   resourceName The name of the Resource the metric should be recorded for. The monitor will check Kafka for messages for this Resource, recording the values for any that match (and also match the given metricType). Messages for other Resources are ignored    Start Recording Integrity Description Start recording the integrity metric for a given Resource. The message of a integrity metric is standard to LM, so has a pre-determined message format which means there is not requirement for a metric definition.\nPasses when:\n the recording has started  Fails when:\n the recording could not be started  Properties    Property Description     metricName Chosen name to be given to this metric. This name can be used in later steps to assert the value of the metric or to stop recording   metricType Type of metric to record e.g. integrity. The monitor will check Kafka for messages with this type, recording the values for any that match (and also match the given resourceId). Messages for other types of metric, even those for the same Resource, are ignored   topicName The name of the Kafka topic to be monitored for metric messages   resourceName The name of the Resource the metric should be recorded for. The monitor will check Kafka for messages for this Resource, recording the values for any that match (and also match the given metricType). Messages for other Resources are ignored    "},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/resource-engineering/drivers/onboarding/","title":"Onboarding","tags":[],"description":"","content":"Onboard Drivers You can onboard drivers with the resourcedriver add command from LMCTL v2.1+. Read more about all the commands from LMCTL resourcedriver command reference\nDrivers by default run with SSL enabled and need to be onboarded with an SSL certificate that allows Brent to communicate with them. To retrieve the SSL certificate for a deployed driver (in base64-encoded PEM format), run the following command:\nkubectl get secret \u0026lt;secretName\u0026gt; -o 'go-template={{index .data \u0026quot;tls.crt\u0026quot;}}' | base64 -d \u0026gt; \u0026lt;secretName\u0026gt;.pem\rWhere \u0026lt;secretName\u0026gt; is the name of the secret holding the SSL certificate for the driver, as show in the table below:\n   Driver SSL Secret     Ansible ald-tls   Openstack ovd-tls   Kubernetes kubedriver-tls    And \u0026lt;secretName\u0026gt;.pem is the local file in which you store the driver\u0026rsquo;s certificate.\nUse this file name again as the --certificate parameter for lmctl onboarding commands. Ensure the --type assigned is the same as the infrastructure-type in your resource package. The --url must be a valid endpoint that is reachable from your installation of Brent; in a Kubernetes cluster you can use the internal service name and port:\nlmctl resourcedriver add --type openstack --url https://os-vim-driver:8292 dev-env --certificate ovd-tls.pem\rlmctl resourcedriver add --type ansible --url https://ansible-lifecycle-driver:8293 dev-env --certificate ald-tls.pem\rlmctl resourcedriver add --type kubernetes --url https://kubedriver:8294 dev-env --certificate kubedriver-tls.pem\rDrivers can be removed at a later time with the delete commands:\nlmctl resourcedriver delete --type openstack dev-env\rNext Steps You should now have an understanding of Resource packages and drivers. You should also have an understanding of how to install and onboard drivers for usage in your environment.\nNow is a great time to run through an example of creating a Resource and using it as part of an Assembly in LM. See the example resource.\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/operations/","title":"Operations","tags":[],"description":"","content":""},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/cicd/pipeline/","title":"Pipeline","tags":[],"description":"","content":""},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/installation/resource-manager/","title":"Resource Manager","tags":[],"description":"","content":""},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/installation/lm/production/configuration/sizing/","title":"Sizing","tags":[],"description":"","content":"Configuring Sizing of Agile Lifecycle Manager (ALM) The default install of LM makes use of the default settings, which will deploy the Standard flavour of LM. This includes:\n single instance of Foundation services execution of the LM configurator job to configure default settings single instance of ALM services, with all security features enabled  Alternative Sizing Flavours The flavours directory included in the lm-helm-charts package contains helm values and other useful files for installing alternative recommended deployments of the ALM. These different flavours allows for variations of sizing for LM.\nThe following flavours are available:\n Basic HA Minimal  Basic HA Directory: ha This flavour is intended for a multi-node Kubernetes cluster and includes the following characteristics:\n Multiple instances of LM and Foundation services are deployed The default storage class is used for Persistent Volumes Kafka topics are created with a higher replication factor and partition count  Minimal Directory: minimal This flavour restricts the amount of CPU/Memory/Disk usage of the Kubernetes resources.\nInstallation To install LM with any of these flavours, ensure the following values file from the lm-helm-charts package are provided during the Helm install process which is described later in this guide:\n   Flavour Helm values file     Basic HA ./flavours/ha/ha-values.yaml   Minimal ./flavours/minimal/minimal-values.yaml    Next steps You can now proceed to review the configuration for Security\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/installation/cicdhub/access-config/","title":"Access Configuration","tags":[],"description":"","content":"Hostnames and Ports By default, the CI/CD Hub services are configured for access as described below:\n   Service Type Address     nexus NodePort \u0026lt;your-cicdhub-host\u0026gt;:32739   gogs NodePort \u0026amp; Ingress \u0026lt;your-cicdhub-host\u0026gt;:32734 OR git.cicdhub:\u0026lt;nginx-ingress-port\u0026gt;   jenkins NodePort \u0026amp; Ingress \u0026lt;your-cicdhub-host\u0026gt;:32732 OR jenkins.cicdhub:\u0026lt;nginx-ingress-port\u0026gt;   openldap NodePort \u0026lt;your-cicdhub-host\u0026gt;:32737, SSL: https://\u0026lt;your-cicdhub-host\u0026gt;:32738   docker registry NodePort \u0026lt;your-cicdhub-host\u0026gt;:32736   nginx-ingress NodePort \u0026lt;your-cicdhub-host\u0026gt;:32080, SSL: https://\u0026lt;your-cicdhub-host\u0026gt;:32443    All ports and Ingress hosts listed above are configurable through the Helm chart. Add any of the following to your custom values to override them:\ndockerregistry:\rservice:\rnodePort: \u0026lt;override-port\u0026gt;\rgogs:\rservice:\rhttpNodePort: \u0026lt;override-port\u0026gt;\ringress:\rhosts:\r- \u0026lt;override-hostname\u0026gt;\rjenkins:\rmaster:\rnodePort: \u0026lt;override-port\u0026gt;\ringress:\rhostName: \u0026lt;override-hostname\u0026gt;\ropenldap:\rservice:\rnodePort: \u0026lt;override-port\u0026gt;\rsslNodePort: \u0026lt;override-port\u0026gt;\rnexus:\rnodePort: \u0026lt;override-port\u0026gt;\rnginx-ingress:\rcontroller:\rservice:\rnodePorts:\rhttp: \u0026lt;override-port\u0026gt;\rhttps: \u0026lt;override-port\u0026gt;\rIngress An Ingress controller is required to allow HTTP and HTTPS Ingress routes from outside the cluster. See Kubernetes - Ingress for more information.\nThe nginx-ingress controller is included with the CI/CD Hub for convenience. If your Kubernetes cluster has an existing Ingress controller then you can disable nginx with:\nnginx-ingress:\renabled: false\rNext Steps Continue configuring your installation with Security\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/installation/resource-manager/ansible-rm/access-config/","title":"Access Configuration","tags":[],"description":"","content":"Hostnames and Ports The Ansible RM is configured to allow access through NodePorts. The default values for the HTTP and HTTPS ports are shown below. Override any defaults by adding them to your custom values.\nservice:\rnodePort: 31080\rsslNodePort: 31081\rNext Steps You may now complete your installation of the Ansible RM\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/resource-engineering/resource-packages/brent/infrastructure-keys-resource/creating-shared-infrastructure-key/","title":"Add Shared Infrastructure Key to LM","tags":[],"description":"","content":"Add the Shared Infrastructure Key to LM See infrastructure key management for a general introduction to infrastructure keys.\nThe resource descriptor defines a server1_ssh_key key property which is populated with the apache1_server_key shared infrastructure key by LM when the Resource driver that stands up the Openstack Apache instance is called. This (SSH) key is assumed to already exist in Openstack (see prerequisites), and also needs to be added (at least the private key portion) as a shared infrastructure key to LM to allow Resource drivers to communicate with the infrastructure in order to provision it.\nNote: in the case of Openstack it\u0026rsquo;s not possible to create key pairs programmatically (using HEAT templates, for example) with existing public and private key portions created using another mechanism (such as openssl), so create the key pair in Openstack UI and use the downloaded private key (e.g server1_ssh_key.pem) when adding as a shared infrastructure key in LM.\nThe best way to add a shared infrastructure key is to use LMCTL. For example:\nlmctl key add --public server1_ssh_key.pub --private server1_ssh_key.pem dev apache1_server_key\rwhere apache1_server_key is the name of LM shared infrastructure key, which should have the same value as the key pair you created in Openstack. server1_ssh_key.pem is the private key of the Openstack SSH keypair (optional, but required if a Resource driver needs to communicate with the infrastructure), and server1_ssh_key.pub contains the public key portion of the Openstack SSH key pair (optional).\nInstantiate Resource Once you have created the shared infrastructure key you may move on to onboard and instantiate the Resource\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/reference/lm-api/api-definition/behavior-testing-api/","title":"Behaviour Testing","tags":[],"description":"","content":""},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/cicd/developing-a-project/create-new-vnf-project/","title":"Create New Project","tags":[],"description":"","content":"Objectives Create a new VNF or Network Service project that will hold all of its descriptor, test and lifecycle script artifacts.\nPre-requisites   Access to a CI/CD Hub and a development environment with a Lifecycle Manager and VIM\n  Design details of the new VNF or Network Service.\n  VNFCs or VNFs it will contain (name and version)\n  Software Images associated with each VNFC. (There may be multiple images for each target VIM type, e.g. OpenStack or k8s)\n  Set-up a new local project For a sample set of network service and virtual network function examples: Marketplace\nTo create a new project, a directory containing the package structure will be crated first on your local machine. For convenience the local directory structure can be created with the LMCTL command line tool and filled in later with the VNF or Network Service artifacts.\nTo create a new VNF run the following LMCTL command:\n$ lmctl project create --name \u0026lt;\u0026lt;YOUR_VNF\u0026gt;\u0026gt; --version 0.1 --servicetype VNF --vnfc \u0026lt;\u0026lt;VNFC1\u0026gt;\u0026gt; --vnfc \u0026lt;\u0026lt;VNFC2\u0026gt;\u0026gt; \u0026lt;\u0026lt;YOUR_VNF_DIR\u0026gt;\u0026gt;\rReplace \u0026laquo;YOUR_VNF\u0026raquo; and \u0026laquo;YOUR_VNF_DIR\u0026raquo; with the name of your new project and the name of the directory you want to create with the package structure in it. For each VNFC contained within your VNF, keep adding \u0026ndash;vnfc \u0026laquo;VNFC_NAME\u0026raquo; options, and their structure will be templated into the new package directory.\nTo create a new Network Service run the following LMCTL command:\n$ lmctl project create --name \u0026lt;\u0026lt;YOUR_NS\u0026gt;\u0026gt; --version 0.1 --servicetype NS \u0026lt;\u0026lt;YOUR_NS_DIR\u0026gt;\u0026gt;\rA Network Service package directory structure with the name \u0026laquo;YOUR_NS\u0026raquo; will be created in the directory \u0026laquo;YOUR_NS_DIR\u0026raquo;.\n$ echo \u0026quot;description: Brief description\u0026quot; \u0026gt;\u0026gt; ./\u0026lt;\u0026lt;YOUR_VNF_DIR\u0026gt;\u0026gt;/Descriptor/assembly.yml\r$ mkdir -p \u0026lt;\u0026lt;YOUR_VNF_DIR\u0026gt;\u0026gt;/Behaviour/Tests\r$ mkdir -p \u0026lt;\u0026lt;YOUR_VNF_DIR\u0026gt;\u0026gt;/Behaviour/Templates\rIf you are creating a VNF and assuming you are using the Ansible RM, each VNFC contained in the newly created VNF project will need to have the following base structure to capture each VNFCs ansible lifecycle scripts:\n VNFCs\r├── \u0026lt;\u0026lt;VNFC1\u0026gt;\u0026gt;\r├── descriptor\r│ └── \u0026lt;\u0026lt;VNFC1\u0026gt;\u0026gt;.yml\r├── lifecycle\r│ ├── Configure.yml\r│ ├── Install.yml\r│ ├── Integrity.yml\r│ ├── Start.yml\r│ ├── Stop.yml\r│ └── Unistall.yml\r└── Meta-Inf\r└── manifest.MF\rDownload the following bash script and run it with the name, version and location of your project directory. It will then create the structure above with minimal content\n#!/usr/bin/env bash\rname=\u0026quot;$1\u0026quot;\rversion=\u0026quot;$2\u0026quot;\rlocation_dir=\u0026quot;$3\u0026quot;\r# Create Directories\rcd ${location_dir}\rmkdir -p ${name}/descriptor\rmkdir -p ${name}/lifecycle\rmkdir -p ${name}/Meta-Inf\r# Create manifest file\recho \u0026quot;name: ${name}\rversion: ${version}\rresource-manager: ansible\u0026quot; \u0026gt; ${name}/Meta-Inf/manifest.MF\r# Create miminal descriptor\recho \u0026quot;name: resource::${name}::${version}\rdescription: ${name} vnfc\u0026quot; \u0026gt; ${name}/descriptor/${name}.yml\r# Create (empty) lifecycle playbooks\recho \u0026quot;---\u0026quot; \u0026gt; ${name}/lifecycle/Install.yml\recho \u0026quot;---\u0026quot; \u0026gt; ${name}/lifecycle/Unistall.yml\recho \u0026quot;---\u0026quot; \u0026gt; ${name}/lifecycle/Configure.yml\recho \u0026quot;---\u0026quot; \u0026gt; ${name}/lifecycle/Start.yml\recho \u0026quot;---\u0026quot; \u0026gt; ${name}/lifecycle/Stop.yml\recho \u0026quot;---\u0026quot; \u0026gt; ${name}/lifecycle/Integrity.yml\rThe above steps do not have to be performed for a Network Service package.\nCommit the package Next step is to create a local git project and commit the changes you have made. You can run the following commands on you local machine to do this.\n$ cd \u0026lt;\u0026lt;YOUR_VNF\u0026gt;\u0026gt;\r$ git init\r$ echo \u0026quot;_lmctl\u0026quot; \u0026gt; .gitignore\r$ git add .\r$ git commit -m 'initial project'\rCreate the project on Git Server (Gogs) To share your project and to attach it to the CI Server, you need to push it to Gogs.\n log in to Gogs Create \u0026ldquo;+ New Repository\u0026rdquo; with the same name as your local project (e.g. \u0026laquo;YOUR_VNF\u0026raquo;) Select the organization (e.g. \u0026lsquo;marketplace\u0026rsquo;) Note the commands that Gogs gives you for \u0026lsquo;Push an existing repository from the command line\u0026rsquo; and use them in the next step.  Push project to Gogs, on your local machine:\n$ git remote add origin http://\u0026lt;GOGS ADDRESS\u0026gt;/\u0026lt;\u0026lt;YOUR_VNF\u0026gt;\u0026gt;.git\r$ git push -u origin master Create git develop branch\n$ git checkout -b develop\rIn Gogs, go to the project you have just created and select settings and collaboration. Add New Collaborator for Jenkins and any user you wish to be able the make changes to the new project.\nYou are now ready to start developing and testing your new project.\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/resource-engineering/drivers/creating-drivers/","title":"Creating Resource Drivers","tags":[],"description":"","content":"Introduction You may create your own resource driver to support integration with virtual infrastructure and to complete transitions and operations with different scripting mechanisms. The driver must fulfill resource driver APIs and be capable of accepting HTTP requests and returning responses asynchronously on Kafka.\nThe programming language used to create the driver is at the discretion of the user. The SOL 003 Lifecycle Driver is a good example of writing a driver with Java.\nThe Ignition Framework has been created to help developers quickly build drivers in Python. It provides boilerplate code for handling API requests and sending asynchronous Kafka responses. This enables developers to focus on the real functionality of their driver by providing implementations of an interface per API to act on the target request. Read through the documentation on creating a driver to learn how to get started.\nThe Resource Driver API is specified (in OpenAPI format) in the Ignition Framework. Copy the definition and paste it into the Swagger Editor to view a pretty version of it.\nResource Infrastructure and Operation (Lifecycle) Requests On a transition or operation request, Brent will send the following to the resource driver:\n the contents of the driver lifecycle scripts directory in the package, for the given lifecycle type (e.g. if the \u0026ldquo;Install\u0026rdquo; lifecycle driver type is \u0026ldquo;ansible\u0026rdquo; then the \u0026ldquo;Lifecycle/ansible\u0026rdquo; directory is sent, similarly if the \u0026ldquo;Create\u0026rdquo; lifecycle driver type is \u0026ldquo;openstack\u0026rdquo; then \u0026ldquo;Lifecycle/openstack\u0026rdquo; with the Openstack infrastructure template is sent) the name of the transition/operation to execute e.g. Create, Start, Stop, operationA, etc. all values set on the properties of the Resource descriptor additional system properties including the intended Resource ID and name the deployment location the Resource instance is to be deployed to  The following diagram shows the interactions between Brent and drivers The driver will assign the request an ID and return this to Brent so it may be used to track the completion of the request. If a Create transition is requested, the IDs of relevant infrastructure objects will be generated by the resource driver handling the infrastructure type and included in associatedTopology property of the Resource. Once the lifecycle script has executed successfully, the driver should also return any outputs of the scripts.\nIf any of the outputs have the same name as a property passed in, then Brent overwrites the existing value with the value of the output. The descriptor property values are passed back up to LM, making any new values viewable on the instance. Any non-descriptor properties are kept by Brent for the next request.\nThe structure of resource package driver subdirectories under the Lifecycle directory is defined not by Brent but by the resource driver.\nExecute The /execute operation is used to execute any lifecycle transition or operation requests for a Resource. The request will include:\n BASE64-encoded zip of the contents of the Lifecycle/[driver type] folder. the name of the transition/operation to execute system properties such as the fue Resource instance ID and name property values, including all of the values for properties on the descriptor and details for infrastructure objects in associatedTopology the deployment location the Resource is assigned to  The operation is expected to be potentially long running. To cater for this, the API states it should return an immediate response with an identifier for the request. Ignition will handle these aspects of request handling, or at least provide APIs that handle/managed it for you.\nOn completion, the driver is expected to produce a message on the lm_vnfc_lifecycle_execution_events Kafka topic with the request identifier and details of the success or failure of the operation. Brent is constantly monitoring this topic for messages and will use the earlier returned request identifier to determine the operation each message relates to. For a full definition of the expected message format see Asynchronous Message Format\nFind Reference Infrastructure Assemblies designed in Agile Lifecycle Manager (ALM) are allowed to have external references to Resources which may or may not have been created by Brent. These Resources should be discovered by Brent, using a name and deployment location. Once the Resource has been found, operations can be executed on it. Resource Driver API /references/find is designed for Brent to discover infrastructures previously created on a LM deployment location, to be referenced as a resource associated topology.\nOn a find reference request, Brent will send the following to the resource driver:\n BASE64-encoded zip of the contents of the Lifecycle/[driver type] folder (for example contents may be files in TOSCA discovery templates if defined as such by your resource driver) instanceName - the name of the target infrastructure to be found (this can be used by the driver as input to the template) name, type, and properties of the deployment location that the Resource instance is in  If the driver can find the infrastructure required, then it should return a resource ID managing the infrastructure and infrastructure ID as associatedTopology for that resource, along with outputs from the discovery template. If any of outputs have the same name as a property on the Resource descriptor, then Brent sets the instance value with the value of the output. The descriptor property values are passed back up to LM, making the new value viewable on the instance.\nAny outputs that do not have the same name as a descriptor property are kept by Brent so they are still usable in any lifecycle scripts.\nThe operation is expected to return immediately with a result. If the infrastructure could not be found then the response should be empty, rather than throwing an error.\nThe types of infrastructure that may be found with this operation is at the discretion of the driver implementation and the infrastructure type of the deployment location.\nAsynchronous Message Format Messages on Kafka for resource lifecycle operations should be of the following format:\n{\r\u0026quot;requestId\u0026quot;: \u0026quot;\u0026lt;identifier of the request\u0026gt;\u0026quot;,\r\u0026quot;status\u0026quot;: \u0026quot;\u0026lt;IN_PROGRESS, COMPLETE or FAILED\u0026gt;,\r\u0026quot;failureDetails\u0026quot;: {\r\u0026quot;failureCode\u0026quot;: \u0026quot;\u0026lt;RESOURCE_ALREADY_EXISTS, RESOURCE_NOT_FOUND, INFRASTRUCTURE_ERROR, INSUFFICIENT_CAPACITY or INTERNAL_ERROR\u0026gt;\u0026quot;,\r\u0026quot;description\u0026quot;: \u0026quot;\u0026lt;error message\u0026gt;\u0026quot;\r},\r\u0026quot;outputs\u0026quot;: {\r\u0026quot;outputPropertyIfExists\u0026quot;: \u0026quot;outputValue\u0026quot;\r},\r\u0026quot;associatedTopology\u0026quot;: {\u0026lt;infrastructure (e.g Kubernetes, Openstack) objects associated with the resource\u0026gt;}\r}\r"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/cicd/developing-a-project/","title":"Developing a Project","tags":[],"description":"","content":""},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/resource-engineering/resource-packages/brent/basic-resource/instantiate-resource/","title":"Instantiate Resource","tags":[],"description":"","content":"Push Resource Push the Resource project to your target environment with LMCTL\nlmctl project push example-env\rAdd Resource to design Create a new Assembly in the Agile Lifecycle Manager (ALM) UI\nOpen the Assembly design and click \u0026ldquo;Add Element\u0026rdquo; in the top right\nAdd an element using the hw-apache2 Resource descriptor now available in the environment\nSelect the \u0026ldquo;apache2-service\u0026rdquo; node in the designer and open the right hand panel by selecting the menu icon in the top right\nClick \u0026ldquo;Add New Property\u0026rdquo; from the right hand panel to add a new resourceManager property\nRepeat step 5 to create a deploymentLocation property (set the default value to the name your intended deployment location)\nMap the resourceManager and deploymentLocation property from the service node to the server Resource added earlier, by dragging from the blue connection point of the properties to the connection point of the same properties on the Resource node\nSave the design using the \u0026ldquo;Save\u0026rdquo; button in the top right\nCreate Assembly Navigate to \u0026ldquo;Recent Assembly Instances\u0026rdquo; and select \u0026ldquo;Create\u0026rdquo; in the top right. Give the new instance a name and select the \u0026ldquo;apache2-service\u0026rdquo; descriptor created earlier\nEnsure the resourceManager property is set to brent and the deploymentLocation property is set to the name of your Openstack deployment location\nReview the settings and click \u0026ldquo;Create\u0026rdquo;. Once the process has completed you will see the Resource has been created and assigned an internal and public IP\nOpen your Openstack dashboard to see the groups and compute instance created for this Resource\nUninstall the Assembly and wait for the infrastructure to be removed\nNext Steps You have now successfully created an instance of a Resource with infrastructure. Continue reading to add lifecycle scripts to configure software through transitions.\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/administration/security/manage-client-credentials/","title":"Manage Client Credentials","tags":[],"description":"","content":"The following guide details how to configure Client Credentials in Agile Lifecycle Manager (ALM) on a deployed system. These credentials are required to make API requests from external API clients and integrated systems. To assist in maintaining a secure system it is recommended that each external client have its own set of credentials.\nPre-requisites To complete this guide you will need:\n An existing installation of the ALM A REST client Understanding of how to make authenticated requests to LM  Add client credentials Client credentials can be created using the credentials REST API. This API is protected, so authentication is required to access it. You may use the default admin client or any existing client which ultimately has the SECADMIN privilege to perform these actions (typically LmClient, see Default Client Credentials) to do this.\nTo create a new set of credentials, send a POST request to https://\u0026lt;your-lm-api-host\u0026gt;:\u0026lt;your-lm-api-port\u0026gt;/api/credentials with the following JSON fields:\n clientId - the ID of your new client clientSecret - a chosen password for the client authorisedGrantTypes - types of authentication allowed  client_credentials - allows this client to make API requests with just it\u0026rsquo;s own credentials password - allows this client to make API requests with it\u0026rsquo;s own credentials combined with a valid LM username/password refresh_token - can only be used with password. Allows the client to refresh an expired access token using a refresh token (avoids needing to request a user for their password again)   accessTokenValidity - time in seconds an access token remains valid for (defaults to 1200) refreshTokenValidity - time in seconds a refresh token remains valid for (defaults to 30600) roles - the roles assigned to this client (should only be used in combination with the client_credentials grant type, as any client with password should assume the roles of the user at runtime)  Example payload:\n{\r\u0026quot;clientId\u0026quot;: \u0026quot;NewClient\u0026quot;,\r\u0026quot;clientSecret\u0026quot;: \u0026quot;thisisthesecret\u0026quot;,\r\u0026quot;authorisedGrantTypes\u0026quot;: [\r\u0026quot;client_credentials\u0026quot;,\r],\r\u0026quot;accessTokenValidity\u0026quot;: 300,\r\u0026quot;roles\u0026quot;: [\r\u0026quot;SLMAdmin\u0026quot;\r]\r}\rUpdating Client Credentials You may update client credentials with a similar request to a create, but instead the payload must be sent in a PUT request to: https://\u0026lt;your-lm-api-host\u0026gt;:\u0026lt;your-lm-api-port\u0026gt;/api/credentials/\u0026lt;client-id\u0026gt;\nUI login timeout values Login sessions to the LM UI have the following behaviour which is are governed by the token validity parameters set for the NimrodClient:\n after a set period (default 20 minutes), the user\u0026rsquo;s session token (access token) will be automatically refreshed. At this point LDAP is consulted to verify the current suspended status and roles of the user after a longer set period (default 8.5 hours), the user\u0026rsquo;s session is invalidated and they will be forced to login again  In order to modify front-end timeout values, the UI client credentials (NimrodClient) must be updated, adjusting the values of accessTokenValidity and refreshTokenValidity\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/reference/behaviour-testing/step-reference/metric-assertions/","title":"Metric Assertions","tags":[],"description":"","content":"Verify always under threshold Description Checks that all values seen whilst recording a metric are under a given value. Any values equal or over the threshold will cause the step to return a status of failed.\nPasses when:\n all values for the recorded metric are under the given threshold  Fails when:\n any values for the recorded metric are equal to or over the given threshold  Properties    Property Description     metricName Name given to a metric being recorded in this scenario   thresholdValue A numeric value representing the threshold    Verify always over threshold Description Checks that all values seen whilst recording a metric are over a given value. Any values equal or under the threshold will cause the step to return a status of failed.\nPasses when:\n all values for the recorded metric are over the given threshold  Fails when:\n any values for the recorded metric are equal to or under the given threshold  Properties    Property Description     metricName Name given to a metric being recorded in this scenario   thresholdValue A numeric value representing the threshold    Verify always equal to value Description Checks that all values seen whilst recording a metric are equal to a given value. Any values under or over the threshold will cause the step to return a status of failed.\nPasses when:\n all values for the recorded metric are equal the given value  Fails when:\n any values for the recorded metric are under or over the given value  Properties    Property Description     metricName Name given to a metric being recorded in this scenario   value A numeric value that all recorded metric values should be equal to    Verify currently under threshold Description Checks that the last value seen whilst recording a metric is under a given value. If the value is equal or over the threshold, the step returns a status of failed.\nPasses when:\n the last value for the recorded metric is under the given threshold  Fails when:\n the last value for the recorded metric is equal to or over the given threshold  Properties    Property Description     metricName Name given to a metric being recorded in this scenario   thresholdValue A numeric value representing the threshold    Verify currently over threshold Description Checks that the last value seen whilst recording a metric is over a given value. If the value is equal or under the threshold, the step returns a status of failed.\nPasses when:\n the last value for the recorded metric is over the given threshold  Fails when:\n the last value for the recorded metric is equal to or under the given threshold  Properties    Property Description     metricName Name given to a metric being recorded in this scenario   thresholdValue A numeric value representing the threshold    Verify currently equal to value Description Checks that the last value seen whilst recording a metric is equal to a given value. If the value is under or over the threshold, the step returns a status of failed.\nPasses when:\n the last value for the recorded metric is equal the given value  Fails when:\n the last value for the recorded metric is under or over the given value  Properties    Property Description     metricName Name given to a metric being recorded in this scenario   value A numeric value that the last value should be equal to    Wait for metric to be under threshold Description Waits for a new value of a recorded metric to be under a given threshold. Any values recorded before the execution of this step are ignored.\nThe step will wait for the time specified, once a valid value is seen the step returns a status of pass and the scenario may continue. The step allows the user to specify the result of the step if no valid value is seen before the specified waiting time period.\nPasses when:\n a value is recorded for the metric that is under the threshold no value is recorded that is under the threshold and the timeoutResult is set to Pass  Fails when:\n no value is recorded that is under the threshold and the timeoutResult is set to Fail  Properties    Property Description     maxWait A numeric value for the amount of time to wait (unit specified by timeUnit property)   timeUnit Unit of the maxWait property: milliseconds, seconds or minutes   metricName Name of the metric being recorded in this scenario   value The threshold value   timeoutResult The status of the step if no value is seen under the threshold for the given maxWait time period: Pass or Fail    Wait for metric to be over threshold Description Waits for a new value of a recorded metric to be over a given threshold. Any values recorded before the execution of this step are ignored.\nThe step will wait for the time specified, once a valid value is seen the step returns a status of pass and the scenario may continue. The step allows the user to specify the result of the step if no valid value is seen before the specified waiting time period.\nPasses when:\n a value is recorded for the metric that is over the threshold no value is recorded that is over the threshold and the timeoutResult is set to Pass  Fails when:\n no value is recorded that is over the threshold and the timeoutResult is set to Fail  Properties    Property Description     maxWait A numeric value for the amount of time to wait (unit specified by timeUnit property)   timeUnit Unit of the maxWait property: milliseconds, seconds or minutes   metricName Name of the metric being recorded in this scenario   value The threshold value   timeoutResult The status of the step if no value is seen over the threshold for the given maxWait time period: Pass or Fail    Wait for metric to be equal Description Waits for a new value of a recorded metric to be equal to a given value. Any values recorded before the execution of this step are ignored.\nThe step will wait for the time specified, once a valid value is seen the step returns a status of pass and the scenario may continue. The step allows the user to specify the result of the step if no valid value is seen before the specified waiting time period.\nPasses when:\n a value is recorded for the metric that is equal to the value no value is recorded that is equal to the value and the timeoutResult is set to Pass  Fails when:\n no value is recorded that is equal to the value and the timeoutResult is set to Fail  Properties    Property Description     maxWait A numeric value for the amount of time to wait (unit specified by timeUnit property)   timeUnit Unit of the maxWait property: milliseconds, seconds or minutes   metricName Name of the metric being recorded in this scenario   value The threshold value   timeoutResult The status of the step if no value is seen equal to the given threshold for the given maxWait time period: Pass or Fail    "},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/administration/configuration/upgrade-scaling-policy/","title":"Scale a Running ALM","tags":[],"description":"","content":"The following guide explains how to upgrade a running Agile Lifecycle Manager (ALM) to scale with CPU usage.\nPre-requisites:  The Kubernetes metrics server must be running in your Kubernetes environment for scaling of ALM to work.  Configure ALM Existing installations of ALM can be scaled by adding an Horizontal Pod Autoscaler via kubectl.\nFor each service that you want to scale do the following:\n Ensure that the service has resources set, you can do this by running:  kubectl describe deployment \u0026lt;service name\u0026gt;\rIf the deployment has resources set then you will see the following:\nContainers:\r\u0026lt;service name\u0026gt;:\r...\rLimits:\rcpu: \u0026lt;maximum CPU to allocate to pod\u0026gt;\rmemory: \u0026lt;maximum memory to allocate to pod\u0026gt;\rRequests:\rcpu: \u0026lt;initial CPU to allocate to pod\u0026gt;\rmemory: \u0026lt;initial memory to allocate to pod\u0026gt;\rIf there are Limits and Requests for your chosen service then skip to Step 3 otherwise in order to add these to a service you can add the following to a custom values file:  \u0026lt;service name\u0026gt;:\rapp:\rresources: limits:\rcpu: \u0026lt;maximum CPU to allocate to pod\u0026gt;\rmemory: \u0026lt;maximum memory to allocate to pod\u0026gt;\rrequests:\rcpu: \u0026lt;initial CPU to allocate to pod\u0026gt;\rmemory: \u0026lt;initial memory to allocate to pod\u0026gt;\rFor example if you wanted to scale Apollo your custom values would look as follows:\napollo:\rapp:\rresources: limits:\rcpu: 500m\rmemory: 4Gi\rrequests:\rcpu: 200m\rmemory: 2Gi\rAnd do a helm upgrade:\nhelm upgrade lm \u0026lt;helm chart\u0026gt; --values \u0026lt;custom values file\u0026gt;\rOnce your service has resources then you can create an autoscaler using a command like this:  kubectl autoscale deployment \u0026lt;ALM service deployment\u0026gt; --min=\u0026lt;minimum pods\u0026gt; --max=\u0026lt;maximum pods\u0026gt; --cpu-percent=\u0026lt;cpu target percentage\u0026gt;\rFor example if you want to scale Apollo between 1 and 3 pods whenever a pod exceeds 80% CPU you would use this command:\nkubectl autoscale deployment apollo --min=1 --max=3 --cpu-percent=80\rYou can inspect your Horizontal Pod Autoscalers by running:  kubectl get hpa\r"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/installation/lm/production/configuration/security/","title":"Security","tags":[],"description":"","content":"Configuring Security in Lifecycle Manager The recommended installation of Lifecycle Manager is with security enabled. This will give the following benefits:\n HTTPS access to services Role-based Access Control to the Lifecycle Manager user interface Protected access to LM APIs  Disabling Security The easiest way to disable security during installation is by including the No Security flavour\nNo Security Flavour Directory: no-security This flavour disables LM security and is intended for development/testing purposes. To be more precise, this flavour disables:\n HTTPS access to the LM UI and API inclusion of Openldap LM UI authenticated access LM API authenticated access  This flavour may be used in combination with other flavours included in the LM package.\nInstallation To install LM with this flavour, ensure the following values file from the lm-helm-charts package is provided during the Helm install process which is described later in this guide: `./flavours/no-security/no-security-values.yaml\nChanging default passwords It is recommended all system passwords are modified from their default values during installation for added security. This can be achieved as follows:\nOpenLDAP Administrator Password In order to modify the default password for administering OpenLDAP, modify this Helm value in your custom values file.\nglobal: ldap: managerPassword: lmadmin # modify this with your custom password LM API Administration User In order to administer Client Credentials in LM, the system is setup with some default Client Credential. These can be modified or added to by declaring values in the custom Helm values file as follows (change only the clientId and clientSecret, leaving grantTypes and roles as shown below):\nconfigurator: security: lm: clientCredentials: - clientId: LmClient clientSecret: pass123 grantTypes: client_credentials roles: SLMAdmin Internal Clients LM installs with some clients configured to allow secure communication internally. These clients are mandatory, but their default passwords can be changed. Additionally their respective token validitity periods can also be configured (all values in seconds). Their meanings are as follows:\n the value configurator.security.lm.nimrod.accessTokenValidity represents the frequency with which the Lifecycle Manager user interface will refresh its authentication session. A refresh involves a seamless reconfirmation of the user\u0026rsquo;s validity and available roles with the LDAP user store. the value configurator.security.lm.nimrod.refreshTokenValidity represents the period of time over which the access token can be automatically refreshed. After this has expired, the user will be forced to login again. the value configurator.security.lm.doki.accessTokenValidity represents the frequency with which the backend will internally reauthenticate itself when executing Behaviour Testing scenarios.  In order to modify the default passwords or timeouts, add these Helm values into your custom values file.\nconfigurator: security: lm: nimrod: clientSecret: pass123 # modify this with your custom password accessTokenValidity: 1200 # 20 minutes refreshTokenValidity: 30600 # 8.5 hours doki: clientSecret: pass123 # modify this with your custom password accessTokenValidity: 1200 # 20 minutes Keystore Password Some of the services within LM host SSL certificates used for secure internal communications. To configure the password used for this certificate keystore, modify this value:\nconfigurator: security: lm: keyStorePassword: keypass # modify this with your custom password Vault Access Token In order to lookup application configuration, the services of LM will need to connect to Vault. They require use of an access token to facilitate this. To modify this from the default value, generate a new UUID and replace it in the following Helm value into your custom values file.\nvaultInit: vaultToken: # insert your new generated UUID This access token will be automatically setup in Vault during installation, and the services of LM will use this for authentication with Vault. This token can be used to login to the Vault UI after installation in order to modify application configuration.\nBy default, Vault tokens will be created with a duration as specified in the values file for helm-foundation, i.e.\nvault: vault: config: max_lease_ttl: 87600h # this is the maximum duration a token can exist, before which it can be renewed, after which it will be revoked default_lease_ttl: 87600h # this is the default duration a token will exist, after which it will be revoked, unless renewed See also Managing Vault Tokens\nNext Steps Continue the configuration of Storage\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/reference/behaviour-testing/step-reference/utilities/","title":"Utilities","tags":[],"description":"","content":"Delay Description Delay the scenario from continuing for a specified time period. This does not prevent any background tasks, such as metric recording, from continuing.\nProperties    Property Description     sleepTime Numeric value representing the amount of time to delay (unit specified by timeUnit property)   timeUnit Unit of measurement for the sleepTime property: milliseconds, seconds, minutes    "},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/resource-engineering/resource-packages/brent/basic-resource/add-lifecycle/","title":"Add Lifecycle","tags":[],"description":"","content":"Create Lifecycle and Operation Scripts The next step is to create scripts for the the standardized lifecycles (start, stop, configure, install, integrity and uninstall) and operations defined in the resource descriptor.\nThe scripts may be implemented using any scripting language of choice, as long as there is a Resource Driver capable of executing them. Scripts are located in a subdirectory of the \u0026lsquo;Lifecycle\u0026rsquo; directory of the resource package, named after the type of the Resource driver that will use them. The contents of this directory is defined by the Resource driver. For example, in this example we are using the Ansible Lifecycle driver, each lifecycle script maps to an Ansible playbook that is called the same as the lifecycle step, and is located in the \u0026lsquo;Lifecycle/ansible/scripts/\u0026rsquo; directory. The Configure script would be located at \u0026lsquo;Lifecycle/ansible/scripts/Configure.yml\u0026rsquo; or \u0026lsquo;Lifecycle/ansible/scripts/Configure.yaml\u0026rsquo; (note: case is important). More information about Ansible playbooks can be found here.\nA resource package does not need to implement all lifecycle scripts, only those defined in the resource descriptor.\nIn addition to the standard lifecycle scripts, specific operations may also be added. Operations are bespoke transitions that are invoked by the Resource manager when Agile Lifecycle Manager (ALM) is establishing relationships between two resources. These allow for action to take place between the resources to enable a Network Service to work. Operations and their definitions are part of the public interface of a resource.\nCreate an operation by adding the operation script in the \u0026lsquo;Lifecycle/ansible/scripts\u0026rsquo; folder where the name of the operation is the name of the script. For example, the \u0026ldquo;deleteNetworkInterface\u0026rdquo; script should be located in the file \u0026lsquo;Lifecycle/ansible/scripts/deleteNetworkInterface.yml\u0026rsquo; or \u0026lsquo;Lifecycle/ansible/scripts/deleteNetworkInterface.yaml\u0026rsquo; (note, case is important).\nAdd Lifecycle Scripts If you\u0026rsquo;ve completed the earlier sections to this guide then you should be in possession of a project for a Resource, which when instantiated, creates some basic infrastructure to standup a compute instance.\nWe now need to add some software to our a Resource to give it a real function. Typically, a Resource engineer would achieve this by adding scripts to be executed as part of the Install, Configure and Start transitions.\nWe will be using the Ansible lifecycle driver, so our scripts will take the form of playbooks.\nConfigure Inventory To use the Ansible lifecycle driver we will need to setup an inventory file which tells Ansible how to SSH to the compute instance created as part of the Resource infrastructure.\nUpdate the inventory file under the Lifecycle/ansible/config with the following contents:\n[resource]\rresource-inst\rRename the example-host.yml file under the Lifecycle/ansible/config/host_vars directory to resource-inst.yml (it\u0026rsquo;s important to Ansible that this file matches the name of our host, which is shown on the 2nd line of our inventory file)\nmv Lifecycle/ansible/config/host_vars/example-host.yml Lifecycle/ansible/config/host_vars/resource-inst.yml Update the contents of resource-inst.yml with:\n---\ransible_host: {{ properties.public_ip }}\ransible_user: ubuntu\ransible_ssh_pass: ubuntu\ransible_become_pass: ubuntu\ransible_sudo_pass: ubuntu\rThe value of ansible_host is set a templated variable. This allows us to configure the inventory at runtime with a property from our instance. In this case, as we don\u0026rsquo;t know the IP address of the compute instance until it has been created, we will substitute the address at runtime using the public_ip property (remember this property is in our Resource descriptor and is set by an output with the same name in our infrastructure template).\nAdd Install The install lifecycle transition is a suitable time to:\n check the infrastructure is ready for use (check we can SSH to it) install any software required by the Resource  We will add a script to install Apache2 as a service:\nCreate a new file called Install.yaml in Lifecycle/ansible/scripts\ntouch Lifecycle/ansible/scripts/Install.yaml\rAdd the following content to the playbook. The tasks listed will install Apache2 but not start it\n- name: Install\rhosts: resource\rbecome: yes\rgather_facts: False\rtasks:\r- name: Pause for Python\rraw: while [ -z \u0026quot;$(command -v python)\u0026quot; ]; do sleep 1; done\r- name: Prevent apache2 auto-starting on install)\rshell: ln -s /dev/null /etc/systemd/system/apache2.service\r- name: Install apache2\rretries: 60\rdelay: 1\rapt:\rupdate_cache: yes\rname: apache2\rregister: apache_install\runtil: '\u0026quot;Could not get lock\u0026quot; not in apache_install[\u0026quot;stderr\u0026quot;]'\r- name: Verify service apache2\rsystemd:\rname: apache2\rstate: started\renabled: yes\rmasked: no\rNow on an install transition, Apache2 will be installed on the compute instance of the Resource.\nAdd Configure The configure lifecycle transition is a suitable time to setup configuration files or any other auxiliary items required by the intended software so it is ready to use.\nIn this guide we will add a script to configure Apache2\u0026rsquo;s default site with our own content.\nIn Lifecycle/ansible/scripts create a new directory called site, then create an index.html file in that directory\nmkdir Lifecycle/ansible/scripts/site \u0026amp;\u0026amp; touch Lifecycle/ansible/scripts/site/index.html\rUpdate index.html with the following content\n\u0026lt;html\u0026gt;\r\u0026lt;head\u0026gt;\r\u0026lt;title\u0026gt;Hello, {{ properties.greeting_receiver }}\u0026lt;/title\u0026gt;\r\u0026lt;/head\u0026gt;\r\u0026lt;body\u0026gt;\r\u0026lt;h1\u0026gt;Hello, {{ properties.greeting_receiver }}\u0026lt;/h1\u0026gt;\r\u0026lt;p\u0026gt;Success!\u0026lt;/p\u0026gt;\r\u0026lt;/body\u0026gt;\r\u0026lt;/html\u0026gt;\rIn Lifecycle/ansible/scripts create a new directory called conf, then create a hw-site.conf file in that directory\nmkdir Lifecycle/ansible/scripts/conf \u0026amp;\u0026amp; touch Lifecycle/ansible/scripts/conf/hw-site.conf\rUpdate hw-site.conf with the following content\n\u0026lt;VirtualHost *:{{properties.site_port}}\u0026gt;\rServerAdmin {{properties.site_admin_email}}\rDocumentRoot /var/www/{{properties.site_name}}\rErrorLog ${APACHE_LOG_DIR}/error.log\rCustomLog ${APACHE_LOG_DIR}/access.log combined\r\u0026lt;/VirtualHost\u0026gt;\rCreate a new file called Configure.yaml in Lifecycle/ansible/scripts\ntouch Lifecycle/ansible/scripts/Configure.yaml\rAdd the following content to the playbook. The tasks listed will add a new site to Apache2 and remove the default\n---\r- name: Configure\rhosts: resource\rbecome: yes\rgather_facts: False\rtasks:\r- name: Disable default site\rcommand: a2dissite 000-default.conf\rfailed_when: false\r- name: Remove default site config\rfile:\rstate: absent\rpath: /etc/apache2/sites-available/000-default.conf\r- name: Create new site directory\rfile:\rstate: directory\rpath: /var/www/{{ properties.site_name }}\r- name: Copy site html\rtemplate:\rsrc: site/index.html\rdest: /var/www/{{ properties.site_name }}/index.html\rmode: '0755'\r- name: Add site config\rtemplate:\rsrc: conf/hw-site.conf\rdest: /etc/apache2/sites-available/hw-site.conf\rmode: '0755'\r- name: Enable new site\rcommand: a2ensite hw-site.conf\rYou may have noticed a few additional properties referenced in these files, such as site_name, they must be added to our descriptor. Open Definitions/lm/resource.yaml and add the new properties.\nproperties:\r...other properties...\rgreeting_receiver:\rdefault: world\rdescription: Name to append to the greeting on the default site (i.e. Hello, World!)\rsite_port:\rdefault: 80\rdescription: Port to bind the apache2 site\rsite_admin_email: default: host@localhost\rdescription: ServerAdmin setting for the apache2 site\rsite_name:\rdefault: hw\rdescription: Name given to the apache2 site\rAdd Start The start lifecycle transition is a suitable time to activate/enable software so it is ready for use. In this guide we will add a script to start our configured Apache2 site.\nCreate a new file called Start.yaml in Lifecycle/ansible/scripts\ntouch Lifecycle/ansible/scripts/Start.yaml\rAdd the following content to the playbook\n---\r- name: Start\rhosts: resource\rbecome: yes\rgather_facts: False\rtasks:\r- name: Restart apache2\rsystemd:\rstate: restarted\rname: apache2\rAdd Stop As we have a start transition which enables the Apache2 service it is a good idea to also add a stop, which will disable the Apache2 service.\nCreate a new file called Stop.yaml in Lifecycle/ansible/scripts\ntouch Lifecycle/ansible/scripts/Stop.yaml\rAdd the following content to the playbook\n---\r- name: Stop\rhosts: resource\rbecome: yes\rgather_facts: False\rtasks:\r- name: Stop apache2\rsystemd:\rstate: stopped\rname: apache2\rEnabling Lifecycle Transitions To indicate to LM that the Resource supports additional transitions we must update the Resource descriptor (Definitions/lm/resource.yaml). Open this file and add \u0026ldquo;Install\u0026rdquo;, \u0026ldquo;Configure\u0026rdquo;, \u0026ldquo;Start\u0026rdquo; and \u0026ldquo;Stop\u0026rdquo; to the list of supported lifecycle:\ninfrastructure:\rOpenstack: {}\rlifecycle:\rCreate:\rdrivers:\ropenstack:\rselector:\rinfrastructure-type:\r- Openstack\rInstall:\rdrivers:\ransible:\rselector:\rinfrastructure-type:\r- Openstack\rConfigure:\rdrivers:\ransible:\rselector:\rinfrastructure-type:\r- Openstack\rStart:\rdrivers:\ransible:\rselector:\rinfrastructure-type:\r- Openstack\rStop:\rdrivers:\ransible:\rselector:\rinfrastructure-type:\r- Openstack\rDelete:\rdrivers:\ropenstack:\rselector:\rinfrastructure-type:\r- Openstack\rNotice how ansible is used under drivers section to each lifecycle Install, Configure, Start, and Stop, while Create and Delete use openstack. This section defines which Resource Driver should be used to handle the transition for the default infrastructure type. Since ansible is being used for the majority of lifecycle steps here, a more succint way of specifying this is as follows:\ninfrastructure:\rOpenstack: {}\rdefault-driver:\ransible:\rselector:\rinfrastructure-type:\r- Openstack\rlifecycle:\rCreate:\rdrivers:\ropenstack:\rselector:\rinfrastructure-type:\r- Openstack\rInstall: {}\rConfigure: {}\rStart: {}\rStop: {}\rDelete: {}\rdrivers:\ropenstack:\rselector:\rinfrastructure-type:\r- Openstack\rThe default-driver section specifies to use ansible, unless overridden by a drivers definition in a specific lifecycle configuration.\nRe-instantiate The Resource is ready for usage. Push the Resource to your target environment and create an instance of the Assembly (created earlier in Instantiate Resource).\nYou\u0026rsquo;ll see the Install, Configure and Start transitions execute for the Resource, upon completion you should be able to access the hosted site with at: http://\u0026lt;public_ip\u0026gt;:\u0026lt;site_port\u0026gt;.\nTo see the Stop transition execute, make the Assembly inactive by opening the \u0026ldquo;New Intent\u0026rdquo; dropdown in the top right and selecting \u0026ldquo;Make Inactive\u0026rdquo;.\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/users/","title":"End Users","tags":[],"description":"","content":"End Users of Agile Lifecycle Manager (ALM) The types of users for ALM can be divided into a set of roles based on their main responsibility:\n VNF Engineering Network Service Design Installation and Administration Operations  The main use cases of the user roles are described below.\nVNF Engineering The VNF Engineer is a VNF subject matter expert with responsibility for the creation and/or onboarding of third party VNF software through the creation of an operational package that can be managed by ALM. The VNF Developer creates VNF functional tests and behaviour scenarios\nNetwork Service Designer  The Service Engineer is responsible for realizing a network service from a documented network service design and the set of VNFs which it is composed from. The Service Engineer translates service designs from other SMEs into a Network Service design. The Service Engineer implements the Network Service design by connecting Network Service and VNF packages to a location based network configuration. The Service Engineer validates that network service packages and all their included third party VNFs behave as expected. The Service Engineer brings network service and VNF packages through a pre-production verification process and publishes the packages for production use when appropriate.  Operations  Operations manages the life of service instances within production environments. Operations is responsible for executing new or planned changes to service instances. Operations is responsible for diagnosing and resolving reported errors or anomalies.  Installer and Administrator  The Administrator configures and administers ALM, tailoring the system to manage their environment. The Administrator models the infrastructure universe that ALM will be deploying to, modeling placement groups and available infrastructure features. The Administrator configures pre-production environments that can run and test network service designs. The Administrator onboards Resource drivers and external resource managers that are required to run a network service.  "},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/network-service-design/develop-a-network-service-package/finalize-a-project/","title":"Finalize a Project","tags":[],"description":"","content":"At this stage, you have created your NS and behaviour tests, and you are happy with the design. You can pull the project back to your local directory before pushing it to your GIT.\nPull project to local directory If you want to pull the design you have made in the ALM UI into your local project directory run the following command:\nlmctl project pull dev\rThis will pull all NS artifacts you created in the designer out from your ALM development installation into your local directory.\nCommit, Create and Push Project to GIT Ones you have pulled your design to your local directory you can push the design to the remote Git that you have setup for this project as follows (from the project directory)\nCommit and push the package  $ git add .\r$ git commit -m 'NS design'\r$ git push\rCreating CI Pipeline To learn how to setup a CI/CD pipeline for your VNF package, read\n Create CI Pipeline  "},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/resource-engineering/resource-packages/brent/infrastructure-keys-resource/instantiate-resource/","title":"Instantiate Resource","tags":[],"description":"","content":"Push Resource Push the Resource project to your target environment with LMCTL\nlmctl project push example-env\rAdd Resource to design Create a new Assembly in the Agile Lifecycle Manager (ALM) UI\nOpen the Assembly design and click \u0026ldquo;Add Element\u0026rdquo; in the top right\nAdd an element using the hw-apache2 Resource descriptor now available in the environment\nSelect the \u0026ldquo;apache2-service\u0026rdquo; node in the designer and open the right hand panel by selecting the menu icon in the top right\nClick \u0026ldquo;Add New Property\u0026rdquo; from the right hand panel to add a new resourceManager property\nRepeat step 5 to create a deploymentLocation property (set the default value to the name your intended deployment location)\nAlso create a server1_ssh_key property to be mapped to the hw-apache2 Resource (set the default value to the name of your existing Openstack SSH key pair). The use of this property is explained when you created the resource\nMap the resourceManager and deploymentLocation and server1_ssh_key property from the service node to the server Resource added earlier, by dragging from the blue connection point of the properties to the connection point of the same properties on the Resource node\nSave the design using the \u0026ldquo;Save\u0026rdquo; button in the top right\nCreate Assembly Navigate to \u0026ldquo;Recent Assembly Instances\u0026rdquo; and select \u0026ldquo;Create\u0026rdquo; in the top right. Give the new instance a name and select the \u0026ldquo;apache2-service\u0026rdquo; descriptor created earlier\nEnsure the resourceManager property is set to brent and the deploymentLocation property is set to the name of your Openstack deployment location.\nIf you decide to use a different SSH key pair than the default one you\u0026rsquo;ve set in the assembly design, you can provide it as property here.\nReview the settings and click \u0026ldquo;Create\u0026rdquo;. Once the process has completed you will see the Resource has been created and 2 servers are assigned an internal and public IP\nOpen your Openstack dashboard to see the groups and 2 compute instances created for this Resource\nUninstall the Assembly and wait for the infrastructure to be removed\nNext Steps You have now successfully created an instance of a Resource with infrastructure. Continue reading to add lifecycle scripts to configure software through transitions.\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/administration/security/managing-vault-tokens/","title":"Managing Vault Tokens","tags":[],"description":"","content":"Managing Vault Tokens Vault tokens do not have an indefinite expiry and will need to be renewed or regenerated after their expiration. By default, the Vault token used by LM to access configuration will be created with a duration as per the default value configured in the Vault settings, i.e.\nvault: vault: config: max_lease_ttl: 87600h # this is the maximum duration a token can exist, before which it can be renewed, after which it will be revoked default_lease_ttl: 87600h # this is the default duration a token will exist, after which it will be revoked, unless renewed Expired LM Vault Tokens If the Vault token used by LM expires, then any services in LM will be unable to access configuration, and as such will be unable to start. Most services will just silently fail the startup if this happens, but Conductor should provide a log message if it or another services fails to start for this reason.\nIf this is the case, then the Vault token should be renewed (or replaced in the case where it can no longer be renewed).\nRenewing Vault Token If a token is renewed, then its expiry will be extended so that its new TTL is the value in default_lease_ttl. A token can only exist up until the value specified max_lease_ttl, after which it will be automatically revoked and not able to be renewed. At this point it will be necessary to create a new token. See Forceful token renewal through the Vault CLI (skip revoke token command).\nToken renewal through the Vault UI It is possible to renew a token through the Vault UI. You must login with the token to be renewed, then click the link in the top-right dropdown.\nToken renewal through the Vault CLI Alternatively, it is possible to renew the token through the Vault CLI. Firstly, exec onto the Vault Kubernetes pod, e.g.\nkubectl exec -it foundation-vault-f46b47456-24cmc /bin/sh\rTo use the Vault CLI, it is always necessary to execute the following\nexport VAULT_ADDR='https://127.0.0.1:8200'\rThen execute the following commands (substituting in the valid LM token ID):\nvault login \u0026lt;insert LM token ID here\u0026gt;\rvault renew\rTo establish the current expiry time for a given token, execute the following (will display for the currently logged in token)\nvault token lookup\rForceful token renewal through the Vault CLI In order to more forcefully renew a token, to reset its max TTL and ensure it is valid for UI login, it will be necessary to revoke and create the token.\nIt is possible to create a new token with the same ID as a previously revoked token. However if a new token ID is required, all LM services will need to be updated with the new value. This is normally stored in the Kubernetes secret called vault-token. Simply replacing the token ID within this secret will be sufficient to enable LM\u0026rsquo;s services to use the new token next time they access Vault on restart.\nIn order to revoke and create tokens, the root Vault token will be required.\nExecute the following commands (substituting in the valid root and LM token IDs):\nvault login \u0026lt;insert root token ID here\u0026gt;\rvault token revoke \u0026lt;insert LM token ID here\u0026gt;\rvault token create -policy=lm-policy -id=\u0026lt;insert LM token ID here\u0026gt;\r"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/reference/lm-api/api-definition/resource-manager/","title":"Resource Manager","tags":[],"description":"","content":""},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/installation/cicdhub/security/","title":"Security","tags":[],"description":"","content":"Access Credentials Many of the CI/CD Hub services have default usernames and passwords that may only be changed after installation. However, a few allow a value to be provided through the custom values.\nThe default values for each service are shown below. Override any defaults by adding them to your custom values.\nglobal:\r## Note that the Openldap password is set through a global variable\rldap:\rmanagerPassword: admin\rdomain: lm.com\rgogs:\rpostgresql:\rpostgresUser: admin\rpostgresPassword: admin\rjenkins:\rmaster:\radminUser: admin\radminPassword: admin\rDefault credentials for any service not shown here will be detailed later in the installation.\nNext Steps You may now complete your installation of the CI/CD Hub\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/installation/lm/production/configuration/storage/","title":"Storage","tags":[],"description":"","content":"Storage Class By default, any service requiring persistence is configured to use the default provisioner of your Kubernetes cluster. You can check your default with kubectl:\nkubectl get storageclass\rThe default storage class will be shown with (default) alongside it\u0026rsquo;s name. If you have no default, you can mark an existing class as the default with:\nkubectl patch storageclass \u0026lt;your-class-name\u0026gt; -p '{\u0026quot;metadata\u0026quot;: {\u0026quot;annotations\u0026quot;:{\u0026quot;storageclass.kubernetes.io/is-default-class\u0026quot;:\u0026quot;true\u0026quot;}}}'\rAlternatively, you may explicitly set the storage class for each service by adding storageClass fields into your custom values file.\ncassandra: persistence: storageClass: elasticsearch: master: persistence: storageClass: data: persistence: storageClass: kafka: persistence: storageClass: zookeeper: persistence: storageClass: openldap: persistence: storageClass: Here is an example of storageClass configuration for Host Path Persistent Volumes:\ncassandra: persistence: storageClass: cassandra-storage ​ elasticsearch: master: persistence: storageClass: esm-storage data: persistence: storageClass: esd-storage ​ kafka: persistence: storageClass: kafka-storage zookeeper: persistence: storageClass: zookeeper-storage ​ openldap: persistence: storageClass: openldap-storage Storage Size The size of persistent volume created for each service may be configured by adding resource size settings to the custom values file. See Kubernetes - Persistent Volumes for more information about how the size of a volume is managed.\nThe default values for each service are shown below. Override any defaults by adding them to your custom values file.\ncassandra: persistence: size: \u0026#34;60Gi\u0026#34; elasticsearch: master: persistence: size: \u0026#34;20Gi\u0026#34; data: persistence: size: \u0026#34;30Gi\u0026#34; kafka: persistence: size: \u0026#34;150Gi\u0026#34; zookeeper: persistence: size: \u0026#34;15Gi\u0026#34; openldap: persistence: size: \u0026#34;5Gi\u0026#34; Next Steps Continue configuring your installation with External LDAP\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/administration/configuration/themes/","title":"Themes","tags":[],"description":"","content":""},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/cicd/developing-a-project/upload-images/","title":"Uploading Images","tags":[],"description":"","content":"Objectives Manage versions of all VNF or test software images in a general repository. Making the images available in a formal manner to the CI/CD tools.\nPre-requisites  You are provided with the software images to upload on your local machine.  Repository Structure The image directory structure in the CI/CD nexus general repository is as follows:\n.\r├── raw\r└── vdus\r└── \u0026lt;vdu-name\u0026gt;\r├── \u0026lt;image-name-version\u0026gt;.\u0026lt;type\u0026gt;\r├── image-3.4.12.qcow2\r├── image-3.4.00.tar\tTo upload an image into nexus:\n Log into the Nexus repository UI Check the image does not already exist under the image name. Assuming it required\u0026hellip; On the left nav-bar, select \u0026lsquo;Upload\u0026rsquo; Select \u0026lsquo;raw\u0026rsquo; as the repository to upload to. Select the image file from your local machine Enter \u0026lsquo;VDUs/\u0026lt;vnfname\u0026gt; in the \u0026lsquo;Directory\u0026rsquo; field e.g. \u0026lsquo;VDUs/myvnf\u0026rsquo; press \u0026lsquo;Upload\u0026rsquo;  You can also curl the images in, e.g.:\n $ curl -v -u \u0026lt;username\u0026gt;:\u0026lt;password\u0026gt; --upload-file ./image-3.4.12.qcow2 http://\u0026lt;hub_nexus_url\u0026gt;/repository/raw/vdus/firewall/image-3.4.12.qcow2\rLoad images into target VIM:\nFollow instructions from the target VIM to load images, e.g. for openstack\n $ openstack image create --disk-format qcow2 --container-format bare \\\r--public --file ./image-3.4.12.qcow2 image-3.4.12\r"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/resource-engineering/resource-packages/brent/infrastructure-keys-resource/add-lifecycle/","title":"Add Lifecycle","tags":[],"description":"","content":"Create Lifecycle and Operation Scripts The next step is to create scripts for the the standardized lifecycles (start, stop, configure, install, integrity and uninstall) and operations defined in the resource descriptor.\nThe scripts may be implemented using any scripting language of choice, as long as there is a Resource Driver capable of executing them. Scripts are located in a subdirectory of the \u0026lsquo;Lifecycle\u0026rsquo; directory of the resource package, named after the type of the Resource driver that will use them. The contents of this directory is defined by the Resource driver. For example, in this example we are using the Ansible Lifecycle driver, each lifecycle script maps to an Ansible playbook that is called the same as the lifecycle step, and is located in the \u0026lsquo;Lifecycle/ansible/scripts/\u0026rsquo; directory. The Configure script would be located at \u0026lsquo;Lifecycle/ansible/scripts/Configure.yml\u0026rsquo; or \u0026lsquo;Lifecycle/ansible/scripts/Configure.yaml\u0026rsquo; (note: case is important). More information about Ansible playbooks can be found here.\nA resource package does not need to implement all lifecycle scripts, only those defined in the resource descriptor.\nIn addition to the standard lifecycle scripts, specific operations may also be added. Operations are bespoke transitions that are invoked by the Resource manager when Agile Lifecycle Manager (ALM) is establishing relationships between two resources. These allow for action to take place between the resources to enable a Network Service to work. Operations and their definitions are part of the public interface of a resource.\nCreate an operation by adding the operation script in the \u0026lsquo;Lifecycle/ansible/scripts\u0026rsquo; folder where the name of the operation is the name of the script. For example, the \u0026ldquo;deleteNetworkInterface\u0026rdquo; script should be located in the file \u0026lsquo;Lifecycle/ansible/scripts/deleteNetworkInterface.yml\u0026rsquo; or \u0026lsquo;Lifecycle/ansible/scripts/deleteNetworkInterface.yaml\u0026rsquo; (note, case is important).\nAdd Lifecycle Scripts If you\u0026rsquo;ve completed the earlier sections to this guide then you should be in possession of a project for a Resource, which when instantiated, creates some basic infrastructure to standup a compute instance.\nWe now need to add some software to our a Resource to give it a real function. Typically, a Resource engineer would achieve this by adding scripts to be executed as part of the Install, Configure and Start transitions.\nWe will be using the Ansible lifecycle driver, so our scripts will take the form of playbooks.\nConfigure Inventory To use the Ansible lifecycle driver we will need to setup an inventory file which tells Ansible how to SSH to the compute instance created as part of the Resource infrastructure.\nUpdate the inventory file under the Lifecycle/ansible/config with the following contents:\n[apache1]\rapache1-inst\r[apache2]\rapache2-inst\rDuplicate the example-host.yml file under the Lifecycle/ansible/config/host_vars directory to apache1-inst.yml and apache2-inst.yml (it\u0026rsquo;s important to Ansible that these files match the names of our hosts in the inventory files).\ncp Lifecycle/ansible/config/host_vars/example-host.yml Lifecycle/ansible/config/host_vars/apache1-inst.yml\rmv Lifecycle/ansible/config/host_vars/example-host.yml Lifecycle/ansible/config/host_vars/apache2-inst.yml\rUpdate the contents of apache1-inst.yml with:\n---\ransible_host: {{ properties.server1_public_ip }}\ransible_user: ubuntu\ransible_become_pass: ubuntu\ransible_sudo_pass: ubuntu\ransible_ssh_common_args: \u0026quot;-i {{ properties.server1_ssh_key_path }} -o 'UserKnownHostsFile=/dev/null' -o StrictHostKeyChecking=no\u0026quot;\rUpdate the contents of apache2-inst.yml with:\n---\ransible_host: {{ properties.server2_public_ip }}\ransible_user: ubuntu\ransible_become_pass: ubuntu\ransible_sudo_pass: ubuntu\ransible_ssh_common_args: \u0026quot;-i {{ properties.server2_ssh_key_path }} -o 'UserKnownHostsFile=/dev/null' -o StrictHostKeyChecking=no\u0026quot;\rThe value of ansible_host is set using a templated variable. This allows us to configure the inventory at runtime with a property from our instance. In this case, as we don\u0026rsquo;t know the IP address of the compute instance until it has been created, we will substitute the address at runtime using the server§_public_ip and server2_public_ip properties (remember these properties are in our Resource descriptor and are set by outputs with the same names in our infrastructure template).\nThe Ansible Lifecycle Driver supports infrastructure keys. The private key portion of any infrastructure key input property is written to disk for the duration of the transition request, and the path made available to the playbook as an Ansible property. In this case, the \u0026ldquo;server1_ssh_key\u0026rdquo; and \u0026ldquo;server2_ssh_key\u0026rdquo; input properties (of type key - see resource descriptor) are exposed as intrinsic properties \u0026ldquo;server1_ssh_key_path\u0026rdquo; and \u0026ldquo;server2_ssh_key_path\u0026rdquo; (the property name with a _path suffix) so that they can be used in inventory. In addition, the key name (that is, the value of the key property) is exposed as an intrinsic property called [property name]_name.\nAdd Install The install lifecycle transition is a suitable time to:\n check the infrastructure is ready for use (check we can SSH to it) install any software required by the Resource  We will add a script to install Apache2 as a service:\nCreate a new file called Install.yaml in Lifecycle/ansible/scripts\ntouch Lifecycle/ansible/scripts/Install.yaml\rAdd the following content to the playbook. The tasks listed will install Apache2 but not start it\n- name: Install\rhosts: apache1,apache2\rbecome: yes\rgather_facts: False\rtasks:\r- name: Pause for Python\rraw: while [ -z \u0026quot;$(command -v python)\u0026quot; ]; do sleep 1; done\r- name: Prevent apache2 auto-starting on install)\rshell: ln -s /dev/null /etc/systemd/system/apache2.service\r- name: Install apache2\rretries: 60\rdelay: 1\rapt:\rupdate_cache: yes\rname: apache2\rregister: apache_install\runtil: '\u0026quot;Could not get lock\u0026quot; not in apache_install[\u0026quot;stderr\u0026quot;]'\r- name: Verify service apache2\rsystemd:\rname: apache2\rstate: started\renabled: yes\rmasked: no\rNow on an install transition, Apache2 will be installed on the compute instance of the Resource.\nAdd Configure The configure lifecycle transition is a suitable time to setup configuration files or any other auxiliary items required by the intended software so it is ready to use.\nIn this guide we will add a script to configure Apache2\u0026rsquo;s default site with our own content.\nIn Lifecycle/ansible/scripts create a new directory called site, then create an index.html file in that directory\nmkdir Lifecycle/ansible/scripts/site \u0026amp;\u0026amp; touch Lifecycle/ansible/scripts/site/index.html\rUpdate index.html with the following content\n\u0026lt;html\u0026gt;\r\u0026lt;head\u0026gt;\r\u0026lt;title\u0026gt;Hello, {{ properties.greeting_receiver }}\u0026lt;/title\u0026gt;\r\u0026lt;/head\u0026gt;\r\u0026lt;body\u0026gt;\r\u0026lt;h1\u0026gt;Hello, {{ properties.greeting_receiver }}\u0026lt;/h1\u0026gt;\r\u0026lt;p\u0026gt;Success!\u0026lt;/p\u0026gt;\r\u0026lt;/body\u0026gt;\r\u0026lt;/html\u0026gt;\rIn Lifecycle/ansible/scripts create a new directory called conf, then create a hw-site.conf file in that directory\nmkdir Lifecycle/ansible/scripts/conf \u0026amp;\u0026amp; touch Lifecycle/ansible/scripts/conf/hw-site.conf\rUpdate hw-site.conf with the following content\n\u0026lt;VirtualHost *:{{properties.site_port}}\u0026gt;\rServerAdmin {{properties.site_admin_email}}\rDocumentRoot /var/www/{{properties.site_name}}\rErrorLog ${APACHE_LOG_DIR}/error.log\rCustomLog ${APACHE_LOG_DIR}/access.log combined\r\u0026lt;/VirtualHost\u0026gt;\rCreate a new file called Configure.yaml in Lifecycle/ansible/scripts\ntouch Lifecycle/ansible/scripts/Configure.yaml\rAdd the following content to the playbook. The tasks listed will add a new site to Apache2 and remove the default\n---\r- name: Configure\rhosts: apache1,apache2\rbecome: yes\rgather_facts: False\rtasks:\r- name: Disable default site\rcommand: a2dissite 000-default.conf\rfailed_when: false\r- name: Remove default site config\rfile:\rstate: absent\rpath: /etc/apache2/sites-available/000-default.conf\r- name: Create new site directory\rfile:\rstate: directory\rpath: /var/www/{{ properties.site_name }}\r- name: Copy site html\rtemplate:\rsrc: site/index.html\rdest: /var/www/{{ properties.site_name }}/index.html\rmode: '0755'\r- name: Add site config\rtemplate:\rsrc: conf/hw-site.conf\rdest: /etc/apache2/sites-available/hw-site.conf\rmode: '0755'\r- name: Enable new site\rcommand: a2ensite hw-site.conf\rYou may have noticed a few additional properties referenced in these files, such as site_name, they must be added to our descriptor. Open Definitions/lm/resource.yaml and add the new properties.\nproperties:\r...other properties...\rgreeting_receiver:\rdefault: world\rdescription: Name to append to the greeting on the default site (i.e. Hello, World!)\rsite_port:\rdefault: 80\rdescription: Port to bind the apache2 site\rsite_admin_email: default: host@localhost\rdescription: ServerAdmin setting for the apache2 site\rsite_name:\rdefault: hw\rdescription: Name given to the apache2 site\rAdd Start The start lifecycle transition is a suitable time to activate/enable software so it is ready for use. In this guide we will add a script to start our configured Apache2 site.\nCreate a new file called Start.yaml in Lifecycle/ansible/scripts\ntouch Lifecycle/ansible/scripts/Start.yaml\rAdd the following content to the playbook\n---\r- name: Start\rhosts: apache1,apache2\rbecome: yes\rgather_facts: False\rtasks:\r- name: Restart apache2\rsystemd:\rstate: restarted\rname: apache2\rAdd Stop As we have a start transition which enables the Apache2 service it is a good idea to also add a stop, which will disable the Apache2 service.\nCreate a new file called Stop.yaml in Lifecycle/ansible/scripts\ntouch Lifecycle/ansible/scripts/Stop.yaml\rAdd the following content to the playbook\n---\r- name: Stop\rhosts: apache1,apache2\rbecome: yes\rgather_facts: False\rtasks:\r- name: Stop apache2\rsystemd:\rstate: stopped\rname: apache2\rEnabling Lifecycle Transitions infrastructure:\rOpenstack: {}\rlifecycle:\rCreate:\rdrivers:\ropenstack:\rselector:\rinfrastructure-type:\r- Openstack\rInstall:\rdrivers:\ransible:\rselector:\rinfrastructure-type:\r- Openstack\rConfigure:\rdrivers:\ransible:\rselector:\rinfrastructure-type:\r- Openstack\rStart:\rdrivers:\ransible:\rselector:\rinfrastructure-type:\r- Openstack\rStop:\rdrivers:\ransible:\rselector:\rinfrastructure-type:\r- Openstack\rDelete:\rdrivers:\ropenstack:\rselector:\rinfrastructure-type:\r- Openstack\rNotice how ansible is used under drivers section to each lifecycle Install, Configure, Start, and Stop, while Create and Delete use openstack. This section defines which Resource Driver should be used to handle the transition for the default infrastructure type. Since ansible is being used for the majority of lifecycle steps here, a more succint way of specifying this is as follows:\ninfrastructure:\rOpenstack: {}\rdefault-driver:\ransible:\rselector:\rinfrastructure-type:\r- Openstack\rlifecycle:\rCreate:\rdrivers:\ropenstack:\rselector:\rinfrastructure-type:\r- Openstack\rInstall: {}\rConfigure: {}\rStart: {}\rStop: {}\rDelete: {}\rdrivers:\ropenstack:\rselector:\rinfrastructure-type:\r- Openstack\rThe default-driver section specifies to use ansible, unless overridden by a drivers definition in a specific lifecycle configuration.\nRe-instantiate The Resource is ready for usage. Push the Resource to your target environment and create an instance of the Assembly (created earlier in Instantiate Resource).\nYou\u0026rsquo;ll see the Install, Configure and Start transitions execute for the Resource, upon completion you should be able to access the hosted site with at: http://\u0026lt;public_ip\u0026gt;:\u0026lt;site_port\u0026gt;.\nTo see the Stop transition execute, make the Assembly inactive by opening the \u0026ldquo;New Intent\u0026rdquo; dropdown in the top right and selecting \u0026ldquo;Make Inactive\u0026rdquo;.\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/best-practices/","title":"Best Practices","tags":[],"description":"","content":""},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/reference/security/configuring-external-ldap/","title":"Configuring External LDAP Connections","tags":[],"description":"","content":"Agile Lifecycle Manager (ALM) makes use of LDAP as its user store for secure access. It is configured by default to connect to the OpenLDAP service which is part of the installation. It is possible to disable this installation of this OpenLDAP service, and to connect to an external LDAP implementation using different mechanisms including support for Active Directory.\nConfiguring an External LDAP Server To use an LDAP Server other than the pre-configured OpenLDAP instance which comes as part of the LM installation, follow these steps.\nModify LDAP connection configuration ALM comes with configuration based on the directory structure which exists in the pre-configured install of OpenLDAP.\nWhen using an existing LDAP server which uses a different directory structure, the LDAP directory queries can be configured as below.\nALM supports 2 distinct LDAP authentication strategies which are as follows:\n ldapBind - (default) this mechanism binds to LDAP using manager credentials to initially locate the user\u0026rsquo;s DN record using search filters, then attempting to bind this user with their password once found. This method should be selected in order to authenticate against Active Directory. ldapSimple - this mechanism binds to LDAP either using manager credentials, or anonymously if this is allowed. Once connected, the user\u0026rsquo;s DN record is located using search filters and a password compare is performed against the password attribute within this record.  Once the most appropriate strategy is selected, follow the relevant configuration section below.\nLDAP Bind (default) The ldapBind authentication strategy is the default during an installation of LM. It can be customised with the following configuration, which can be set in Vault:\nalm: ishtar: security: authenticationProvider: ldapBind ldap: url: ldap://ldap-server:389 base: null managerDn: cn=admin,,dc=lm,dc=com managerPassword: adminpassword userSearchBase: ou=people userSearchFilter: uid={0} groupSearchBase: ou=groups groupSearchFilter: member={0} The purpose of each configuration item is as follows:\n url - the URL of an LDAP server listening with the LDAP protocol base - the base suffix from which all operations should origin. If set, then this base should not be included in userSearchBase or groupSearchBase. managerDn - the Username (DN) of the \u0026ldquo;manager\u0026rdquo; user identity (i.e. \u0026ldquo;uid=admin,ou=system\u0026rdquo;) which will be used to authenticate to the LDAP server. If omitted, anonymous access will be used managerPassword - the password for the manager DN userSearchBase - the base used during directory searches for a user, i.e. the context name to search in userSearchFilter - the filter expression used in the user search. This is an LDAP search filter (as defined in \u0026lsquo;RFC 2254\u0026rsquo;) with optional arguments groupSearchBase - the search string used during directory searches for a group. If this is an empty string the search will be performed from the root DN of the context factory. If null, no search will be performed. groupSearchFilter - the pattern to be used for the group members search. {0} is the user\u0026rsquo;s DN  LDAP Simple The ldapSimple authentication strategy relies on LM using a manager password to bind to LDAP, upon which it queries the user’s DN record and compares the password. The following configuration is required, which can be set in Vault:\nalm: ishtar: security: ldap: url: ldap://ldap-server:389 base: dc=lm,dc=com managerDn: cn=admin,dc=lm,dc=com managerPassword: adminpassword userSearchBase: ou=people userSearchFilter: (\u0026amp;(uid={0})(!(isSuspended=true))) groupSearchBase: ou=groups groupSearchFilter: member={0} passwordAttribute: userPassword passwordEncoding: BCRYPT The purpose of each configuration item is as follows:\n url - the URL of an LDAP server listening with the LDAP protocol base - the base suffix from which all operations should origin. If set, then this base should not be included in userSearchBase or groupSearchBase. managerDn - the Username (DN) of the \u0026ldquo;manager\u0026rdquo; user identity (i.e. \u0026ldquo;uid=admin,ou=system\u0026rdquo;) which will be used to authenticate to the LDAP server. If omitted, anonymous access will be used managerPassword - the password for the manager DN userSearchBase - the base used during directory searches for a user, i.e. the context name to search in userSearchFilter - the filter expression used in the user search. This is an LDAP search filter (as defined in \u0026lsquo;RFC 2254\u0026rsquo;) with optional arguments. The user query can be used to filter for suspended users based upon the value of a certain field if this exists or has been defined as a custom type in the LDAP schema (e.g. isSuspended) groupSearchBase - the base DN from which the search for group membership should be performed groupSearchFilter - the pattern to be used for the group members search. {0} is the user\u0026rsquo;s DN passwordAttribute - the attribute in the directory which contains the user password passwordEncoding - the encoding on the stored password (only BCRYPT and PLAIN are supported)  If using passwordEncoding of BCRYPT, the user\u0026rsquo;s password must be provided as a BCrypt encoded value. The BCrypt hashing library used in ALM currently only supports hashed passwords with the $2a prefix. It should be ensured that any LDAP passwords generated make use of a BCrypt hashing algorithm which only generates password hashes with the $2a prefix, otherwise login attempts will fail. An example tool that may be used to convert plain-text passwords to BCrypt is the bcrypt-cli.\nRevoke user access An existing user may be suspended in order to disable their access\nSuspension using groups Suspension can be performed by making a user a members of a group called Suspended:\ndn: cn=Suspended,ou=groups,dc=lm,dc=com\robjectclass: groupOfNames\rcn: Suspended\rmember: uid=TestUserA,ou=people,dc=lm,dc=com\rSuspension using Password Policy The default installation of LM, which comes with an instance of OpenLDAP, will have a password policy installed and enforced. This meachanism can be used to lockout users. The easiest way to lockout a user is to set a value within the pwdAccountLockedTime attribute under the user\u0026rsquo;s DN record. An example of this is shown below:\nLock User Execute the following command on the OpenLDAP pod to lockout the user \u0026lsquo;jack\u0026rsquo;:\nldapmodify -D cn=admin,dc=lm,dc=com -W\rdn: uid=Jack,ou=people,dc=lm,dc=com\rchangetype: modify\rdelete: pwdAccountLockedTime\rUnlock User Execute the following command on the OpenLDAP pod to unlock the user \u0026lsquo;jack\u0026rsquo;:\nldapmodify -D cn=admin,dc=lm,dc=com -W\rdn: uid=Jack,ou=people,dc=lm,dc=com\rchangetype: modify\radd: pwdAccountLockedTime\rpwdAccountLockedTime: 22000101000000Z\rActive Directory To connect to active directory as an LDAP source, the ldapBind mechanism should be used. The following is an example of a configuration that might typically work with Active Directory.\nalm: ishtar: security: authenticationProvider: ldapBind ldap: url: ldap://active-directory-server:389 base: null managerDn: cn=Administrator,cn=Users,dc=lm,dc=local managerPassword: adminpassword userSearchBase: cn=Users,dc=lm,dc=local userSearchFilter: cn={0} groupSearchBase: dc=lm,dc=local groupSearchFilter: member={0} "},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/cicd/pipeline/create-ci-pipeline/","title":"Create CI Pipeline","tags":[],"description":"","content":"Objectives Create a continuous integration (CI) pipeline that will automatically build and test VNF or Network Service packages when changes are detected in Git projects. This CI pipeline will be triggered when there is a check-in to the master branch of the VNF or Network Service git project.\nPre-requisites  All VNF and Network Services packages the VNF or Network Service project under development depends on for both testing and when it is released.  Introduction The CI pipeline will perform the following tasks:\n Checkout the Git project to run for the VNF or Network Service project. Upload all software images from Nexus to the target test environment. Push all dependent VNF or Network Service packages to the target environment. Push the project under development into the target environment Run all included behaviour tests If all tests pass:  A new package is created for the project in nexus. The version of the package is the same as the version in the descriptor of the VNF or Network Service under test. The project in git will be updated with the latest the version number    The package is then available for use in other projects.\nCreate the Pipeline script The following is an example Jenkins pipeline script that executes the steps in the previous section. The target Agile Lifecycle Manager (ALM) environment in the case below is called testing.\nCreate a pipeline file name \u0026lsquo;Jenkinsfile\u0026rsquo; at the top-level of your VNF or Network Service project.\ndef descriptor = 'UNKNOWN'\rdef version = 'UNKNOWN'\rpipeline{\ragent { label 'lmctl' }\renvironment {\rORGANISATION = 'marketplace'\rTEST_ENV = 'testing'\rGOGS_HOST = \u0026quot;${CICDHUB_GOGS_SERVICE_HOST}:${CICDHUB_GOGS_SERVICE_PORT_GOGS_HTTP}\u0026quot;\rNEXUS_URL = \u0026quot;http://${NEXUS_SVC_NODEPORT_SERVICE_HOST}:${NEXUS_SVC_NODEPORT_SERVICE_PORT_NEXUS_HTTP}\u0026quot;\r} stages{\rstage('Onboard VNF and NS Project Dependencies'){\rsteps{\rscript {\ronboard_dependencies(\u0026quot;test.deps\u0026quot;)\ronboard_dependencies(\u0026quot;release.deps\u0026quot;)\r} }\r}\rstage(\u0026quot;Onboard Project\u0026quot;){\rsteps{\rscript{ /* get the descriptor name from the top-level assembly, that will be the package name */\rdescriptor = get_descriptor()\rsh(\u0026quot;echo ${descriptor}\u0026quot;)\r/* get version from top-level assembly. this is the version number to use for packaging */ version = get_version()\rsh(\u0026quot;echo ${version}\u0026quot;)\r/* Push the project into the test environment */\rsh(\u0026quot;lmctl project push ${TEST_ENV}\u0026quot; )\r} }\r}\rstage('Behaviour Tests'){\rsteps{\rsh \u0026quot;lmctl project test ${TEST_ENV}\u0026quot;\r}\r}\rstage('Version Project'){\rsteps{\rversion_project(\u0026quot;${descriptor}\u0026quot;, \u0026quot;${version}\u0026quot;)\r} }\rstage('Package'){\rsteps{\r/* we have a new snapshot for this version, delete the old one and upload the new\r*/\rwithCredentials([usernamePassword(credentialsId: 'nexus-id', passwordVariable: 'NEXUS_PASSWORD', usernameVariable: 'NEXUS_USERNAME')]) {\rsh \u0026quot;curl -X DELETE -v -u ${NEXUS_USERNAME}:${NEXUS_PASSWORD} ${NEXUS_URL}/repository/raw/packages/${descriptor}/${descriptor}-${version}-SNAPSHOT.tgz\u0026quot;\rsh \u0026quot;curl -v -u ${NEXUS_USERNAME}:${NEXUS_PASSWORD} --upload-file ./_lmctl/_build/${descriptor}-${version}.tgz ${NEXUS_URL}/repository/raw/packages/${descriptor}/${descriptor}-${version}-SNAPSHOT.tgz\u0026quot; }\r}\r}\r}\r}\rdef get_descriptor()\r{\rreturn sh(returnStdout: true, script: \u0026quot;grep -E ^name:[[:space:]].*::.*::.* ${WORKSPACE}/Descriptor/assembly.yml |cut -d ':' -f4\u0026quot;).trim()\r}\rdef get_version()\r{\rreturn sh(returnStdout: true, script: \u0026quot;grep -E ^name:[[:space:]].*::.*::.* ${WORKSPACE}/Descriptor/assembly.yml |cut -d ':' -f6\u0026quot;).trim()\r}\rdef version_project(descriptor, version)\r{\r/*\rDelete the tag if it exists already, we want to move it.\rThen tag this commit\r*/\rwithCredentials([usernamePassword(credentialsId: 'gogs-id', passwordVariable: 'GIT_PASSWORD', usernameVariable: 'GIT_USERNAME')]) {\rsh(\u0026quot;git config --global user.email 'jenkins@cicdhub.com'\u0026quot;)\rsh(\u0026quot;git config --global user.name 'Jenkins'\u0026quot;)\rsh(\u0026quot;git tag -d ${version} |true\u0026quot;)\rsh(\u0026quot;git push http://${GIT_USERNAME}:${GIT_PASSWORD}@${GOGS_HOST}/${ORGANISATION}/${descriptor} --delete ${version} |true\u0026quot;)\rsh(\u0026quot;git push ${GIT_URL} --delete ${version} |true\u0026quot;)\rsh(\u0026quot;git tag -a ${version} -m 'Jenkins' ${GIT_COMMIT} |true\u0026quot;)\rsh(\u0026quot;git push http://${GIT_USERNAME}:${GIT_PASSWORD}@${GOGS_HOST}/${ORGANISATION}/${descriptor} --tags\u0026quot;)\r} }\rdef onboard_dependencies(dependency_filename)\r{\rdef n /* name of package */\rdef v /* version of package */\rif (fileExists(\u0026quot;${WORKSPACE}/${dependency_filename}\u0026quot;)) {\rdef packages = readFile(\u0026quot;${WORKSPACE}/${dependency_filename}\u0026quot;).split('\\n')\rfor (int i = 0; i \u0026lt; packages.size(); ++i) {\rn = packages[i].split('-')[0].trim()\rv = packages[i].split('-')[1].trim()\r/*\rtry to get a released package (without -SNAPSHOT suffix)\rif not try get a SNAPSHOT\r*/\rif (package_exists(\u0026quot;raw\u0026quot;, \u0026quot;packages/${n}/${n}-${v}.tgz\u0026quot;) == \u0026quot;0\u0026quot;)\r{\rsh(\u0026quot;wget ${NEXUS_URL}/repository/raw/packages/${n}/${n}-${v}.tgz\u0026quot;)\rsh(\u0026quot;lmctl pkg push ${n}-${v}.tgz ${TEST_ENV}\u0026quot;)\recho \u0026quot;onboarded ${n}-${v}.tgz \u0026quot; }\relse\r{\rsh(\u0026quot;wget ${NEXUS_URL}/repository/raw/packages/${n}/${n}-${v}-SNAPSHOT.tgz\u0026quot;)\rsh(\u0026quot;lmctl pkg push ${n}-${v}-SNAPSHOT.tgz ${TEST_ENV}\u0026quot;)\recho \u0026quot;onboarded ${n}-${v}-SNAPSHOT.tgz\u0026quot; }\r}\r}\r}\rdef package_exists(repo, package_name)\r{\rresponse = sh( returnStdout: true, script: \u0026quot;curl -X GET \\\u0026quot;${NEXUS_URL}/service/rest/v1/search?repository=${repo}\u0026amp;name=${package_name}\\\u0026quot;\u0026quot;\r)\recho \u0026quot;response: ${response}\u0026quot;\rstatus = sh( returnStatus: true,\rscript: \u0026quot;echo \\\u0026quot;${response}\\\u0026quot; | grep ${package_name}\u0026quot;)\recho \u0026quot;status: ${status}\u0026quot;\rreturn \u0026quot;${status}\u0026quot;\r}\rAdd the file to your local git repository and push to Gogs, by running the following commands.\n$ git add .\r$ git commit -m \u0026quot;added Jenkins file\u0026quot;\r$ git push\rConfigure the pipeline in Jenkins The newly created pipeline needs to be added to Jenkins and configured to auto-trigger when changes are detected in Gogs. First we add the pipeline from our VNF or Network Service project to Jenkins, by running the following steps.\n In the Jenkins home page select \u0026lsquo;New Item\u0026rsquo; Enter the name of the VNF (or Network Service), Select \u0026lsquo;Pipeline\u0026rsquo; and press \u0026lsquo;OK\u0026rsquo; Check \u0026lsquo;discard old builds\u0026rsquo; and \u0026lsquo;Do not allow concurrent builds\u0026rsquo; Gogs Webhook: Use Gogs secret set to \u0026lsquo;gogs\u0026rsquo; Pipeline Definition, select \u0026lsquo;Pipeline script from SCM\u0026rsquo; SCM: Git Add the repository URL of your project from Gogs. Credentials drop list select \u0026lsquo;jenkins\u0026rsquo; Script Path, set the name of your Jenkinsfile (if different) Save  Configure Gogs Webhook Gogs Webhooks must be configured to trigger Jenkins to run the pipeline file configured earlier. To do this run the following tasks.\n Login in to Gogs Browse to your project and select \u0026lsquo;settings\u0026rsquo; then \u0026lsquo;Webhooks\u0026rsquo; Select \u0026lsquo;Add Webhook\u0026rsquo; - \u0026lsquo;Gogs\u0026rsquo; Enter \u0026lsquo;Payload URL\u0026rsquo;. this will be of the form http://\u0026lt;jenkins_host\u0026gt;:\u0026lt;jenkins_port\u0026gt;/gogs-webhook/?job=\u0026lt;your jenkins job name\u0026gt; Secret enter \u0026lsquo;gogs\u0026rsquo; Leave all other defaults and press \u0026lsquo;Add Webhook\u0026rsquo; Select the \u0026lsquo;pencil\u0026rsquo; edit but on your new Webhook. At the bottom of the section, Press \u0026lsquo;Test Delivery\u0026rsquo; If this succeeds in Gogs, then go to your project in Jenkins. You should see a job has been started.  Configure your project dependencies The pipeline jobs in Jenkins needs some additional information to know what packages must be loaded to run any tests (e.g. test VNFs or Network Services) if using the example pipeline scripts.\nFor packages only needed to support testing create a file named test.deps containing a list of the package names and versions separated by a space on each line of the file.\nBelow is an example test.deps file containing some dependent VNF packages required for running the complete VNF or Network Service and its behaviour tests:\ntraffic_generator 1.0\rtraffic_probe 1.1\rThe pipeline job will push traffic_generator-1.0-SNAPSHOT.tgz and traffic_probe-1.1.tgz into the \u0026lsquo;testing\u0026rsquo; environment.\nFor packages that the project depends on when released (e.g. VNFs or other Network Services contained in a Network Service design) create another file named release.deps with the list of dependent \u0026lt;name\u0026gt; \u0026lt;version\u0026gt; packages. This may be a subset of the packages in test.deps files since it will not include any packages needed only for testing.\nBoth files must be updated as your dependencies (including versions) change.\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/reference/lm-api/api-definition/resource-manager/deployment-location-api/","title":"Deployment Location","tags":[],"description":"","content":"The following details the API used to manage Deployment Locations within LM.\nAssociated with each definition are details of the request parameters and responses. These include the name of each field plus a brief description and whether the field is mandatory. Whether a field is required or not is based on the context of the examples. The underlying API definition may mark a field as optional, but in some contexts, the fields must be supplied.\nFor a 40x, 50x or any other error response please see error response codes\nCreate a new Deployment Location Creates a new Deployment Location to be created\nRequest    Aspect Value     Endpoint URL /api/deploymentLocations   Content-Type application/json   HTTP Method POST    Body    Field Description Mandatory     name The name of the Deployment Location Yes   description Description of the Deployment Location No   resourceManager ID of the Resource Manager to manage the Deployment Location Yes   infrastructureType Type of infrastructure managed at this Deployment Location Yes   infrastructureSpecificProperties Set of name-value pairs to be passed to the Resource Manager managing this location. The list of properties is dependent upon the location but is usually where connection details for the location are set, so the Resource Manager can reach the location No    Example:\n{\r​ \u0026quot;name\u0026quot;:\u0026quot;London-4\u0026quot;,\r​ \u0026quot;description\u0026quot;:\u0026quot;London 4 Data Center - OpenStack\u0026quot;,\r​ \u0026quot;resourceManager\u0026quot;:\u0026quot;8e266bc5-e613-4b0d-9ae0-50db6454b026\u0026quot;,\r​ \u0026quot;infrastructureType\u0026quot;:\u0026quot;docker\u0026quot;,\r​ \u0026quot;infrastructureSpecificProperties\u0026quot;: [\r​ {\r​ \u0026quot;propertyName-1\u0026quot; : \u0026quot;propertyValue-1\u0026quot;,\r​ \u0026quot;propertyName-2\u0026quot; : \u0026quot;propertyValue-2\u0026quot;\r​ }\r​ ]\r}\rResponse    Aspect Value     Response Code 201 (Created)    Headers    Field Description     location Endpoint to Deployment Location resource    Body    Field Description     id Unique ID of the Deployment Location   name The name of the Deployment Location   description Description of the Deployment Location   resourceManager ID of the Resource Manager that manages the Deployment Location   infrastructureType Type of infrastructure managed at this Deployment Location    Update Deployment Location Update either the description or the infrastructure properties of a Deployment Location.\nRequest    Aspect Value     Endpoint URL /api/deploymentLocation/{deployLocId}   Content-Type application/json   HTTP Method PUT    Body    Field Description Mandatory     description Description of the Deployment Location No   infrastructureSpecificProperties Set of name-value pairs to be passed to the Resource Manager managing this location. The list of properties is dependent upon the location but is usually where connection details for the location are set, so the Resource Manager can reach the location No    Example:\n{\r​ \u0026quot;description\u0026quot;:\u0026quot;London 4 Data Center - OpenStack\u0026quot;,\r​ \u0026quot;infrastructureSpecificProperties\u0026quot;: [\r​ {\r​ \u0026quot;propertyName-1\u0026quot; : \u0026quot;propertyValue-1\u0026quot;,\r​ \u0026quot;propertyName-2\u0026quot; : \u0026quot;propertyValue-2\u0026quot;\r​ }\r​ ]\r}\rResponse    Aspect Value     Response Code 200 (OK)    Remove Deployment Location Remove a Deployment Location from LM.\nRequest Format    Aspect Value     Endpoint URL /api/deploymentLocation/{deployLocId}   HTTP Method DELETE    Path Parameters    Field Description Mandatory     deployLocId The unique ID of the Deployment Location being removed Yes    Response    Aspect Value     Response Code 204 (No Content)    Get all Deployment Locations Get a list of all existing Deployment Locations. An optional match against a partial name string or Resource Manager name can be provided.\nRequest    Aspect Value     Endpoint URL /api/deploymentLocations?{query-params}   HTTP Method GET   Parameters Name    Query Parameters    Field Description Mandatory     name name to use as a partial name match in order to file the results No   resourceManagerName name of a Resource Manager to reduce the results to the locations associated with it No    Response    Aspect Value     Response Code 200 (OK)   Content-Type application/json    Body The body includes a single list of locations, each with the following fields:\n   Field Description     id Unique ID of the Deployment Location   name The name of the Deployment Location   description Description of the Deployment Location   resourceManager ID of the Resource Manager that manages the Deployment Location   infrastructureType Type of infrastructure managed at this Deployment Location    Example:\n[\r​ {\r​ \u0026quot;id\u0026quot;:\u0026quot;London-4\u0026quot;,\r​ \u0026quot;name\u0026quot;:\u0026quot;London-4\u0026quot;,\r​ \u0026quot;description\u0026quot;:\u0026quot;London 4 Data Center - OpenStack\u0026quot;,\r​ \u0026quot;resourceManager\u0026quot;:\u0026quot;8e266bc5-e613-4b0d-9ae0-50db6454b026\u0026quot;,\r​ \u0026quot;infrastructureType\u0026quot;:\u0026quot;docker\u0026quot;\r​ }\r]\rGet Deployment Location Request the details for a specific Deployment Location identified by it’s unique ID.\nRequest    Aspect Value     Endpoint URL /api/deploymentLocation/{deployLocId}   HTTP Method GET    Path Parameters    Field Description Mandatory     deployLocId The unique ID of the Deployment Location being requested Yes    Response    Aspect Value     Response Code 200 (OK)   Content-Type application/json    Body    Field Description     id Unique ID of the Deployment Location   name The name of the Deployment Location   description Description of the Deployment Location   resourceManager ID of the Resource Manager that manages the Deployment Location   infrastructureType Type of infrastructure managed at this Deployment Location    Example:\n{\r​ \u0026quot;id\u0026quot;:\u0026quot;8e266bc5-e613-4b0d-9ae0-50db6454b026\u0026quot;,\r​ \u0026quot;name\u0026quot;:\u0026quot;London-4\u0026quot;,\r​ \u0026quot;description\u0026quot;:\u0026quot;London 4 Data Center - Docker\u0026quot;,\r​ \u0026quot;resourceManager\u0026quot;:\u0026quot;8e266bc5-e613-4b0d-9ae0-50db6454b026\u0026quot;,\r​ \u0026quot;infrastructureType\u0026quot;:\u0026quot;docker\u0026quot;\r}\r"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/installation/lm/production/configuration/external-ldap/","title":"External LDAP","tags":[],"description":"","content":"Agile Lifecycle Manager (ALM) makes use of LDAP as its user store for secure access. It is configured by default to connect to the OpenLDAP service which is part of installation. Is is possible to disable this installation of this OpenLDAP service, and to connect to an external LDAP implementation. This includes support for Active Directory.\nConfiguring an External LDAP Server To use an LDAP Server other than the pre-configured OpenLDAP instance which comes as part of the LM installation, follow these steps.\nDisable Installation and Set Up of OpenLDAP To disable the installation and set up of OpenLDAP when using an existing LDAP, add the following to the custom Helm values file:\nopenldap: enabled: false configurator: security: ldap: enabled: false Modify LDAP connection configuration LM comes with configuration based on the directory structure which exists in the pre-configured install of OpenLDAP.\nWhen using an existing LDAP server which uses a different directory structure, the LDAP directory queries can be configured as per the example below.\nFor a full range of configuration options for external LDAP sources, including how to use LDAP Bind mechanisms and Active Directory, see Configuring External LDAP Connections.\nIn the following example, LM is configured to point to an external LDAP source by changing the configuration within Ishtar. This configuration is imported into Vault to be made available to Ishtar by adding the following to the custom Helm values file during installation:\nconfigurator: lmConfigImport: ishtar: alm: ishtar: security: ldap: url: ldap://openldap:389 base: dc=lm,dc=com managerDn: cn=admin,dc=lm,dc=com managerPassword: lmadmin userSearchBase: ou=people userSearchFilter: (\u0026amp;(uid={0})(!(isSuspended=true))) groupSearchBase: ou=groups groupSearchFilter: member={0} passwordAttribute: userPassword passwordEncoding: BCRYPT The purpose of each configuration item is as follows:\n url - the url of an LDAP server listening with the LDAP protocol base - the base suffix from which all operations should origin. If set, then this base should not be included in userSearchBase or groupSearchBase. managerDn - the Username (DN) of the \u0026ldquo;manager\u0026rdquo; user identity (i.e. \u0026ldquo;uid=admin,ou=system\u0026rdquo;) which will be used to authenticate to the LDAP server. If omitted, anonymous access will be used managerPassword - the password for the manager DN userSearchBase - the base used during directory searches for a user, i.e. the context name to search in userSearchFilter - the filter expression used in the user search. This is an LDAP search filter (as defined in \u0026lsquo;RFC 2254\u0026rsquo;) with optional arguments groupSearchBase - the base DN from which the search for group membership should be performed groupSearchFilter - the pattern to be used for the group members search. {0} is the user\u0026rsquo;s DN passwordAttribute - the attribute in the directory which contains the user password passwordEncoding - the encoding on the stored password (only BCRYPT and PLAIN are supported)  Next Steps Continue to Scaling ALM.\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/administration/configuration/languages/","title":"Languages","tags":[],"description":"","content":""},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/reference/lm-api/api-definition/topology/","title":"Topology","tags":[],"description":"","content":""},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/reference/lm-api/api-definition/asynchronous-state-change-events/","title":"Asynchronous State Change Events","tags":[],"description":"","content":"LM emits events when the state of an assembly and its components changes. Messages that are sent asynchronously are put onto a Kafka bus. The exact topics can be configured. These are emitted in response to Intent Requests causing the state of the Assembly Instance, or its associated components, to change. In the event of a failure to change state, an event will also be emitted.\nAssociated with each example is a table explaining the fields in the example. This includes the name of the field, a brief description and whether the field is mandatory. Whether a field is required or not is based on the context of the examples. The underlying API definition may mark a field as optional, but in some contexts, the fields must be supplied.\nIn the examples below field names shown in italics are example names and not the real one to be used in messages.\nProcessStateChangeEvent These events are associated with the process that performs the Intent request. These are sent out at the Start of the processing and when completed or failed.\nProcessStateChangeEvent Examples Example of an initial state change event:\n{\r\u0026quot;processId\u0026quot;: \u0026quot;e29b86a8-ca75-413b-921b-c4b895996c12\u0026quot;,\r\u0026quot;assemblyId\u0026quot;: \u0026quot;e4c198d1-2dbb-4557-baae-b5891fa258cf\u0026quot;,\r\u0026quot;assemblyName\u0026quot;: \u0026quot;example2\u0026quot;,\r\u0026quot;assemblyDescriptorName\u0026quot;: \u0026quot;assembly::t_single::1.0\u0026quot;,\r\u0026quot;intentType\u0026quot;: \u0026quot;CreateAssembly\u0026quot;,\r\u0026quot;intent\u0026quot;: {\r\u0026quot;assemblyName\u0026quot;: \u0026quot;example2\u0026quot;,\r\u0026quot;descriptorName\u0026quot;: \u0026quot;assembly::t_single::1.0\u0026quot;,\r\u0026quot;intendedState\u0026quot;: \u0026quot;Active\u0026quot;,\r\u0026quot;properties\u0026quot;: {\r\u0026quot;data\u0026quot;: \u0026quot;example data\u0026quot;,\r\u0026quot;deplomentLocation\u0026quot;: \u0026quot;admin@local\u0026quot;\r}\r},\r\u0026quot;processState\u0026quot;: \u0026quot;In Progress\u0026quot;,\r\u0026quot;processStartedAt\u0026quot;: \u0026quot;2017-09-14T13:06:17.499Z\u0026quot;,\r\u0026quot;eventId\u0026quot;: \u0026quot;e08039e7-7efd-4b7e-86a3-9d39c48c2dd7\u0026quot;,\r\u0026quot;eventCreatedAt\u0026quot;: \u0026quot;2017-09-14T13:06:17.499Z\u0026quot;,\r\u0026quot;eventType\u0026quot;: \u0026quot;ProcessStateChangeEvent\u0026quot;\r}\rThe example above shows the first message sent when an intent has been received by LM. The processId will be used in all subsequent state change events. These happen when the process changes state.\nExample of the final message on successful completion of the Intent:\n{\r\u0026quot;processId\u0026quot;: \u0026quot;e29b86a8-ca75-413b-921b-c4b895996c12\u0026quot;,\r\u0026quot;assemblyId\u0026quot;: \u0026quot;e4c198d1-2dbb-4557-baae-b5891fa258cf\u0026quot;,\r\u0026quot;assemblyName\u0026quot;: \u0026quot;example2\u0026quot;,\r\u0026quot;assemblyDescriptorName\u0026quot;: \u0026quot;assembly::t_single::1.0\u0026quot;,\r\u0026quot;intentType\u0026quot;: \u0026quot;CreateAssembly\u0026quot;,\r\u0026quot;intent\u0026quot;: {\r​ \u0026quot;assemblyName\u0026quot;: \u0026quot;example2\u0026quot;,\r​ \u0026quot;descriptorName\u0026quot;: \u0026quot;assembly::t_single::1.0\u0026quot;,\r​ \u0026quot;intendedState\u0026quot;: \u0026quot;Active\u0026quot;,\r​ \u0026quot;properties\u0026quot;: {\r​ \u0026quot;data\u0026quot;: \u0026quot;example data\u0026quot;,\r​ \u0026quot;deplomentLocation\u0026quot;: \u0026quot;admin@local\u0026quot;\r​ }\r},\r\u0026quot;processState\u0026quot;: \u0026quot;Completed\u0026quot;,\r\u0026quot;processStartedAt\u0026quot;: \u0026quot;2017-09-14T13:06:17.499Z\u0026quot;,\r\u0026quot;processFinishedAt\u0026quot;: \u0026quot;2017-09-14T13:06:19.648Z\u0026quot;,\r\u0026quot;eventId\u0026quot;: \u0026quot;405cf14c-752e-4030-8a4e-6706e04b50f5\u0026quot;,\r\u0026quot;eventCreatedAt\u0026quot;: \u0026quot;2017-09-14T13:06:19.649Z\u0026quot;,\r\u0026quot;eventType\u0026quot;: \u0026quot;ProcessStateChangeEvent\u0026quot;\r}\rExample of the final message indicating Intent Failed:\n{\r\u0026quot;processId\u0026quot;: \u0026quot;8c922ec6-7589-4ba8-8b4e-d7841b9a9654\u0026quot;,\r\u0026quot;assemblyId\u0026quot;: \u0026quot;42ecbe49-0069-41f5-ac38-595b090c3d65\u0026quot;,\r\u0026quot;assemblyName\u0026quot;: \u0026quot;example3\u0026quot;,\r\u0026quot;assemblyDescriptorName\u0026quot;: \u0026quot;assembly::t_single::1.0\u0026quot;,\r\u0026quot;intentType\u0026quot;: \u0026quot;CreateAssembly\u0026quot;,\r\u0026quot;intent\u0026quot;: {\r​ \u0026quot;assemblyName\u0026quot;: \u0026quot;example3\u0026quot;,\r​ \u0026quot;descriptorName\u0026quot;: \u0026quot;assembly::t_single::1.0\u0026quot;,\r​ \u0026quot;intendedState\u0026quot;: \u0026quot;Active\u0026quot;,\r​ \u0026quot;properties\u0026quot;: {\r​ \u0026quot;data\u0026quot;: \u0026quot;Example Data\u0026quot;,\r​ \u0026quot;deplomentLocation\u0026quot;: \u0026quot;admin@local\u0026quot;\r​ }\r},\r\u0026quot;processState\u0026quot;: \u0026quot;Failed\u0026quot;,\r\u0026quot;processStateReason\u0026quot;: \u0026quot;Exception …\u0026quot;,\r\u0026quot;processStartedAt\u0026quot;: \u0026quot;2017-09-14T13:13:58.966Z\u0026quot;,\r\u0026quot;processFinishedAt\u0026quot;: \u0026quot;2017-09-14T13:13:59.616Z\u0026quot;,\r\u0026quot;eventId\u0026quot;: \u0026quot;11acf385-e6f9-41cf-9651-38dc3ed6a53a\u0026quot;,\r\u0026quot;eventCreatedAt\u0026quot;: \u0026quot;2017-09-14T13:13:59.616Z\u0026quot;,\r\u0026quot;eventType\u0026quot;: \u0026quot;ProcessStateChangeEvent\u0026quot;\r}\rField Details    Field Description Mandatory     processId The id given to the process that was initiated by an Intent request Yes   assemblyId The LM internal id for the assembly instance associated with the process Yes   assemblyName The name of the assembly instance as supplied in the Intent request Yes   intentType The name of the intent type. The values correspond to the Intents described in the Managing Assembly section Yes   intent Contains details of the intent request supplied. Yes   processState The processState may contain – \u0026ldquo;In Progress\u0026rdquo;, \u0026ldquo;Completed\u0026rdquo; or \u0026ldquo;Failed\u0026rdquo;. Yes   processStartedAt The data and time the process was started Yes   eventId The id of this event from the LM view point (each message will have a unique Id Yes   eventCreatedAt The date and time when the event happened from the LM viewpoint Yes   eventType Will always contain \u0026ldquo;ProcessStateChangeEvent\u0026rdquo; Yes    ComponentStateChangeEvent These events are sent when the root assembly changes state and when each of its associated resources successfully transitions to a new state. In the event of a failure of the process no events will be sent.\nComponentStateChangeEvent Examples First message sent indicating first component transitioning to the Installed State:\n{\r\u0026quot;eventId\u0026quot;: \u0026quot;901d4794-7734-4511-8e24-6035ee5cb22a\u0026quot;,\r\u0026quot;eventCreatedAt\u0026quot;: \u0026quot;2017-09-14T13:06:18.37Z\u0026quot;,\r\u0026quot;rootAssemblyId\u0026quot;: \u0026quot;e4c198d1-2dbb-4557-baae-b5891fa258cf\u0026quot;,\r\u0026quot;rootAssemblyName\u0026quot;: \u0026quot;example2\u0026quot;,\r\u0026quot;resourceId\u0026quot;: \u0026quot;7a03bc63-bccf-4731-b6b2-9389609d9fa5\u0026quot;,\r\u0026quot;resourceName\u0026quot;: \u0026quot;example2__A\u0026quot;,\r\u0026quot;resourceManager\u0026quot;: \u0026quot;test-rm\u0026quot;,\r\u0026quot;deploymentLocation\u0026quot;: \u0026quot;admin@local\u0026quot;,\r\u0026quot;externalId\u0026quot;: \u0026quot;06d20929-a1c0-44cf-8009-aee47bac3f99\u0026quot;,\r\u0026quot;processId\u0026quot;: \u0026quot;f4538d94-4c42-4b3b-993a-530f7862120f\u0026quot;,\r\u0026quot;previousState\u0026quot;: null,\r\u0026quot;newState\u0026quot;: \u0026quot;Installed\u0026quot;,\r\u0026quot;eventType\u0026quot;: \u0026quot;ComponentStateChangeEvent\u0026quot;\r}\rThe above message is an example of a resource transitioning to the Installed state. The previous state is null indicating the resource did not exists before.\nAn event indicating the root assembly has transitions to Installed State:\n{\r\u0026quot;eventId\u0026quot;: \u0026quot;33a63fca-2bed-49c3-8615-56e5b35aa3bb\u0026quot;,\r\u0026quot;eventCreatedAt\u0026quot;: \u0026quot;2017-09-14T13:06:18.572Z\u0026quot;,\r\u0026quot;rootAssemblyId\u0026quot;: \u0026quot;e4c198d1-2dbb-4557-baae-b5891fa258cf\u0026quot;,\r\u0026quot;rootAssemblyName\u0026quot;: \u0026quot;example2\u0026quot;,\r\u0026quot;processId\u0026quot;: \u0026quot;f4538d94-4c42-4b3b-993a-530f7862120f\u0026quot;,\r\u0026quot;previousState\u0026quot;: null,\r\u0026quot;newState\u0026quot;: \u0026quot;Installed\u0026quot;,\r\u0026quot;eventType\u0026quot;: \u0026quot;ComponentStateChangeEvent\u0026quot;\r} The example above will be sent when all resources associated with the root assembly have successfully transitioned to the Installed State.\nField Details    Field Description Mandatory     eventId The internal Id generated by LM in response to an Orchestration Event request Yes   eventCreatedAt The date and time when the event happened from the LM viewpoint Yes   rootAssemblyId The internal LM id for the root assembly instance Yes   rootAssemblyName The name of the root assembly as supplied in the Intent request Yes   resourceId The id of the resource as defined by LM No (used for resources only)   resourceName The name of the resource as defined by LM No (used for resources only)   resourceManager The name of the resource manager that manages the resource No (used for resources only)   deploymentLocation The location that the resource manager was requested to install the resource No (used for resources only)   externalId The id of the resource as defined by the resource manager No (used for resources only)   processId The internal Id of the process created by LM to perform the state change. No   previousState ‡ The state that the assembly or component was in before the state change happened. Allowed values: Installed, Inactive, Active. When a Heal event has been requested, LM puts the component into the Broken state. This is a temporary state that is used to trigger the Heal processing. This will be set to \u0026ldquo;null\u0026rdquo; when the resource or assembly is transitioning t the Installed State Yes   newState ‡ The state to which the assembly or component instance transitioned in the event of a successful state change or the state that would have resulted if a failure had not occurred. This will be \u0026ldquo;null\u0026rdquo; when the resource or assembly is being Uninstalled Yes   eventType Expected value ‘ComponentStateChangeEvent’ Yes    ‡ When LM is requested to heal a component, LM will indicate this with a set of state transitions from Active to Broken and then Broken to Inactive.\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/cicd/pipeline/create-release-pipeline/","title":"Create Release Pipeline","tags":[],"description":"","content":"Objectives Create a release pipeline that will package a VNF or Network Service project into a binary package.\nPre-requisites  CI Pipeline has already been configured and tested.  Introduction This pipeline will be triggered when there is a check-in to the master branch. It will:\n Checkout the project (tagged version) Check that all packages this project depends on in release have also been released (i.e. not SNAPSHOT versions) Check this version of the project has not already been released Rename the package in nexus (remove the SNAPSHOT) If the project is a NS, then deploy it\u0026rsquo;s package, and all the packages it depends on for release to the production LM.  Create the Release Pipeline script The following is an example pipeline scripts\n\rdef descriptor = 'UNKNOWN'\rdef version = 'UNKNOWN'\rpipeline\r{\ragent { label 'lmctl' }\renvironment {\rSERVICE_TYPE = 'NS'\rNEXUS_URL = \u0026quot;http://${NEXUS_SVC_NODEPORT_SERVICE_HOST}:${NEXUS_SVC_NODEPORT_SERVICE_PORT_NEXUS_HTTP}\u0026quot;\r} stages\r{\rstage('Check VNF and NS Project Dependencies')\r{\rsteps\r{\rcheck_dependencies(\u0026quot;release.dep\u0026quot;)\r} }\rstage('Package Release')\r{\rsteps\r{\rscript\r{ /* get the descriptor name from the top-level assembly, that will be the package name */\rdescriptor = get_descriptor()\rsh(\u0026quot;echo releasing named: ${descriptor}\u0026quot;)\r/* get version from top-level assembly. this is the version number to use for packaging */ version = get_version()\rsh(\u0026quot;echo release version: ${version}\u0026quot;)\rif (package_exists(\u0026quot;raw\u0026quot;, \u0026quot;packages/${descriptor}/${descriptor}-${version}.tgz\u0026quot;) == 0)\r{\rsh(\u0026quot;echo ${descriptor}-${version}.tgz has alread been released cannot continue with this version\u0026quot;)\rcurrentBuild.result = 'ABORTED'\rerror('Cannot release with a version that has previously been released')\r} move_package_to_release(\u0026quot;${descriptor}\u0026quot;, \u0026quot;${version}\u0026quot;)\r}\r}\r}\rstage('Deploy Images')\r{\rsteps\r{\rscript{\rif (\u0026quot;${SERVICE_TYPE}\u0026quot; =='NS'){\rsh(\u0026quot;echo get image list and targets\u0026quot;)\rsh(\u0026quot;echo check image in target vim and load if new\u0026quot;)\r} else {\rsh(\u0026quot;echo Skipping deployment images stage for non NS\u0026quot;)\r}\r}\r}\r}\rstage('Deploy Packages')\r{\rsteps\r{\rscript\r{\rif (\u0026quot;${SERVICE_TYPE}\u0026quot; =='NS')\r{\rdeploy_packages(\u0026quot;${descriptor}\u0026quot;, \u0026quot;${version}\u0026quot;)\r} else\r{\rsh(\u0026quot;echo Skipping deployment of packages stage for non NS\u0026quot;) }\r}\r}\r}\r}\r}\rdef get_descriptor()\r{\rreturn sh(returnStdout: true, script: \u0026quot;grep -E ^name:[[:space:]].*::.*::.* ${WORKSPACE}/Descriptor/assembly.yml |cut -d ':' -f4\u0026quot;).trim()\r}\rdef get_version()\r{\rreturn sh(returnStdout: true, script: \u0026quot;grep -E ^name:[[:space:]].*::.*::.* ${WORKSPACE}/Descriptor/assembly.yml |cut -d ':' -f6\u0026quot;).trim()\r}\rdef move_package_to_release(descriptor, version)\r{\r/* we remove the snapshot for this version and upload release (without -SNAPSHOT)\r*/\rwithCredentials([usernamePassword(credentialsId: 'nexus-id', passwordVariable: 'NEXUS_PASSWORD', usernameVariable: 'NEXUS_USERNAME')]) {\rsh \u0026quot;wget ${NEXUS_URL}/repository/raw/packages/${descriptor}/${descriptor}-${version}-SNAPSHOT.tgz\u0026quot; sh \u0026quot;curl -v -u ${NEXUS_USERNAME}:${NEXUS_PASSWORD} --upload-file ./${descriptor}-${version}-SNAPSHOT.tgz ${NEXUS_URL}/repository/raw/packages/${descriptor}/${descriptor}-${version}.tgz\u0026quot; sh \u0026quot;curl -X DELETE -v -u ${NEXUS_USERNAME}:${NEXUS_PASSWORD} ${NEXUS_URL}/repository/raw/packages/${descriptor}/${descriptor}-${version}-SNAPSHOT.tgz\u0026quot;\r} }\rdef deploy_packages(descriptor, version)\r{\rscript {\rdef n /* name of package */\rdef v /* version of package */\rif (fileExists(\u0026quot;${WORKSPACE}/packages.dep\u0026quot;)) {\rdef packages = readFile(\u0026quot;${WORKSPACE}/packages.dep\u0026quot;).split('\\n')\rfor (int i = 0; i \u0026lt; packages.size(); ++i) {\rn = packages[i].split('-')[0]\rv = packages[i].split('-')[1]\rsh(\u0026quot;wget ${NEXUS_URL}/repository/raw/packages/${n}/${n}-${v}.tgz\u0026quot;)\rsh(\u0026quot;lmctl pkg push ${n}-${v}.tgz production\u0026quot;)\rsh(\u0026quot;echo onboarded ${n}-${v}.tgz\u0026quot;) }\r}\rsh(\u0026quot;echo deploy new package into production\u0026quot;)\rsh(\u0026quot;wget ${NEXUS_URL}/repository/raw/packages/${descriptor}/${descriptor}-${version}.tgz\u0026quot;)\rsh(\u0026quot;lmctl pkg push ${descriptor}-${version}.tgz production\u0026quot;)\rsh(\u0026quot;echo onboarded ${descriptor}-${version}.tgz\u0026quot;) } }\rdef check_dependencies(dependency_filename){\rscript {\rdef n /* name of package */\rdef v /* version of package */\rif (fileExists(\u0026quot;${WORKSPACE}/${dependency_filename}\u0026quot;)) {\rdef packages = readFile(\u0026quot;${WORKSPACE}/${dependency_filename}\u0026quot;).split('\\n')\rfor (int i = 0; i \u0026lt; packages.size(); ++i) {\rn = packages[i].split('-')[0]\rv = packages[i].split('-')[1]\r/*\rtry to get a released package (without -SNAPSHOT suffix)\rif not we fail. Only released packages can be used.\r*/ if (package_exists(\u0026quot;raw\u0026quot;, \u0026quot;packages/${n}/${n}-${v}.tgz\u0026quot;) == 0)\r{ sh(\u0026quot;echo checked ${n}-${v}.tgz has been released\u0026quot;) }\relse\r{\rsh(\u0026quot;echo ${n}-${v}.tgz has NOT been released cannot continue\u0026quot;)\rcurrentBuild.result = 'ABORTED'\rerror('Cannot release with dependent packages that are not also released')\r} }\r}\r} }\rdef package_exists(repo, package_name)\r{\rresponse = sh( returnStdout: true, script: \u0026quot;curl -X GET \\\u0026quot;${NEXUS_URL}/service/rest/v1/search?repository=${repo}\u0026amp;name=${package_name}\\\u0026quot;\u0026quot;\r)\rreturn sh( returnStatus: true,\rscript: \u0026quot;echo \\\u0026quot;${response}\\\u0026quot; | grep ${package_name}\u0026quot;\r)\r}\rConfigure Jenkins and Gogs Follow the steps to create the pipeline job in Jenkins from the steps in CI pipeline.\nDo not create an auto hook. This job is intended to be triggered manually when all stakeholders agree the project has been sufficiently tested and is ready for release.\nThe pipelines can be developed to auto-trigger automatic release and deployment. But that is not covered here.\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/reference/lm-api/api-definition/resource-manager/infrastructure-keys/","title":"Infrastructure Keys","tags":[],"description":"","content":"The following section details the APIs used to manage Infrastructure Keys.\nAssociated with each definition are details of the request parameters and responses. These include the name of each field plus a brief description and whether the field is mandatory. Whether a field is required or not is based on the context of the examples. The underlying API definition may mark a field as optional, but in some contexts, the fields must be supplied.\nFor a 40x, 50x or any other error response please see error response codes.\nNote: any infrastructure keys (shared or resource) returned in an API response will not include the private key.\nCreate Shared Infrastructure Key Creates a new shared infrastructure key.\nRequest    Aspect Value     Endpoint URL /api/resource-manager/infrastructure-keys/shared   HTTP Method POST    Request Headers:    Header Description Mandatory     Content-Type Both JSON and YAML requests are supported. application/json is the default. To send YAML, set this header to application/yaml. Given that private SSH keys contain newline characters, we recommend you use application/yaml for this. Yes    Body    Field Description Mandatory     name The name of the infrastructure key (must be unique) Yes   description Infrastructure key description No   publicKey Public key portion of the infrastructure key No   privateKey Private key portion of the infrastructure key No    Example (YAML) name: key\rdescription: key\rpublicKey: ssh-rsa XXAAAAB3NzaC1yc2EAAAADAQABAAABAQDJ1t35uQorDiR8gmXQtbGB6W+NrK4nx+IYj8bTBsQu0gx4jlYNwb2wzaeejLU7QP8FB5G3eclo7emUNK479Eg/UMtPmNzsQhRjlUKonuYLPncLGKtetFgjMnmePvi8mudl9o1alvcdtn41VofNZ4dN8RcgKIJOdS1Xd4vwHEW57PhUNIHs0x2MHfPQ0wsz1vzUrof4B2bxFkXI4f3kTXKV2n3MMfeb736TYfrhBcji4i9OOiQV3jwlrmqeOQyddgrCUB6TEDPJfO6XGqxWSl56CxvbqSSZ3ugplptAoObUHuxJs5P6pMGvC03D5nDq3pSKajAgTix6xjM6Z4LKFHPx accanto@blacktower\rprivateKey: |\r-----BEGIN RSA PRIVATE KEY-----\rMIIEpAIBAAKCAQEAvdugKkZpDMCrDks78JMQd9KrL7rPkQeUheAob6oUVeLwhyXm\r2q+EQo7oCSc4k2nKXdBTIl2d/ob/jjHQmpeYzjJD3Xk5GHuzRdJy1T+g+j5ZaJMg\r5w8paUqoaFEoJsuZS5zu4dpLEo5htJdbLhtv29LCl0u68V6EnL+WyT4kzxTz3UjX\rBMZTH243dByFjdANEXBQdf0aDcsuregbowV8eUVzC9yd0j3UvZzIFZnkKOE3yDBG\rPX4qtrmYLhEYkn+QY85bRlT2dAP5Gcfff0LMx1IpePAcQSQINMNamX36B4Atsa2u\r+bVsJCmoIZZWcrhxSOeURdZz6xYalPr3sGnUPQIDAQABAoIBAGr6lgU8J3VIGxqy\rydOFCoJ58nuyh8Lwwn+tDxvcehjrBx0f/jS7MYtPeu+tafOmaKD0AeQbXCtPZjB5\rYVG6mh+VsyfYZpOlIB73tjzy3YIkH70NKj0IDg2GQ96D3Fv/3SD+DJy2pBaQo/1e\rS8JvM1hqKbuWsH/RAmhQjGZY3eTQrtKn2jvM4C/pThKHdeRytmUFuxG8i7MN4YmD\rGfyNGvuNk4YGAXUPwD+5acZfms/nukY5QGQeLI3ma2rl6/psVeQk1Dt20zg/zp5I\rV5FidIqgpJ1zJnevcM+CYO6DlhO6cWpFwTb1BCyHlKy51t+/5UaxVWWYC8uXgfKs\rOV8W4AECgYEA/L4noGF0Y2XDPpa5ym8CPRp7+lM6yo++pKo0tSSe3vSjzlC/1svU\rmbPNp0eX7GAkK6Om2WQtEdlXAk2Cd38hxemH5VMBAB0sV6cUhdMXspvW5E20dJBW\rlWGfjZooH1xzu02z9/RokXvSd5fihpGElo7kUOi7Au/XuA9xq4PJr50CgYEAwE4A\rg5cR2nOAZLTcrtY0HXFvdJYsJQNq5iLymYTI8qS9C/ibLDeKIvj+UEeCZ0K7M8E9\rgtQ5d4FpuwaXyatgyJa0/QwY2g+4+DMYlA8mLkQuL+K/18ERDMl3w1dflUsZuK4h\rWBjjvLLoQ0DW1lt/sBaVexuScoJR0Anf884CpSECgYEAzYkWUKbwL6UiElQg4y9D\rcs2G5RuqQ/CB6QdBtc4CJO5uB+pDGDrKufQu2kqukdvdjl+X4FJHFKzaxVSjNOp+\rXKR9j1DhD2aqGN4XIn14WBfDugY3KlP3FaQ+TLmAx4A33apjINHx993qWrd50FWm\rrYmmwuWAr/K8S0LD/ec+rUECgYEAlkYeFrad5/RsN8Yx0Zpn/T4gl52mf4INLivP\rInJmSV68sDcko0f/dVbX/Mn0uFaoTUhLlgnbomyJv/Hl+V0n83f5lOi4Huc8ZxAe\ru8yezHbRFPB/c81WUoS5RvfnJJYcUnoYamyAwnuYDr4DZFz6ChOL2jjwlvUJk6Wz\r0SSCYqECgYB9DUiP8sLrf5YmzwxzHLXYKJRJKEmhRYgE2ZYBQ+CbhkBcGoJwLk1F\r4WMdzm8hyddPQvE4UU2fVVrbppYtnkkTAaIXdBzQA/hjHnQlpooHr/cMmftE4dPO\rUQ122Zq7AHx3P58o1Q0CVhakGkoa+mc0+VK4O2uYl3sMrKCpVjs2ZQ==\r-----END RSA PRIVATE KEY-----\rNote the syntax for the private key; it must use the pipe | symbol and be indented to conform to the PEM format.\nResponse    Aspect Value     Content-Type application/json   Success Status Code 201    Response Headers    Field Description     location Endpoint to infrastructure key resource    Response Body    Field Description     name The name of the infrastructure key   description Infrastructure key description   privateKey Private key portion of the infrastructure key   publicKey Public key portion of the infrastructure key    Update Shared Infrastructure Key Update an existing shared infrastructure key.\nRequest    Aspect Value     Endpoint URL /api/resource-manager/infrastructure-keys/shared/{name}   HTTP Method PUT    Request Headers:    Header Description Mandatory     Content-Type Both JSON and YAML requests are supported. application/json is the default. To send YAML, set this header to application/yaml. Given that private SSH keys contain newline characters, we recommend you use application/yaml for this. Yes    Path Parameters:    Field Description Mandatory     name Name of the shared infrastructure key Yes    Body    Field Description Mandatory     description Infrastructure key description No   publicKey Public key portion of the infrastructure key No   privateKey Private key portion of the infrastructure key No    Example (YAML) description: key\rpublicKey: ssh-rsa XXAAAAB3NzaC1yc2EAAAADAQABAAABAQDJ1t35uQorDiR8gmXQtbGB6W+NrK4nx+IYj8bTBsQu0gx4jlYNwb2wzaeejLU7QP8FB5G3eclo7emUNK479Eg/UMtPmNzsQhRjlUKonuYLPncLGKtetFgjMnmePvi8mudl9o1alvcdtn41VofNZ4dN8RcgKIJOdS1Xd4vwHEW57PhUNIHs0x2MHfPQ0wsz1vzUrof4B2bxFkXI4f3kTXKV2n3MMfeb736TYfrhBcji4i9OOiQV3jwlrmqeOQyddgrCUB6TEDPJfO6XGqxWSl56CxvbqSSZ3ugplptAoObUHuxJs5P6pMGvC03D5nDq3pSKajAgTix6xjM6Z4LKFHPx accanto@blacktower\rprivateKey: |\r-----BEGIN RSA PRIVATE KEY-----\rMIIEpAIBAAKCAQEAvdugKkZpDMCrDks78JMQd9KrL7rPkQeUheAob6oUVeLwhyXm\r2q+EQo7oCSc4k2nKXdBTIl2d/ob/jjHQmpeYzjJD3Xk5GHuzRdJy1T+g+j5ZaJMg\r5w8paUqoaFEoJsuZS5zu4dpLEo5htJdbLhtv29LCl0u68V6EnL+WyT4kzxTz3UjX\rBMZTH243dByFjdANEXBQdf0aDcsuregbowV8eUVzC9yd0j3UvZzIFZnkKOE3yDBG\rPX4qtrmYLhEYkn+QY85bRlT2dAP5Gcfff0LMx1IpePAcQSQINMNamX36B4Atsa2u\r+bVsJCmoIZZWcrhxSOeURdZz6xYalPr3sGnUPQIDAQABAoIBAGr6lgU8J3VIGxqy\rydOFCoJ58nuyh8Lwwn+tDxvcehjrBx0f/jS7MYtPeu+tafOmaKD0AeQbXCtPZjB5\rYVG6mh+VsyfYZpOlIB73tjzy3YIkH70NKj0IDg2GQ96D3Fv/3SD+DJy2pBaQo/1e\rS8JvM1hqKbuWsH/RAmhQjGZY3eTQrtKn2jvM4C/pThKHdeRytmUFuxG8i7MN4YmD\rGfyNGvuNk4YGAXUPwD+5acZfms/nukY5QGQeLI3ma2rl6/psVeQk1Dt20zg/zp5I\rV5FidIqgpJ1zJnevcM+CYO6DlhO6cWpFwTb1BCyHlKy51t+/5UaxVWWYC8uXgfKs\rOV8W4AECgYEA/L4noGF0Y2XDPpa5ym8CPRp7+lM6yo++pKo0tSSe3vSjzlC/1svU\rmbPNp0eX7GAkK6Om2WQtEdlXAk2Cd38hxemH5VMBAB0sV6cUhdMXspvW5E20dJBW\rlWGfjZooH1xzu02z9/RokXvSd5fihpGElo7kUOi7Au/XuA9xq4PJr50CgYEAwE4A\rg5cR2nOAZLTcrtY0HXFvdJYsJQNq5iLymYTI8qS9C/ibLDeKIvj+UEeCZ0K7M8E9\rgtQ5d4FpuwaXyatgyJa0/QwY2g+4+DMYlA8mLkQuL+K/18ERDMl3w1dflUsZuK4h\rWBjjvLLoQ0DW1lt/sBaVexuScoJR0Anf884CpSECgYEAzYkWUKbwL6UiElQg4y9D\rcs2G5RuqQ/CB6QdBtc4CJO5uB+pDGDrKufQu2kqukdvdjl+X4FJHFKzaxVSjNOp+\rXKR9j1DhD2aqGN4XIn14WBfDugY3KlP3FaQ+TLmAx4A33apjINHx993qWrd50FWm\rrYmmwuWAr/K8S0LD/ec+rUECgYEAlkYeFrad5/RsN8Yx0Zpn/T4gl52mf4INLivP\rInJmSV68sDcko0f/dVbX/Mn0uFaoTUhLlgnbomyJv/Hl+V0n83f5lOi4Huc8ZxAe\ru8yezHbRFPB/c81WUoS5RvfnJJYcUnoYamyAwnuYDr4DZFz6ChOL2jjwlvUJk6Wz\r0SSCYqECgYB9DUiP8sLrf5YmzwxzHLXYKJRJKEmhRYgE2ZYBQ+CbhkBcGoJwLk1F\r4WMdzm8hyddPQvE4UU2fVVrbppYtnkkTAaIXdBzQA/hjHnQlpooHr/cMmftE4dPO\rUQ122Zq7AHx3P58o1Q0CVhakGkoa+mc0+VK4O2uYl3sMrKCpVjs2ZQ==\r-----END RSA PRIVATE KEY-----\rNote the syntax for the private key; it must use the pipe | symbol and be indented to conform to the PEM format.\nResponse    Aspect Value     Content-Type application/json   Success Status Code 204    List Shared Infrastructure Keys View details of all Shared Infrastructure Keys, sorted by name (ascending).\nRequest    Aspect Value     Endpoint URL /api/resource-manager/infrastructure-keys/shared   HTTP Method GET    Request Headers:    Header Description Mandatory     Accept Both JSON and YAML requests are supported. JSON is the default. To return YAML, set this header to application/yaml No    Request Parameters:    Field Description Mandatory     includePrivateKey Whether to include private keys (true) or not (false) No (default is \u0026ldquo;false\u0026rdquo;)    Response    Aspect Value     Content-Type application/json   Success Status Code 200    Get Shared Infrastructure Key by Name View details of a shared Infrastructure Key\nRequest    Aspect Value     Endpoint URL /api/resource-manager/infrastructure-keys/{name}   HTTP Method GET    Request Headers:    Header Description Mandatory     Accept Both JSON and YAML requests are supported. JSON is the default. To return YAML, set this header to application/yaml No    Path Parameters:    Field Description Mandatory     name Name of the shared infrastructure key Yes    Request Headers:    Header Description Mandatory     Accept Both JSON and YAML requests are supported. JSON is the default. To return YAML, set this header to application/yaml No    Response    Aspect Value     Content-Type application/json   Success Status Code 200    Remove Shared Infrastructure Key Remove a shared Infrastructure Key\nRequest    Aspect Value     Endpoint URL /api/resource-manager/infrastructure-keys/shared/{name}   HTTP Method DELETE    Path Parameters:    Field Description Mandatory     name Name of the shared infrastructure key Yes    Response    Aspect Value     Content-Type application/json   Success Status Code 204    "},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/installation/lm/production/configuration/scaling-policy/","title":"Scaling ALM","tags":[],"description":"","content":"The following guide explains how to prepare Agile Lifecycle Manager (ALM) to scale with CPU usage.\nPre-requisites:  The Kubernetes metrics server must be running in your Kubernetes environment for scaling of ALM to work.  Configure ALM ALM services can be setup to scale from the point they are installed by setting some custom helm values. Each service has configurable options for resources and autoscaling thresholds.\nBefore running a helm install of ALM you can create a custom values file with content like the following for each service:\n\u0026lt;service name\u0026gt;:\rapp:\rresources: limits:\rcpu: \u0026lt;maximum CPU to allocate to pod\u0026gt;\rmemory: \u0026lt;maximum memory to allocate to pod\u0026gt;\rrequests:\rcpu: \u0026lt;initial CPU to allocate to pod\u0026gt;\rmemory: \u0026lt;initial memory to allocate to pod\u0026gt;\rautoscaler:\renabled: \u0026lt;boolean to enable/disable autoscaler\u0026gt;\rmaxReplicas: \u0026lt;maximum pods service can scale up to\u0026gt;\rminReplicas: \u0026lt;minumum pods service can scale down to\u0026gt;\rtargetCPUUtilizationPercentage: \u0026lt;percent of CPU to trigger scaling\u0026gt;\rFor example if you wanted Apollo to automatically scale you could add this to a custom values file:\napollo:\rapp:\rresources: limits:\rcpu: 500m\rmemory: 4Gi\rrequests:\rcpu: 200m\rmemory: 2Gi\rautoscaler:\renabled: true\rmaxReplicas: 5\rminReplicas: 1\rtargetCPUUtilizationPercentage: 80\rWhen resources are set and the autoscaler is enabled for a service then a Horizontal Pod Autoscaler will be created for that service.\nYou can inspect your Horizontal Pod Autoscalers by running:\nkubectl get hpa\rContinue to Installing Error Links.\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/cicd/developing-a-project/developing-new-vnf/","title":"Developing a Package","tags":[],"description":"","content":"Objectives Develop VNF or Network Service project artifacts that, when complete, will constitute a releasable package.\nPre-requisites  CI/CD Hub and a development environment with a dedicated Agile Lifecycle Manager (ALM) and VIM A VNF or Network Service project already exists.  Developing a VNF or Network Service Descriptor Push your project into dev environment (assume this is called \u0026lsquo;dev\u0026rsquo; in the LMCONFIG file for LMCTL):\n$ cd \u0026lt;myvnf dir\u0026gt;\r$ lmctl project push dev\rDesign the VNF or Network Service Descriptor\n Log into the ALM UI Go to the \u0026lsquo;Assembly Designer\u0026rsquo; and open the descriptor with your project name. Select Add Element and add resource elements that should be contained in your design. For a Network Service this can be VNFs or other Network Services. For VNFs this will be VNFCs/resource descriptors. Edit the descriptors:  Properties Property mappings Relationships Promoted operations Policies   Save your changes  Developing VNFC/Resource Lifecycles These should be detailed in the VNF or NS design\nFor a VNF add VNFC artifacts\n You need to add playbooks for each lifecycle operation of each VNFC that is in your design. Edit the playbooks in the package structure. Remove any that are not used. Add playbooks for any relationship operations on your VNFC in your design Edit each VNFC descriptor with:  Properties Lifecycle Operations Relationship Operations Metrics Policies These should be detailed in the VNFC design    Commit changes to the develop branch as you go.\nCreating behaviour Tests In the ALM designer, you can add Behaviour Tests to your project by performing the following tasks:\n Design a behaviour test for the VNF or Network Services that installs it to it\u0026rsquo;s active state. Now run the test above. If the behaviour test is passing run the following commands on your local machine to export the behaviour test to your local machine, and add it to your git project:   $ lmctl project pull dev\r$ git add .\r$ git commit -m 'Lifecycle Behaviour Test passed'\r```\r  If the test is failing, investigate the failure and fix. Retest until all behaviour tests are passing  Push your project changes to the CI/CD Hub. You have now completed a basic VNF or Network Service set of steps. You know that it can be deployed and have tested it\u0026rsquo;s lifecycle.\n $ git push origin master develop\rNow you have a VNF that can be installed and managed through it\u0026rsquo;s lifecycle. You can add further tests to validate that it is all functioning correctly.\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/installation/lm/production/configuration/error-links-install/","title":"Install Error Links","tags":[],"description":"","content":"Direct links to a logging dashboard can be enabled in the Agile Lifecycle Manager (ALM) User Interface. This is an optional feature that will generate a link to a relevant search in a logging dashboard based on any error that has occurred in an assembly instance. The link displays like this:\nEnabling the dashboard links during ALM installation  Create a values YAML file the following content in order to enable the logging dashboard:   configurator:\rloggingDashboard:\renabled: true\r## The domain used to connect to the logging dashboard UI\rendpoint: http://kibana.lm:31001\r## Chosen logging dashboard name (currently only 'kibana' is supported)\rapplication: kibana\rkibana:\r## Name of index to be created in kibana\rindex: lm-logs\r## Kibana service API endpoint that will be used to create the index\rconfigurationEndpoint: http://foundation-kibana:443\rUse the values file when installing the lm-configurator helm chart.  Continue to Install ALM.\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/cicd/developing-a-project/release-package/","title":"Releasing a package","tags":[],"description":"","content":"Objectives Release a VNF or Network Service package that has been confirmed ready for production use to a production environment\nPre-requisites  The VNF or Network Service package is available in the Nexus general repository and marked ready for production deployment.  Introduction When a package has been developed and its been agreed that it has been fully tested by its stakeholders, it can be released.\nReleasing a VNF or Network Service package is done by triggering the Release pipeline job manually in Jenkins and giving the version of the package to deploy.\nIf successful, the package will be deployed into the Production environment along with all packages it depends on for release. Also the package will be available in nexus for other projects to use in production.\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/reference/lm-api/api-definition/resource-manager/resource-drivers/","title":"Resource Drivers","tags":[],"description":"","content":"The following section details the APIs used to manage Brent Resource drivers.\nAssociated with each definition are details of the request parameters and responses. These include the name of each field plus a brief description and whether the field is mandatory. Whether a field is required or not is based on the context of the examples. The underlying API definition may mark a field as optional, but in some contexts, the fields must be supplied.\nFor a 40x, 50x or any other error response please see error response codes\nOnboard Resource Driver Onboard a new resource driver with Brent\nRequest    Aspect Value     Endpoint URL /api/resource-manager/resource-drivers   Content-Type: application/json OR application/yaml   HTTP Method POST    Body    Field Description Mandatory     type The type of the driver Yes   baseUri The base URI of the Resource driver so it may be accessed from Brent Yes   certificate An SSH certificate to communicate with an HTTPs-enabled Resource driver Yes, if the baseUri is https    Response    Aspect Value     Content-Type application/json   Success Status Code 201    Headers    Field Description     location Endpoint to Resource driver resource    Body    Field Description     id The ID assigned to the Resource driver   type The type of the driver   baseUri The base URI of the Resource driver so it may be accessed from Brent    View Details of Resource Driver View details of a resource driver\nRequest    Aspect Value     Endpoint URL /api/resource-manager/resource-drivers/{id}   HTTP Method GET    Path Parameters:    Field Description Mandatory     id ID of the resource driver Yes    Response    Aspect Value     Content-Type application/json   Success Status Code 200   Not Found Status Code 404    Body    Field Description     id The ID assigned to the resource driver   type The type of the driver   baseUri The base URI of the resource driver so it may be accessed from Brent    View Details of Resource Driver by Type View details of a resource driver, retrieved by the type it manages\nRequest    Aspect Value     Endpoint URL /api/resource-manager/resource-drivers?type={type}   HTTP Method GET    Query Parameters:    Field Description Mandatory     type type of the driver Yes    Response    Aspect Value     Content-Type application/json   Success Status Code 200   Not Found Status Code 404    Body    Field Description     id The ID assigned to the resource driver   type The type of the driver   baseUri The base URI of the resource driver so it may be accessed from Brent    Remove Resource Driver Remove a resource driver\nRequest    Aspect Value     Endpoint URL /api/resource-manager/resource-drivers/{id}   HTTP Method DELETE    Path Parameters:    Field Description Mandatory     id ID of the resource driver Yes    Response    Aspect Value     Content-Type application/json   Success Status Code 204   Not Found Status Code 404    "},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/reference/lm-api/api-definition/resource-health-events/","title":"Resource Health Events","tags":[],"description":"","content":"Integrity Events These events are sent to enable a resource to indicate if a resource is working or broken.\nExample Integrity Metric Event {\r\u0026quot;metricKey\u0026quot; : \u0026quot;142971c5-a84b-4d34-af15-435ba8640aec\u0026quot;,\r\u0026quot;metricName\u0026quot; : \u0026quot;h_integrity\u0026quot;,\r\u0026quot;integrity\u0026quot; : \u0026quot;OK\u0026quot;,\r\u0026quot;message\u0026quot; : \u0026quot;Everything is working\u0026quot;\r}\rField Details    Field Description Mandatory     metricKey The key given to the resource manager when the resource was created as a token to be used within these messages Yes   metricName The name of the metric as defined in the resource descriptor Yes   integrity A value indicating if the resource associated with the metric Key is working allowed values are “OK” for working and “BROKEN” when healing is required Yes   message An optional test string to include information about the integrity of the resource. for example it may include an error code No    Load Events These events indicate a resources load. This may be an aggregation across many resources as seen for example by a load balancer.\nExample Load Metric Message {\r\u0026quot;metricKey\u0026quot; : \u0026quot;818127b3-1904-4737-a60c-8c7bab73532d\u0026quot;,\r\u0026quot;metricName\u0026quot; : \u0026quot;h_load\u0026quot;,\r\u0026quot;load\u0026quot; : 76,\r\u0026quot;message\u0026quot; : \u0026quot;Load is high\u0026quot;\r}\rField Details    Field Description Mandatory     metricKey The key given to the resource manager when the resource was created as a token to be used within these messages Yes   metricName The name of the metric as defined in the resource descriptor Yes   load A value between 0 and 100, indicating the load on the resources Yes   message An optional test string to include information about the integrity of the resource. for example it may include an error code No    "},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/cicd/developing-a-project/add-tests-to-package/","title":"Add Tests to Package","tags":[],"description":"","content":"Objectives This section shows how to add VNF functional behaviour tests or Network Service performance and interoperability test artifacts to your project structure in the CI/CD Hub.\nPre-requisites  Development VIM and CI/CD Hub and ALM up and running LMCTL setup on your local machine with a profile configured for the above development environment VNF design documenting how test scenarios are expected to be run and the metrics and behaviour from all participating VNFs that constitute a successful test run. VNF design includes the additional VNF or Network Service test package versions required to actually run the test software itself (e.g. traffic generators and/or probes etc) and the test software images.  Push your project to Lifecycle Manager If you have not already done so, make sure the project you are working on is loaded to your development environment. This will be required to use the Agile Lifecycle Manager (ALM) tools to design behaviour tests.\n$ git clone \u0026lt;YOUR PROJECT FROM GOGS\u0026gt;\r$ cd \u0026lt;YOUR PROJECT DIR\u0026gt;\r$ lmctl project push dev\rLoad test software images The first step is to check the VIM in your development environment and load any test software images required to run the test software. These images should be stored in the Nexus general repository.\nLoad dependent packages Check ALM in your development environment and load any extra VNF or Network Service packages required from the Nexus general repository.\nFor each extra package:\n$ wget http://\u0026lt;nexus ip address and port\u0026gt;\u0026gt;/repository/raw/\u0026lt;package name\u0026gt;/\u0026lt;package-name\u0026gt;-\u0026lt;package-version\u0026gt;.tgz\r$ lmctl pkg push \u0026lt;pakage-name\u0026gt;-\u0026lt;package-version\u0026gt;.tgz dev\rCreate Behaviour Tests Log into the development ALM and design and run the new VNF behaviour tests. When the tests are performing as expected it is time to export the tests into your project and commit back to Gogs.\nExport Tests and Commit On your local machine, run the following commands to retrieve the behaviour tests designed in the previous step.\n$ cd \u0026lt;YOUR PROJECT DIR\u0026gt;\r$ lmctl project pull dev\rNow, run the following commands to commit your changes to the Gogs git repository.\n$ git add .\r$ git commit -m 'Additional Behaviour Tests Added'\r$ git push origin master develop\rMoving from Development to Test Once you are satisfied the VNF is behaving as expected and your tests are validating that this is the case, the next step is to promote your project from development to test. Merging your changes with the master branch will trigger a CI pipeline which will run a series of additional tests that validate your package is ready for others to use.\nTo merge your changes with the master branch, you need to run the following commands on you local machine:\n$ git checkout master\r$ git merge --no-ff develop -m 'validate version'\r$ git push\r"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/authenticated-api-requests/","title":"Authenticated API Requests","tags":[],"description":"","content":"This guide details how to connect from a calling system into the Agile Lifecycle Manager (ALM) APIs securely.\nPre-requisites  an existing installation of the ALM a valid set of client credentials which can be used for authentication. If not, see Managing Client Credentials  Making an API call LM uses the standard OAuth 2.0 mechanism to authenticate incoming API requests. This requires some additional tokens to be setup in the headers of any HTTPS request that is made.\nThe following steps make use of the Postman Client to demonstrate how to make authenticated API calls.\nRequest a new Access Token Start by getting a new access token using type OAuth 2.0 and the Access Token URL https://\u0026lt;app hostname\u0026gt;:\u0026lt;app port\u0026gt;/oauth/token. If using the default installation, the hostname and port are as shown below.\nEnter the following values into these fields:\n   Postman Field Value     TYPE OAuth 2.0   Add authentication data to Request Headers   Access Token (adjust hostname and port accordingly) https://app.lm:32443/oauth/token    Click the [Get New Access Token] button. Postman will prompt you for the security credentials it needs to encode into the Header of the request:\nEnter the following values into these fields:\n   Postman Field Value     Token Name \u0026lt;Any name you like to use\u0026gt;   Grant Type Client Credentials    Client ID \u0026lt;The Client ID\u0026gt;   Client Secret \u0026lt;The Client Secret\u0026gt;   Scope all   Client Authentication Send as Basic Auth Header    The “Client Secret” is the value setup as part of the initial installation of LM matching the “Client ID” you have chosen\nClick the [Get New Access Token] button. If your request is successful, then you will get a JSON response similar to this:\n{ \u0026quot;access_token\u0026quot;: \u0026quot;fdf8e754-1abe-42ae-b064-7969b05788ca\u0026quot;, \u0026quot;token_type\u0026quot;: \u0026quot;bearer\u0026quot;, \u0026quot;expires_in\u0026quot;: 1199, \u0026quot;scope\u0026quot;: \u0026quot;all\u0026quot; } Using the new Access Token Postman will automatically embed the token in the new Access Token in the request headers when an authorization type of OAuth 2.0 is selected.\nTo use the token outside of Postman, i.e. as part of your you system-to-system calls, ensure any requests contain the header “Authorization” with the token as the value prefixed with “Bearer “, e.g. Authorization: Bearer fdf8e754-1abe-42ae-b064-7969b05788ca\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/best-practices/demos/","title":"Demos","tags":[],"description":"","content":""},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/reference/lm-api/api-definition/error-responses/","title":"Error Responses","tags":[],"description":"","content":"All the API calls described in this section will return the following information in the Response Body when an error is encountered (e.g. for a 404, 403, 500 Response Code):\n{\r\u0026quot;details\u0026quot;: {},\r\u0026quot;localizedMessage\u0026quot;: \u0026quot;string\u0026quot;,\r\u0026quot;url\u0026quot;: \u0026quot;string\u0026quot;\r}\rResponse properties\n   Field Description     details Any additional information related to the error that may be useful during debugging   localizedMessage User-friendly description of the error that has occurred   url the URL of the root cause of the problem or null if unknown    "},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/installation/cicdhub/install-cicdhub/","title":"Install","tags":[],"description":"","content":"This section assumes you have followed the configuration steps and now have:\n a CI/CD Hub Helm chart a custom values YAML file  Installing with Helm Install CI/CD Hub with the helm install command:\nStandard:\nhelm install \u0026lt;cicdhub-helm-chart\u0026gt; --name \u0026lt;your-release-name\u0026gt; --namespace \u0026lt;your-namespace\u0026gt; -f \u0026lt;your-custom-values-file\u0026gt;\rICP:\nhelm install \u0026lt;cicdhub-helm-chart\u0026gt; --name \u0026lt;your-release-name\u0026gt; --namespace \u0026lt;your-namespace\u0026gt; -f \u0026lt;your-custom-values-file\u0026gt; --tls\r cicdhub-helm-chart - path to the CI/CD Hub Helm chart your-release-name - unique name used to identify this installation in Helm your-namespace - Kubernetes namespace to install into (leave out to use default) your-custom-values-file - path to the YAML file created with any configuration overrides (if you have any)  For example:\nhelm install cicdhub-2.0.5.tgz --name cicdhub --namespace cicdhub -f custom-values.yaml\rCheck the status of your installation with helm or kubectl:\nkubectl get pods -n \u0026lt;your-namespace\u0026gt;\rStandard:\nhelm status \u0026lt;your-release-name\u0026gt;\rICP:\nhelm status \u0026lt;your-release-name\u0026gt; --tls\rWait for all pods to show least one ready instance.\nUpdate storage to prevent data loss Persistent volumes dynamically provisioned have a default reclaim policy of Delete - this means the volume will be deleted when the claim for it is removed, which takes place on an uninstall. It is recommended to update the policy to Retain so any data can be recovered or the volume can be removed manually.\nList the volumes in your cluster to find any that may need updating.\nkubectl get pv\rFor each volume you wish to update, use patch to change the policy:\nkubectl patch pv \u0026lt;your-pv-name\u0026gt; -p '{\u0026quot;spec\u0026quot;:{\u0026quot;persistentVolumeReclaimPolicy\u0026quot;:\u0026quot;Retain\u0026quot;}}'\rAccessing Services It is recommended that you access each service and any default username/passwords immediately\nBe sure to update any ports, hostnames, usernames and passwords shown if you provided overrides in your custom values file.\nGogs Access the UI in your browser by navigating to http://\u0026lt;your-cicdhub-host\u0026gt;:32734 or git.cicdhub:\u0026lt;ingress-port\u0026gt; (see using Ingress hosts).\nThere is no default user for Gogs, instead you must register an account through the UI (or API). The first registered user will automatically be assigned the admin role.\nJenkins Access the UI in your browser by navigating to http://\u0026lt;your-cicdhub-host\u0026gt;:32732 or jenkins.cicdhub:\u0026lt;ingress-port\u0026gt; (see using Ingress hosts).\nThe default username and password is admin/admin, unless you provided alternatives.\nOpenldap Accessing Openldap requires an LDAP enabled client tool such as LDAP Admin. Create a new connection with the following settings:\n Host: \u0026lt;your-cicdhub-host\u0026gt; Port: 32737 Base: dc=lm,dc=com (change to reflect your chosen domain if overridden. For example, example.com would become: dc=example,dc=com) Username: cn=admin,dc=lm,dc=com (repeat changes to dc= as above) Password: admin (change to reflect your chosen managerPassword if overridden)  Configure Openldap with LM Users To make the Openldap installed as part of the CI/CD Hub usable in LM environments, you will need to create the user schema.\nFirst create a file named schema.ldif and add the following contents:\ndn: cn=schema,cn=config\rchangetype: modify\radd: olcAttributeTypes\rolcAttributeTypes: ( 2.25.128424792425578037463837247958458780603.1\rNAME 'isSuspended'\rDESC 'Is user suspended'\rEQUALITY caseIgnoreMatch\rSUBSTR caseIgnoreSubstringsMatch\rSYNTAX 1.3.6.1.4.1.1466.115.121.1.15 )\r-\radd: olcObjectClasses\rolcObjectClasses: ( 2.25.128424792425578037463837247958458780603.3\rNAME 'extendedPerson'\rDESC 'extendedPerson'\rSUP person\rSTRUCTURAL\rMAY (isSuspended)\r)\rImport this LDIF file to Openldap with an LDAP client. For a Unix based system you can install ldap-utils:\nsudo apt-get install ldap-utils\rThen use the ldapmodify command:\nldapmodify -acx -H ldap://\u0026lt;your-cicdhub-host\u0026gt;:32737 -D \u0026quot;cn=admin,cn=config\u0026quot; -w config -f schema.ldif\rCreate a second file named users.ldif and add the following contents:\ndn: ou=groups,dc=lm,dc=com\robjectclass: top\robjectclass: organizationalUnit\rou: groups\rdn: ou=people,dc=lm,dc=com\robjectclass: top\robjectClass: organizationalUnit\rou: people\rdn: uid=Jack,ou=people,dc=lm,dc=com\robjectClass: extendedPerson\robjectClass: uidObject\rcn: Jack\rsn: Jack\ruid: Jack\r# BCrypt encdoded version of 'jack'\ruserPassword: $2a$10$nwrT968GpcF8/EWyO3yBQOuIcB7a/PBHZ2bMMX0JuuAx5X04GmxJi\rdn: uid=Jill,ou=people,dc=lm,dc=com\robjectClass: extendedPerson\robjectClass: uidObject\rcn: Jill\rsn: Jill\ruid: Jill\r# BCrypt encdoded version of 'jill'\ruserPassword: $2a$10$2qVcIE3iEdaKpQZUdJtVeuGWE2rzF7PTXwqfwa6.ZcylO9XmEmMVu\rdn: uid=John,ou=people,dc=lm,dc=com\robjectClass: extendedPerson\robjectClass: uidObject\rcn: John\rsn: John\ruid: John\r# BCrypt encdoded version of 'john'\ruserPassword: $2a$04$17kY20gJy5KpWCgtypxoTeIhYuGbKTtHm7BJyuGZKkL0Y/78P2iRO\rdn: uid=Jane,ou=people,dc=lm,dc=com\robjectClass: extendedPerson\robjectClass: uidObject\rcn: Jane\rsn: Jane\ruid: Jane\r# BCrypt encdoded version of 'jane'\ruserPassword: $2a$04$XG5p82YSksI26F43EgHoQ.kwX6ubQMw1J5f6sxKlupzpUiDTW1giu\r#Test user with rootSecAdmin access rights only\rdn: uid=Derek,ou=people,dc=lm,dc=com\robjectClass: extendedPerson\robjectClass: uidObject\rcn: Derek\rsn: Derek\ruid: Derek\r# BCrypt encdoded version of 'derek'\ruserPassword: $2a$04$HiXRVJyVos2nP/9/bx2FReH3YrIlFdsjhPCJJZM16mvd/fpluUAC6\r#Test user with no access rights\rdn: uid=Lisa,ou=people,dc=lm,dc=com\robjectClass: extendedPerson\robjectClass: uidObject\rcn: Lisa\rsn: Lisa\ruid: Lisa\r# BCrypt encdoded version of 'lisa'\ruserPassword: $2a$04$c4sf/e7uAaWR1hvSbc60Gelyhf78l8EBv2rpelrD0yPO/VWYNapa.\r#Test user with readOnly access rights\rdn: uid=Kim,ou=people,dc=lm,dc=com\robjectClass: extendedPerson\robjectClass: uidObject\rcn: Kim\rsn: Kim\ruid: Kim\r# BCrypt encdoded version of 'kim'\ruserPassword: $2a$04$U.7kTWv95LajKcBqMWKtJuI1FaFZzNjc3k5.J4uMMi5puN4rc8GI2\r#Test user, Suspended user with SLMAdmin access rights\rdn: uid=Steve,ou=people,dc=lm,dc=com\robjectClass: extendedPerson\robjectClass: uidObject\rcn: Steve\rsn: Steve\ruid: Steve\risSuspended: true\r# BCrypt encdoded version of 'steve'\ruserPassword: $2a$10$nj.uwhp660e3DJveI84QGumuM5b9yrqNp4KLEWEXr.pxAO0U4Ok6C\rdn: cn=SLMAdmin,ou=groups,dc=lm,dc=com\robjectclass: groupOfNames\rcn: SLMAdmin\rmember: uid=Jack,ou=people,dc=lm,dc=com\rmember: uid=John,ou=people,dc=lm,dc=com\rmember: uid=Jane,ou=people,dc=lm,dc=com\rmember: uid=Steve,ou=people,dc=lm,dc=com\rdn: cn=Portal,ou=groups,dc=lm,dc=com\robjectclass: groupOfNames\rcn: Portal\rmember: uid=Jill,ou=people,dc=lm,dc=com\rdn: cn=RootSecAdmin,ou=groups,dc=lm,dc=com\robjectclass: groupOfNames\rcn: RootSecAdmin\rmember: uid=Jane,ou=people,dc=lm,dc=com\rmember: uid=Derek,ou=people,dc=lm,dc=com\rdn: cn=ReadOnly,ou=groups,dc=lm,dc=com\robjectclass: groupOfNames\rcn: ReadOnly\rmember: uid=Kim,ou=people,dc=lm,dc=com\rImport the file into Openldap:\nldapmodify -acx -H ldap://\u0026lt;your-cicdhub-host\u0026gt;:32737 -D \u0026quot;cn=admin,dc=lm,dc=com\u0026quot; -w admin -f users.ldif\rNexus Access the UI in your browser by navigating to http://\u0026lt;your-cicdhub-host\u0026gt;:32739. API requests may also be made to this address.\nThe default username and password combination is admin/admin123.\nDocker Registry The docker registry can be accessed through the official docker client. Images can be pushed to and pulled from the registry using \u0026lt;your-cicdhub-host\u0026gt;:32736. You will need to add the registry address as an insecure registry in your docker daemon file (usually located at /etc/docker/daemon.json):\n{\r\u0026quot;insecure-registries\u0026quot;: [\r\u0026quot;\u0026lt;your-cicdhub-host\u0026gt;:32736\u0026quot;\r]\r}\rThe example commands below show how to pull an image from Docker Hub and push it into the registry:\ndocker pull hello-world\rdocker tag hello-world \u0026lt;your-cicdhub-host\u0026gt;:32736/hello-world\rdocker push \u0026lt;your-cicdhub-host\u0026gt;:32736/hello-world\rIngress The nginx-controller is ready to serve HTTP traffic at http://\u0026lt;your-cicdhub-host\u0026gt;:32080 and HTTPS traffic at https://\u0026lt;your-cicdhub-host\u0026gt;:32443. As Ingress uses the hostname to route traffic to the desired service, you will need to either:\n add the hostname(s) to your hosts file configure a proxy (such as Apache2) to route traffic to the target Kubernetes ingress controller configure a nameserver to route traffic targeting the hostname(s) to your Kubernetes cluster  Update Hosts File Adding hostnames to your hosts file is an easy way to get started but is not suitable long term as this will need to be repeated on every machine that will access the Hub.\nIf you\u0026rsquo;d like to configure access to Ingress routes this way, then add the following to your hosts file:\n\u0026lt;your-cicdhub-server-ip-address\u0026gt; jenkins.cicdhub git.cicdhub\rYou may now access Jenkins at http://jenkins.cicdhub:32080 and Gogs at http://git.cicdhub:32080.\nProxy with Apache2 If you have an available Apache2 server (or can install one) then it\u0026rsquo;s possible to make the Ingress hosts available through an IP address.\nYou will need to add the Ingress hostnames to the hosts file of the machine running Apache2, see Hostfile.\nCreate a site configuration file:\ntouch /etc/apache2/sites-available/cicdhub.conf\rAdd the following configuration to the file, updating the Ingress hostnames and ports if overridden. Replace \u0026lt;your-gogs-proxy-port\u0026gt; and \u0026lt;your-jenkins-proxy-port\u0026gt; with desired ports for your instances.\nListen \u0026lt;your-gogs-proxy-port\u0026gt;\r\u0026lt;VirtualHost *:\u0026lt;your-gogs-proxy-port\u0026gt;\u0026gt;\rProxyPass / http://git.cicdhub:32080/\rProxyPassReverse / http://git.cicdhub:32080/\r\u0026lt;/VirtualHost\u0026gt;\rListen \u0026lt;your-jenkins-proxy-port\u0026gt;\r\u0026lt;VirtualHost *:\u0026lt;your-jenkins-proxy-port\u0026gt;\u0026gt;\rProxyPass / http://jenkins.cicdhub:32080/\rProxyPassReverse / http://jenkins.cicdhub:32080/\r\u0026lt;/VirtualHost\u0026gt;\rAdd the site to Apache2 (you may need to reload the Apache2 service for the changes to take affect):\na2ensite cicdhub.conf\rYou may now access Jenkins at http://\u0026lt;apache2-server-ip\u0026gt;:\u0026lt;your-jenkins-proxy-port\u0026gt; and Gogs at http://\u0026lt;apache2-server-ip\u0026gt;:\u0026lt;your-gogs-proxy-port\u0026gt;.\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/installation/resource-manager/ansible-rm/install-ansible-rm/","title":"Install","tags":[],"description":"","content":"This section assumes you have followed the configuration steps and now have:\n an Ansible RM Helm chart a custom values YAML file a target Kubernetes namespace with Agile Lifecycle Manager (ALM) installed  Installing with Helm Install the Ansible RM with the helm install command.\nhelm install \u0026lt;ansible-rm-helm-chart\u0026gt; --name \u0026lt;your-release-name\u0026gt; --namespace \u0026lt;your-namespace\u0026gt; -f \u0026lt;your-custom-values-file\u0026gt;\r ansible-rm-helm-chart - path to the Ansible RM Helm chart your-release-name - unique name used to identify this installation in Helm your-namespace - Kubernetes namespace with LM already installed your-custom-values-file - path to the YAML file created with any configuration overrides (if you have any)  For example:\nhelm install osslm-ansible-resource-manager-1.3.6.tgz --name osslm-ansible-rm --namespace lm -f custom-values.yaml\rCheck the status of your installation with helm or kubectl:\nhelm status \u0026lt;your-release-name\u0026gt;\rkubectl get pods -n \u0026lt;your-namespace\u0026gt;\rWait for the osslm-ansible-rm pod to be marked as ready.\nAccessing the Ansible RM Once installation is complete the Swagger UI for the Ansible RM can be loaded in the browser at https://\u0026lt;your-host-ip\u0026gt;:31081/api/v1.0/resource-manager/ui/ (if you changed the HTTPS node port then update the port in the URL to match the value set).\nInitialize Ansible RM Database Before the Ansible RM can be used you must initialize the database tables. In your browser, navigate to the Swagger UI and expand the driver additions APIs.\nExecute the POST API named /database to initialize the database.\nNext Steps Now that the RM is installed it needs to be added/registered to the Lifecycle Manager and Deployment Locations/VIMs need to be registered against the RM in LM. Register RM\nThis will allow LM to make API calls to the appropriate RM instance when seeking to perform Lifecycle Transitions or Operations against a given deployment location/VIM. It is possible to add as many RM instances as may be required by your deployment.\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/reference/lm-api/interface-architecture/","title":"Interface Architecture","tags":[],"description":"","content":"The Agile Lifecycle Manager (ALM) API is responsible for interactions with the operations available from LM.\nAPI Interaction Principles Each of the major components within ALM have been built as micro-services using HTTP as the transport mechanism for requests and responses.\nThe ALM API uses a combination of REST style interfaces and RPC interfaces.\nThe RPC-style interface is used to submit Intents to ALM. All other interfaces use a REST-style.\nBecause the ALM API has been designed to provide a combination of REST and RPC interfaces it does not always conform to all the details of the IETF RFC 7231 specification. However, it is our stance that the ALM API adheres to the spirit of the IETF recommendations.\nUnless detailed otherwise, all the message descriptions are in JSON format and should be submitted with the HTTP content-type header of “application/json”.\nAll Dates will conform to the ISO-8601 standard.\nSupported Methods Each API Service Endpoint can potentially implement a different set of HTTP request methods. The methods generally supported within ALM are:\n   Method Description     GET The GET method requests a representation of the specified resource. Requests using GET only retrieve data and have no other effect.   PUT The PUT method requests that the enclosed entity be stored under the supplied URI. If the URI refers to an existing resource, it is modified; if the URI does not point to an existing resource, then the request will be rejected.   POST The POST method requests that the server accept the entity enclosed in the request as a new instance of the resource identified by the URI   DELETE The DELETE method simply removes the specified resource (if it exists).    API Versioning Currently the ALM API does not support API versioning and has no plans to do so.\nAs far as possible and for as long as possible the API will be maintained to be backwards-compatible.\nPossible HTTP error response codes The following is a table of HTTP response codes that can be returned in various error scenarios. Any client should expect that any API call can return these codes under exceptional circumstances\n   Code Description     400 – Bad Request The request contained invalid information. This may be an incorrect field, invalid value or an inconsistent state on a dependent resource. The HTTP response body should contain a JSON message with further details of the specific issue.   401 – Not Authorized The user or Client making the API request is not authorized to do so. Check the role configuration of the user/Client who is making the request   404 – Not Found The requested endpoint could not be found. or Requested Entity cannot be found   409 - Conflict LM has been unable to process the request due to a conflict produced by some of the information supplied. For example, attempting to create two Resource Managers with the same name.   415 – Unsupported Media Type The HTTP request payload has a Content-Type that is not supported by the LM API.   500 – Internal Server Error An internal error has occurred whilst fulfilling the request. The HTTP response body should contain a JSON message with further details. In some situations, it may be necessary for a system administrator to consult the logs for further information.   502 – Bad Gateway A remote system has failed to respond correctly causing this request to fail. The HTTP response body should contain a JSON message with further details. In some situations, it may be necessary for a system administrator to consult the logs for further information.   503 – Service Unavailable LM is unable to process this request at this time. The request should not be retried until the underlying problem is resolved. The HTTP response body should contain a JSON message with further details. In some situations, it may be necessary for a system administrator to consult the logs for further information.    Error Responses ALM Generated Error All the API calls described in this specification will return the following information in the Response Body when an error is encountered (e.g. for a 40x, 50x Response Code):\n{\r\u0026quot;details\u0026quot;: {},\r\u0026quot;localizedMessage\u0026quot;: \u0026quot;string\u0026quot;,\r\u0026quot;url\u0026quot;: \u0026quot;string\u0026quot;\r}\rResponse Body content type: application/json\nResponse properties\n   Property Name Description Mandatory     details Any additional information related to the error that may be useful during debugging Yes   localizedMessage The most user-friendly description of the error that has occurred Yes   url the URL of the root cause of the problem or null if unknown No    A more fully populated ALM generated error report would look like this:\n{\r\u0026quot;url\u0026quot;: /api/resource-manager/configuration/923227664489862,\r\u0026quot;localizedMessage\u0026quot;: \u0026quot;A FATAL ALM Driver error has occurred: Unknown Resource Manager 923227664489862\u0026quot;,\r\u0026quot;details\u0026quot;: {\r\u0026quot;responseHttpStatus\u0026quot;: 500,\r\u0026quot;errorStatus\u0026quot;: \u0026quot;FATAL\u0026quot;,\r\u0026quot;responseData\u0026quot;: {\r\u0026quot;url\u0026quot;: \u0026quot;http://galileo:8283/api/topology/resource-managers/923227664489862\u0026quot;;,\r\u0026quot;localizedMessage\u0026quot;: \u0026quot;Unknown Resource Manager 923227664489862\u0026quot;,\r\u0026quot;details\u0026quot;: {}\r}\r}\r}\rMore than one set of error details may be included. These will show the errors generated by components internal to ALM and passed back through the layers of requests that were made.\nIf a support call becomes necessary, please include all the information from any ALM generated error reports as this greatly assists with tracking down the root cause of the problem.\nNon-ALM Generated Error Very rarely an error may occur within ALM that for whatever reason cannot be handled gracefully and for which a ALM specific error response is not available.\nThese errors look like this:\n{\r\u0026quot;timestamp\u0026quot;: \u0026quot;2017-08-10T15:28:19.586+0000\u0026quot;,\r\u0026quot;status\u0026quot;: 500,\r\u0026quot;error\u0026quot;: \u0026quot;Internal Server Error\u0026quot;,\r\u0026quot;exception\u0026quot;: \u0026quot;org.springframework.web.client.ResourceAccessException\u0026quot;,\r\u0026quot;message\u0026quot;: \u0026quot;I/O error on GET request for \\\u0026quot;http://9.20.64.abc:8295/api/resource-manager/configuration\\\u0026quot;: 9.20.64.abc; nested exception is java.net.UnknownHostException: 9.20.64.abc\u0026quot;,\r\u0026quot;path\u0026quot;: \u0026quot;/api/resource-managers\u0026quot;\r}\rResponse properties\n   Property Name Description Mandatory     timestamp The date and time to error occurred.    status The HTTP status code returned    error A textual description of the HTTP status code returned    exception The internal Exception type that was raised.    message The actual error reported    path If it was an API call or web service request that failed, then this field will identify what was being requested.     HTTP STATUS CODE USAGE Within the header of the HTTP response received in reply to an HTTP request there is a 3-digit decimal status code. The first digit of the status code specifies one of five standard classes of responses.\n   1xx Informational responses (not used explicitly by ALM)\r    2xx Success\r    3xx Redirection (not used explicitly by ALM)\r    4xx Client errors\r    5xx Server errors\r   The distinction between the HTTP 4xx and 5xx code is intended to make integration easier. An HTTP 404 status code basically means “I cannot find what you are asking for”. An HTTP 400 status code means “The service exists but you have not sent it the information it needs/recognizes” so you have successfully connected with the API end-point and sent it a request but the request itself was not understood. Finally, an HTTP 500 status code means “I understand what you have asked me to do but for some reason I am unable to do it”.\n2xx Status Codes This class of status codes indicates the action requested by the client was received, understood, accepted, and processed successfully.\nDifferent ALM components return different HTTP 2xx status codes dependent on the nature of the request made. To find out which HTTP 2xx status codes will be returned please refer to the individual microservice API guides\n4xx Status Codes This class of status code is intended for situations in which the error seems to have been caused by the client/user. A client can also mean another system that is sending a request into ALM.\nTo understand how the different HTTP 4xx status codes are used we need to separate the definition of a Service Endpoint from the data it is being asked to use.\nOur definition of the Service Endpoint is:\n   The full path to the Service required excluding any variable parameters in the URL.\r   Given this request:\nhttp://\u0026lt;hostname\u0026gt;:\u0026lt;Port\u0026gt;/api/resource-manager/configuration/9.20.64.abc The Service Endpoint is:\nhttp://\u0026lt;hostname\u0026gt;:\u0026lt;Port\u0026gt;/api/resource-manager/configuration\rAll POST requests will send their data in the body of the HTTP request.\nAll GET and DELETE requests will send their data as variable URL parameters.\nAll PUT requests will send their data as variable URL parameters and/or as data in the body of the HTTP request.\nHTTP 400 vs 404 Statuses Within ALM the handling of HTTP 400 \u0026amp; 404 status codes varies depending on what type of request was made.\nGenerally, the 4xx HTTP Status codes will be produced by the ALM functionality concerned with validating and verifying the incoming requests.\nThis table summarizes the internal “rules” StartOSS LM uses regarding when to return an HTTP 400 vs 404 status code:\n   Type URL Invalid URL (with variables) refers to an Entity that does not exist Invalid Body Content Body refers to an Entity that does not exist     REST 404 404 400 N/A   RPC 404 N/A Has no URLs pointing to Entities 400 400    REST-style Requests An HTTP 404 will be returned if the entire requested Service URL (including any variable parameters) does not exist. This could be for one of the following reasons:\n   The Service Endpoint requested does not exist or is not available (check your host name and ports).\r    The requested resource (as identified by the URL variable parameters) does not exist.\r   The HTTP 400 status code (Bad Request) is used by the REST-style requests if the POST or PUT contains bad data. For example, if ALM is given a Descriptor that it does not recognize or is unable to use.\nRPC-style Requests An HTTP 404 will be returned if the requested Service Endpoint does not exist.\nAn HTTP 400 will be returned if there is incorrect content in the request body sent to the Service Endpoint.\nHTTP 409 Status The HTTP 409 status code (Conflict) indicates that the request could not be processed because of conflict in the request, such as the requested resource is not in the expected state or the result of processing the request would create a conflict within the resource.\nExamples of where an HTTP 409 status code would be used within ALM are:\n   Data constraint violations\r    Concurrent modification exceptions.\r   HTTP 5xx Status Response status codes beginning with the digit \u0026ldquo;5\u0026rdquo; indicate cases in which ALM is unable to perform the request/action that has been submitted.\nIt indicates that something internally within ALM has prevented execution of the request, but that there was nothing syntactically wrong with the request itself. That is, the request is syntactically correct, all the mandatory parameters have been provided and have passed basic validation checks.\nTo put it another way the API service is saying:\n “I understand what you have asked me to do and you have provided me with all the information I need/expect but I am unable to do what you have requested.”\n The most usual case will be a simple HTTP 500 status code backed up with a JSON payload in the response body. This payload will contain a human-readable error message summarizing the problem and a section containing further technical details if available.\nAPI Sections The API is divided into four sections. These provide access to the different functionality provided by LM. This API is expected to be called by external OSS systems to perform both fulfillment tasks and some fault management tasks.\nAssembly Orchestration LM allows the creation of Assemblies. These allow services to be created by LM which will interact with resource managers to create and manage virtual resources that need to be provided for the service to work.\nThis part of the API includes two endpoints; the first allows the External OSS system to request a transition against an assembly instance, including a requesting for a new assembly instance. The second allows the External OSS to poll LM to find out the state of the request.\nAsynchronous Events In response to Assembly Orchestration requests, LM will also place on a Kafka bus messages that describe the key events that occur during the processing of the request and also a message to indicate when the processing has been completed. It is recommended that the External OSS use this mechanism to check the state of requests rather than using the polling interface defined in the previous section.\nAssembly Topology External OSS may need details of the assembly instances and of the components that it is comprised. The topology contains a hierarchy of the components of an assembly. A component may be either a resource or an assembly. It is also possible to request details of the events used during the assembly’s life.\nResource Manager Handling Resource Managers are responsible for managing the actual resources that needed for a service to work. LM needs to be told which resources managers it will interface to. This process is known as \u0026lsquo;Onboarding a Resource Manager\u0026rsquo;. The LM API provides a set of calls that allows a new resource manager instance to be onboarded to LM, and also removed from LM. When a resource manager is onboarded the set of resource types and locations that they manager will be extracted using the Resource Manager API. The API also provides an endpoint that will make LM request the associated resource manager for a set of updated resources. Any existing resources will remain unchanged when this update occurs.\nSupported Media Types The media types supported by the ALM API are:\n   application/hal+json\r    application/octet-stream\r    text/plain\r    application/xml\r    text/xml\r    application/x-www-form-urlencoded\r    application/*+xml\r    multipart/form-data\r    **application/json**\r    **application/\\*+json**\r   In most cases using JSON to supply information as part of an API request is the preferred option.\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/reference/","title":"Reference","tags":[],"description":"","content":""},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/reference/lm-api/api-definition/resource-manager/resource-managers/","title":"Resource Managers","tags":[],"description":"","content":"The following details the API used to manage Resource Managers within LM.\nAssociated with each definition are details of the request parameters and responses. These include the name of each field plus a brief description and whether the field is mandatory. Whether a field is required or not is based on the context of the examples. The underlying API definition may mark a field as optional, but in some contexts, the fields must be supplied.\nFor a 40x, 50x or any other error response please see error response codes\nOnboard Resource Manager Creates a record of a Resource Manager within LM and begins the onboarding process. When this request is placed LM will register the Resource Manager and then request details of all the Resource types that it manages.\nRequest    Aspect Value     Endpoint URL /api/resource-managers   Content-Type application/json   HTTP Method POST    Body    Field Description Mandatory     name The name the Resource Manager to be known by Yes   url The url where LM can find the Resource Manager interface Yes    Example:\n{\r\u0026quot;name\u0026quot;: \u0026quot;test\u0026quot;,\r\u0026quot;url\u0026quot;: \u0026quot;http://localhost:8295/api/resource-manager\u0026quot;\r}\rResponse    Aspect Value     Content-Type application/json   Response Code 201 (Created)    Headers    Field Description     location Endpoint to Resource Manager resource    Body The body will include a report on the onboarding process, which specifies the Deployment Locations and Resource types discovered from the Resource Manager and the impact this had on LM.\n   Field Description     resourceManagerOperation The operation attempted on the Resource Manager (this will be ADD)   deploymentLocations List of reports for the DeploymentLocation onboarding operations attempted, indexed by name   resourceTypes List of reports for the ResourceType onboarding operations attempted, indexed by name    Each entry for deploymentLocations and resourceTypes will have the following fields:\n   Field Description     operation Operation attempted on the Deployment Location or Resource Descriptor (ADD/UPDATE)   success Boolean indicating if the operation was successful or not   reason A description of the operation\u0026rsquo;s result. Can be used for success or failure although the most frequent use is to explain why an operation failed   details Any additional details that should be known about the result of this operation    Get Resource Manager Retrieve information about a Resource Manager within LM. The ID in the request is the unique name of the Resource Manager as defined by the “name” field when it was created.\nRequest    Aspect Value     Endpoint URL /api/resource-managers/{id}   Content-Type application/json   HTTP Method METHOD    Path Parameters    Field Description Mandatory     id ID (name) of the Resource Manager to retrieve Yes    Response    Aspect Value     Content-Type application/json   Response Code 200 (OK)    Body    Field Description     name The name the Resource Manager   type Type of the Resource Manager   url The url where LM can find the Resource Manager interface    Example:\n{\r\u0026quot;name\u0026quot;: \u0026quot;test\u0026quot;,\r\u0026quot;type\u0026quot;: \u0026quot;test-rm\u0026quot;,\r\u0026quot;url\u0026quot;: \u0026quot;http://localhost:8295/api/resource-manager\u0026quot;\r}\rUpdate Resource Manager Updates a record of a Resource Manager within LM and re-executes the onboarding process. The ID in this request if the same as the name in the request body.\nWhen this request is made LM will contact the Resource Manager to see if any new Resource types have been added since the initial creation or last update. No pre-existing Resources will be overridden by this request. To change an existing Resource descriptor, the descriptor would have to be removed from LM and then this request fired.\nRequest    Aspect Value     Endpoint URL /api/resource-managers/{id}   Content-Type application/json   HTTP Method PUT    Path Parameters    Field Description Mandatory     id ID (name) of the Resource Manager to update Yes    Body    Field Description Mandatory     name The name the Resource Manager to be known by Yes   url The url where LM can find the Resource Manager interface Yes    Example:\n{\r\u0026quot;name\u0026quot;: \u0026quot;test\u0026quot;,\r\u0026quot;url\u0026quot;: \u0026quot;http://localhost:8295/api/resource-manager\u0026quot;\r}\rResponse    Aspect Value     Content-Type application/json   Response Code 200 (OK)    Body The body will include a report on the onboarding process, which specifies the Deployment Locations and Resource types discovered from the Resource Manager and the impact this had on LM.\n   Field Description     resourceManagerOperation The operation attempted on the Resource Manager (this will be UPDATE)   deploymentLocations List of reports for the DeploymentLocation onboarding operations attempted, indexed by name   resourceTypes List of reports for the ResourceType onboarding operations attempted, indexed by name    Each entry for deploymentLocations and resourceTypes will have the following fields:\n   Field Description     operation Operation attempted on the Deployment Location or Resource Descriptor (ADD/UPDATE)   success Boolean indicating if the operation was successful or not   reason A description of the operation\u0026rsquo;s result. Can be used for success or failure although the most frequent use is to explain why an operation failed   details Any additional details that should be known about the result of this operation    Remove Resource Manager Remove the record of a Resource Manager within LM. The ID in this request is the name that the Resource Manager instance is known by LM.\nRequest    Aspect Value     Endpoint URL /api/resource-managers/{id}   HTTP Method DELETE    Path Parameters    Field Description Mandatory     id ID (name) of the Resource Manager to update Yes    Response    Aspect Value     Response Code 204 (No Content)    "},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/best-practices/demos/introduction/","title":"Introduction","tags":[],"description":"","content":"Agile Lifecycle Manager (ALM) Demonstrations This section introduces a set of working ALM demonstrations. With working code and scripted scenarios to help you get up and running quickly.\nCI/CD Automated Pipeline  Description: Setup and use an automated CI/CD Pipeline to automate on-boarding, test, and deployment of an example Voice Service. Key points: Automated pipeline, automated behaviour driven testing, automated environment creation, automated binary package store Link: CI/CD pipeline  Coming soon  Dummy EPC End to End VoIP Service NFVI Automation  "},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/reference/lm-api/api-definition/resource-manager/resource-packages/","title":"Resource Packages","tags":[],"description":"","content":"The following section details the APIs used to manage Resource packages intended for the Brent (carrier-grade Resource Manager included with LM).\nAssociated with each definition are details of the request parameters and responses. These include the name of each field plus a brief description and whether the field is mandatory. Whether a field is required or not is based on the context of the examples. The underlying API definition may mark a field as optional, but in some contexts, the fields must be supplied.\nFor a 40x, 50x or any other error response please see error response codes\nOnboard Resource Package Adds a new Resource package to Brent\nRequest    Aspect Value     Endpoint URL /api/resource-manager/resource-packages   Content-Type: multipart/form-data   HTTP Method POST    Form Data    Field Description Mandatory     file The Resource package file Yes    Response    Aspect Value     Content-Type application/json   Success Status Code 201    Headers    Field Description     location Endpoint to Resource package    Update Resource Package Update an existing Resource package with new content\nRequest    Aspect Value     Endpoint URL /api/resource-manager/resource-packages/{name}   Content-Type: multipart/form-data   HTTP Method PUT    Query Parameters    Field Description Mandatory     name Name of the existing Resource package Yes    Form Data    Field Description Mandatory     file The Resource package file Yes    Response    Aspect Value     Content-Type application/json   Success Status Code 200    Remove Resource Package Remove an existing Resource package\nRequest    Aspect Value     Endpoint URL /api/resource-manager/resource-packages/{name}   HTTP Method DELETE    Path Parameters:    Field Description Mandatory     name Name of the existing Resource package Yes    Response    Aspect Value     Content-Type application/json   Success Status Code 204    "},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/cicd/developing-a-project/update-package/","title":"Updating a package","tags":[],"description":"","content":"Objectives   Update a VNF or NS Project after Release\n  One a version of a project has been packaged and released it cannot be updated. A new version must be created and tested.\n  Pre-requisites   Existing package\n  New package\n  Update Project to next version In the case where you want to update the last version:\n Checkout the project on the develop branch Edit the projects descriptor (VNF or NS) to increment the version Edit the the Behaviour test template for the NS or VNF being tested to the new version make changes to your project for the update. Ideally add a test to upgrade from the old to the new version Check the tests all work in your dev env Commit all changes on develop branch merge into master and push to origin - this will trigger the CI job on the hub.  Update/Patch a previous version of the project In this case you want to update an existing version, which may not be the latest version (e.g you have released 1.0 and 2.0, but now you find you must fix 1.0, for this you create a 1.0.1 version)\n Checkout the tag for the version you must update and create a branch \u0026lsquo;fix-[tag]\u0026rsquo; (don\u0026rsquo;t give the branch the exact same name as the tag. Edit the version in the descriptor and tests. make the changes Check the tests all work in your dev env In Jenkins add this branch to to the integration pipeline: In \u0026lsquo;Branches to Build, add branch and enter \u0026ldquo;*/fix-1.0.1\u0026rdquo; (or whatever you named your branch) Save the changes Push your branch to gogs (e.g. git push origin master fix-1.0.1) this should now trigger the release pipeline  Now test and release this version of the package.\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/best-practices/demos/cicd-pipeline/","title":"CI/CD Pipeline demonstration","tags":[],"description":"","content":"Objectives This working demonstration showcases Agile Lifecycle Manager (ALM) automated CI/CD capabilities. The demo will show the following\n Automated triggering of Network Service or VNF build from a Git project source change. Creation of a pre-production environment and all VNF or test dependencies required to run the updated Network Service. Automated behaviour testing of the updated Network Service On success the network service binary package is built with an appropriate version number and stored in Nexus  Pre-requisites  Have read the CI/CD Hub Introduction Working CI/CD Hub and ALM environment Install local LMCTL Working Docker Environment to run VNFs  Installing the Demo   Clone the Agile Lifecycle Manager (ALM) demo artifacts from GitHub to your local machine\n  On your local machine run the following commands:\n  mkdir cicd-demo\rcd cicd-demo\rgit clone https://github.com/accanto-systems/marketplace\rCopy the following script to your local machine and run it in the cicd-demo directory. This will load all required demo packages into nexus to prepare your environment to run the rest of the demonstration. Make sure to change \u0026laquo;YOUR NEXUS ADDRESS\u0026raquo; and user/password credentials in the script to point to your CI/CD Hub environment.  #!/usr/bin/env bash\rNEXUS_URL=\u0026quot;http://\u0026lt;\u0026lt;YOUR NEXUS ADDRESS\u0026gt;\u0026gt;:8002/repository/raw/packages\u0026quot;\rNEXUS_CREDENTIALS=\u0026quot;\u0026lt;\u0026lt;USER:PASSWORD\u0026gt;\u0026gt;\u0026quot;\rTEST_ENV=test\rfunction build {\rname=${1}\rversion=${2}\rtype=${3}\rcd ./marketplace/${type}/${name}\rlmctl project build\rcp ./_lmctl/_build/${name}-${version}.tgz ../../../packages\rcd ../../..\r}\rfunction upload {\rname=${1}\rversion=${2}\rpackage=${name}-${version}.tgz\recho uploading ${package}-${version} package to nexus and push to ALM environment named ${TEST_ENV}\rcurl -v -u ${NEXUS_CREDENTIALS} --upload-file ./packages/${package} ${NEXUS_URL}/${name}/${package}\r}\rfunction set_up {\rbuild $1 $2 $3\rupload $1 $2\r}\rfunction create_ns_project {\rsource=\u0026quot;./marketplace/network-services/voice-service\u0026quot;\rtarget=\u0026quot;./voice-service\u0026quot;\rmkdir -p ./packages\rmkdir -p ${target}\rcp -r ${source}/Behaviour ${target}\rcp -r ${source}/Descriptor ${target}\rcp ${source}/lmproject.yml ${target}\rcp ${source}/.gitignore ${target}\r}\rcreate_ns_project\rset_up chaos-monkey 1.0 vnfs\rset_up network 1.0 vnfs\rset_up ip-pbx 1.0 vnfs\rset_up sip-performance 1.0 vnfs\rset_up sip-traffic-manager 1.0 vnfs\rset_up voice-load-generator 1.0 network-services\rset_up voice-overlay-networks 1.0 network-services\rset_up voip-gateway 1.0 vnfs\rRun the script in your cicd-demo directory:   chmod +x ./setup_demo.sh\r./setup_demo.sh\rThe script will create a voice-service directory which is the start of the network service the demonstration is based upon. Run the following commands to create a new local git project.   cd ./voice-service\rgit init\rgit add .\rgit commit -m 'initial project'\r Create a new project for this voice service on the CI/CD Hub Git Server (Gogs), by performing the following tasks\n Go to Gogs UI log in as an admin user create \u0026ldquo;+ New Repository\u0026rdquo; named the same as your local project (e.g. \u0026ldquo;voice-service\u0026rdquo;) select the organization (for this demo, you should use: \u0026lsquo;marketplace\u0026rsquo;) Note the commands that Gogs gives you for \u0026lsquo;Push an existing repository from the command line\u0026rsquo; and use them in the next step. Push project to Gogs from your local machine:     git remote add origin http://\u0026lt;YOUR_GOGS_ADDRESS\u0026gt;:8001/marketplace/voice-service.git\rgit push -u origin master\rCreate a git develop branch   git checkout -b develop\r On Gogs UI, go to the project, select settings and collaboration \u0026lsquo;Add New Collaborator\u0026rsquo; for the Jenkins user and any user you wish to be able the make changes.   Set up the CI Pipeline for Jenkins by following the create CI pipeline instructions. The pipeline uses dependency files to determine what packages are used by the project. For this demo, create the following test.deps and release.deps file in your ./voice-service project directory:\n ./voice-service/test.deps:    network 1.0\rip-pbx 1.0\rsip-performance 1.0\rsip-traffic-manager 1.0\rvoice-overlay-networks 1.0\rvoice-load-generator 1.0\rvoip-gateway 1.0\r ./voice-service/release.deps:  network 1.0\rip-pbx 1.0\rvoice-overlay-networks 1.0\rvoip-gateway 1.0\r Set up Release and Deployment Pipeline in Jenkins:\n  Follow the create Release and deployment pipeline instructions to create a pipeline that will make the voice-service project a released package that can be used in production.\n  Run Demo Scenario You should now be ready to run the demo. Installing the demo has:\n Created a network service project called \u0026lsquo;voice-service\u0026rsquo; in git on a branch called \u0026lsquo;develop\u0026rsquo; Uploaded VNF and NS packages used by \u0026lsquo;voice-service\u0026rsquo; to nexus Created a Jenkins pipeline job to build, test and package \u0026lsquo;voice-service\u0026rsquo; when there there is a git push to the \u0026lsquo;master\u0026rsquo; branch. Created a Jenkins pipeline job to build a release package for \u0026lsquo;voice-service\u0026rsquo; and push to production environments  Demo Steps Look at environment before CI pipeline triggered  Log into the pre-production/test ALM UI and see there are no designs, ALM is \u0026lsquo;clean\u0026rsquo; and ready to test Log into Nexus and browse to raw/packages. You will see the dependent VNF and Network Service packages, but there is no \u0026lsquo;voice-service\u0026rsquo; package. Log into Gogs and browse to marketplace/voice-service. Note there are no \u0026lsquo;releases\u0026rsquo; yet.  Trigger the CI Pipeline  Make a change to your voice service project. Open ./voice-service/Descriptor/assembly.yml in an editor and change the second line \u0026lsquo;description: Voice Service\u0026rsquo; to \u0026lsquo;description: Scalable Voice Service\u0026rsquo; Save the change and commit change to git:  git checkout develop\rgit add .\rgit commit -m 'updated voice service description'\rYou have now made changes, tested them locally in your dev environment. Now you want submit your changes so others can use or test \u0026lsquo;voice-service\u0026rsquo;. You need to merge on the master branch:  git checkout master\rgit merge --no-ff develop -m 'validate version'\rgit push\r Monitor pipeline automation\n Log into Jenkins dashboard, you will see the voice-service job has been triggered. Click on the job and you will see the process stepping through the stages. Log into ALM UI and see the designs appear, \u0026lsquo;including assembly::voice-service::1.0\u0026rsquo; When the pipeline gets to the testing step in Jenkins, go back to ALM\u0026rsquo;s UI and open the assembly::voice-service::1.0 design, then the \u0026lsquo;Behaviour Testing\u0026rsquo; tab at the bottom of the screen. Open the \u0026lsquo;Results\u0026rsquo; and you will see the testing in progress. When the test completes, the pipeline will build the binary package for \u0026lsquo;voice-service\u0026rsquo; and upload it to nexus. On the nexus UI, look in raw/packages/voice-service. You will the voice-service-1.0-SNAPSHOT.tgz. In Gogs browse to the project and you will see there is now a \u0026lsquo;1.0\u0026rsquo; release.    Clean up after demo (ready to run demo again)\n On ALM UI, delete all assemblies from the Assembly Designer. Log into Nexus as admin and delete raw/packages/voice-service    "},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/reference/lm-api/scenarios/","title":"Sequence","tags":[],"description":"","content":"Resource Manager Handling Resource Manager Onboarding When a resource manager is onboarded, Agile Lifecycle Manager (ALM) invokes a set of calls to the Resource Manager detailed in the Resource Manager API document.\nDelete Resource Manager Get Resource Manager Details Update Resource Manager The resource manager returns the details of the resources and locations, and ALM will store any new details. ALM will not remove any existing details.\nAssembly Creation and State Transition Healing Scaling Scale Out ​ ScaleIn ​ Topology Requests "},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/reference/lm-api/api-definition/","title":"API Definitions","tags":[],"description":"","content":""},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/_footer/","title":"","tags":[],"description":"","content":""},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/_header/","title":"","tags":[],"description":"","content":""},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/reference/lm-api/api-definition/behavior-testing-api/assembly-configuration/","title":"Assembly Configuration","tags":[],"description":"","content":"The following details the API to manage Assembly Configurations within LM.\nAssociated with each definition are details of the request parameters and responses. These include the name of each field plus a brief description and whether the field is mandatory. Whether a field is required or not is based on the context of the examples. The underlying API definition may mark a field as optional, but in some contexts, the fields must be supplied.\nFor a 40x, 50x or any other error response please see error response codes\nCreate Assembly Configuration Create a new Assembly Configuration\nRequest    Aspect Value     Endpoint URL /api/behaviour/assemblyConfigurations   Content-Type application/json   HTTP Method POST    Body    Field Description Mandatory     name Name of the Assembly Configuration Yes   description Supplied description of the Assembly Configuration No   projectId Unique identifier for the behaviour test Project this Assembly Configuration belongs to Yes   descriptorName Name of the Assembly Descriptor to be instantiated in this configuration Yes   properties Name-value pairs, each defining a property value to use when instantiating an Assembly from this configuration No    Example:\n{\r​ \u0026quot;name\u0026quot;: \u0026quot;test\u0026quot;,\r​ \u0026quot;projectId\u0026quot;: \u0026quot;assembly::Test::1.0\u0026quot;,\r​ \u0026quot;description\u0026quot;: \u0026quot;description\u0026quot;,\r​ \u0026quot;descriptorName\u0026quot;: \u0026quot;assembly::Test::1.0\u0026quot;,\r\u0026quot;properties\u0026quot;: {\r​ \u0026quot;additionalProp1\u0026quot;: \u0026quot;string\u0026quot;,\r​ \u0026quot;additionalProp2\u0026quot;: \u0026quot;string\u0026quot;,\r​ \u0026quot;additionalProp3\u0026quot;: \u0026quot;string\u0026quot;\r​ }\r}\rResponse    Aspect Value     Content-Type application/json   Response Code 201 (Created)    Headers    Field Description     location Endpoint to created resource    Body    Field Description     id Unique identifier of the Assembly Configuration   name Name of the Assembly Configuration   description Supplied description of the Assembly Configuration   projectId Unique identifier for the behaviour test Project this Assembly Configuration belongs to   descriptorName Name of the Assembly Descriptor to be instantiated in this configuration   createdAt Date and time the Assembly Configuration instance was created   lastModifiedAt Date and time the Assembly Configuration instance was last modified   properties Name-value pairs, each defining a property value to use when instantiating an Assembly from this configuration    Example:\n{\r​ \u0026quot;id\u0026quot;: \u0026quot;8e266bc5-e613-4b0d-9ae0-50db6454b026\u0026quot;,\r​ \u0026quot;name\u0026quot;: \u0026quot;test\u0026quot;,\r​ \u0026quot;projectId\u0026quot;: \u0026quot;assembly::Test::1.0\u0026quot;,\r​ \u0026quot;description\u0026quot;: \u0026quot;description\u0026quot;,\r​ \u0026quot;descriptorName\u0026quot;: \u0026quot;assembly::Test::1.0\u0026quot;,\r​ \u0026quot;createdAt\u0026quot;: \u0026quot;2019-03-01T10:07:21.289Z\u0026quot;,\r​ \u0026quot;lastModifiedAt\u0026quot;: \u0026quot;2019-03-01T10:07:21.289Z\u0026quot;,\r\u0026quot;properties\u0026quot;: {\r​ \u0026quot;additionalProp1\u0026quot;: \u0026quot;string\u0026quot;,\r​ \u0026quot;additionalProp2\u0026quot;: \u0026quot;string\u0026quot;,\r​ \u0026quot;additionalProp3\u0026quot;: \u0026quot;string\u0026quot;\r​ }\r}\rUpdate Assembly Configuration Update an Assembly Configuration details\nRequest    Aspect Value     Endpoint URL /api/behaviour/assemblyConfigurations/{assemblyConfigurationId}   HTTP Method PUT    Path Parameters    Field Description Mandatory     assemblyConfigurationId ID of the Assembly Configuration Yes    Body    Field Description     name Name of the Assembly Configuration   description Supplied description of the Assembly Configuration   projectId Unique identifier for the behaviour test Project this Assembly Configuration belongs to   descriptorName Name of the Assembly Descriptor to be instantiated in this configuration   properties Name-value pairs, each defining a property value to use when instantiating an Assembly from this configuration    Example:\n{\r​ \u0026quot;name\u0026quot;: \u0026quot;test\u0026quot;,\r​ \u0026quot;projectId\u0026quot;: \u0026quot;assembly::Test::1.0\u0026quot;,\r​ \u0026quot;description\u0026quot;: \u0026quot;description\u0026quot;,\r​ \u0026quot;descriptorName\u0026quot;: \u0026quot;assembly::Test::1.0\u0026quot;,\r\u0026quot;properties\u0026quot;: {\r​ \u0026quot;additionalProp1\u0026quot;: \u0026quot;string\u0026quot;,\r​ \u0026quot;additionalProp2\u0026quot;: \u0026quot;string\u0026quot;,\r​ \u0026quot;additionalProp3\u0026quot;: \u0026quot;string\u0026quot;\r​ }\r}\rResponse    Aspect Value     Content-Type application/json   Response Code 200 (OK)    Body    Field Description     id Unique identifier of the Assembly Configuration   name Name of the Assembly Configuration   description Supplied description of the Assembly Configuration   projectId Unique identifier for the behaviour test Project this Assembly Configuration belongs to   descriptorName Name of the Assembly Descriptor to be instantiated in this configuration   createdAt Date and time the Assembly Configuration instance was created   lastModifiedAt Date and time the Assembly Configuration instance was last modified   properties Name-value pairs, each defining a property value to use when instantiating an Assembly from this configuration    Example:\n{\r​ \u0026quot;id\u0026quot;: \u0026quot;8e266bc5-e613-4b0d-9ae0-50db6454b026\u0026quot;,\r​ \u0026quot;name\u0026quot;: \u0026quot;test\u0026quot;,\r​ \u0026quot;projectId\u0026quot;: \u0026quot;assembly::Test::1.0\u0026quot;,\r​ \u0026quot;description\u0026quot;: \u0026quot;description\u0026quot;,\r​ \u0026quot;descriptorName\u0026quot;: \u0026quot;assembly::Test::1.0\u0026quot;,\r​ \u0026quot;createdAt\u0026quot;: \u0026quot;2019-03-01T10:07:21.289Z\u0026quot;,\r​ \u0026quot;lastModifiedAt\u0026quot;: \u0026quot;2019-03-01T10:07:21.289Z\u0026quot;,\r\u0026quot;properties\u0026quot;: {\r​ \u0026quot;additionalProp1\u0026quot;: \u0026quot;string\u0026quot;,\r​ \u0026quot;additionalProp2\u0026quot;: \u0026quot;string\u0026quot;,\r​ \u0026quot;additionalProp3\u0026quot;: \u0026quot;string\u0026quot;\r​ }\r}\rRemove Assembly Configuration Remove a Assembly Configuration\nRequest    Aspect Value     Endpoint URL /api/behaviour/assemblyConfigurations/{assemblyConfigurationId}   HTTP Method DELETE    Path Parameters    Field Description Mandatory     assemblyConfigurationId ID of the Assembly Configuration Yes    Response    Aspect Value     Content-Type application/json   Response Code 204 (No Content)    Get all Assembly Configurations Retrieve all Assembly Configurations in a behaviour testing Project (descriptor)\nRequest    Aspect Value     Endpoint URL /api/behaviour/assemblyConfigurations?projectId={projectId}   HTTP Method GET    Query Parameters    Field Description Mandatory     projectId ID of the behaviour testing project (usually the descriptor name) Yes    Response    Aspect Value     Content-Type application/json   Response Code 200 (OK)    Body The body will contain a single list of Assembly Configurations. Each configuration will have the following fields:\n   Field Description     id Unique identifier of the Assembly Configuration   name Name of the Assembly Configuration   description Supplied description of the Assembly Configuration   projectId Unique identifier for the behaviour test Project this Assembly Configuration belongs to   descriptorName Name of the Assembly Descriptor to be instantiated in this configuration   createdAt Date and time the Assembly Configuration instance was created   lastModifiedAt Date and time the Assembly Configuration instance was last modified   properties Name-value pairs, each defining a property value to use when instantiating an Assembly from this configuration    Example:\n[\r{\r​ \u0026quot;id\u0026quot;: \u0026quot;8e266bc5-e613-4b0d-9ae0-50db6454b026\u0026quot;,\r​ \u0026quot;name\u0026quot;: \u0026quot;test\u0026quot;,\r​ \u0026quot;projectId\u0026quot;: \u0026quot;assembly::Test::1.0\u0026quot;,\r​ \u0026quot;description\u0026quot;: \u0026quot;description\u0026quot;,\r​ \u0026quot;descriptorName\u0026quot;: \u0026quot;assembly::Test::1.0\u0026quot;,\r​ \u0026quot;createdAt\u0026quot;: \u0026quot;2019-03-01T10:07:21.289Z\u0026quot;,\r​ \u0026quot;lastModifiedAt\u0026quot;: \u0026quot;2019-03-01T10:07:21.289Z\u0026quot;,\r\u0026quot;properties\u0026quot;: {\r​ \u0026quot;additionalProp1\u0026quot;: \u0026quot;string\u0026quot;,\r​ \u0026quot;additionalProp2\u0026quot;: \u0026quot;string\u0026quot;,\r​ \u0026quot;additionalProp3\u0026quot;: \u0026quot;string\u0026quot;\r​ }\r}\r]\rGet Assembly Configuration Retrieve a single Assembly Configuration by ID\nRequest    Aspect Value     Endpoint URL /api/behaviour/assemblyConfigurations/{assemblyConfigurationId}   HTTP Method GET    Path Parameters    Field Description Mandatory     assemblyConfigurationId ID of the Assembly Configuration Yes    Response    Aspect Value     Content-Type application/json   Response Code 200 (OK)    Body    Field Description     id Unique identifier of the Assembly Configuration   name Name of the Assembly Configuration   description Supplied description of the Assembly Configuration   projectId Unique identifier for the behaviour test Project this Assembly Configuration belongs to   descriptorName Name of the Assembly Descriptor to be instantiated in this configuration   createdAt Date and time the Assembly Configuration instance was created   lastModifiedAt Date and time the Assembly Configuration instance was last modified   properties Name-value pairs, each defining a property value to use when instantiating an Assembly from this configuration    Example:\n{\r​ \u0026quot;id\u0026quot;: \u0026quot;8e266bc5-e613-4b0d-9ae0-50db6454b026\u0026quot;,\r​ \u0026quot;name\u0026quot;: \u0026quot;test\u0026quot;,\r​ \u0026quot;projectId\u0026quot;: \u0026quot;assembly::Test::1.0\u0026quot;,\r​ \u0026quot;description\u0026quot;: \u0026quot;description\u0026quot;,\r​ \u0026quot;descriptorName\u0026quot;: \u0026quot;assembly::Test::1.0\u0026quot;,\r​ \u0026quot;createdAt\u0026quot;: \u0026quot;2019-03-01T10:07:21.289Z\u0026quot;,\r​ \u0026quot;lastModifiedAt\u0026quot;: \u0026quot;2019-03-01T10:07:21.289Z\u0026quot;,\r\u0026quot;properties\u0026quot;: {\r​ \u0026quot;additionalProp1\u0026quot;: \u0026quot;string\u0026quot;,\r​ \u0026quot;additionalProp2\u0026quot;: \u0026quot;string\u0026quot;,\r​ \u0026quot;additionalProp3\u0026quot;: \u0026quot;string\u0026quot;\r​ }\r}\r"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/reference/lm-api/api-definition/topology/managing-assemblies/","title":"Assembly Intents","tags":[],"description":"","content":"This section covers the APIs used to manage Assembly instances during their life. It is based around the state model of Agile Lifecycle Manager (ALM)\nAssociated with each definition are details of the request parameters and responses. These include the name of each field plus a brief description and whether the field is mandatory. Whether a field is required or not is based on the context of the examples. The underlying API definition may mark a field as optional, but in some contexts, the fields must be supplied.\nFor a 40x, 50x or any other error response please see error response codes\nCreate Assembly Creates a new instance of an Assembly based on the given descriptor and the properties.\nRequest    Aspect Value     Endpoint URL /api/intent/createAssembly   Content-Type application/json   HTTP Method POST    Body    Field Description Mandatory     assemblyName A unique name by which this assembly will be known externally. This cannot contain spaces, consecutive underscores or start with a numeric character. Yes   descriptorName The descriptor name from which this assembly will be created Yes   intendedState The final intended state that the assembly should be brought into (Installed, Inactive, Active) Yes   properties An optional map of name and string value properties that is supplied to the new assembly No    Example:\n{\r\u0026quot;assemblyName\u0026quot;: \u0026quot;WED_102\u0026quot;,\r\u0026quot;descriptorName\u0026quot;: \u0026quot;assembly::t_single::1.0\u0026quot;,\r\u0026quot;intendedState\u0026quot;: \u0026quot;Active\u0026quot;,\r\u0026quot;properties\u0026quot;: {\r\u0026quot;data\u0026quot;: \u0026quot;exampleValue\u0026quot;,\r\u0026quot;deploymentLocation\u0026quot;: \u0026quot;admin@local\u0026quot;\r}\r}\rResponse    Aspect Value     Response Code 201 (Created)    Headers    Field Description     location Endpoint to process created to handle the intent    Change Assembly State Request LM to transition an Assembly instance from one state to another.\nRequest    Aspect Value     Endpoint URL /api/intent/changeAssemblyState   Content-Type application/json   HTTP Method POST    Body    Field Description Mandatory     assemblyName The name of the Assembly instance Yes (if assemblyId not supplied)   assemblyId The ID of the Assembly instance Yes (if assemblyName not supplied)   intendedState The state the Assembly instance will be transitioned to (Installed, Inactive, Active) Yes    Example of changing state of Assembly instance using name:\n{\r\u0026quot;assemblyName\u0026quot;: \u0026quot;WED_102\u0026quot;,\r\u0026quot;intendedState\u0026quot;: \u0026quot;Inactive\u0026quot;\r}\rExample of changing state of Assembly instance using id:\n{\r\u0026quot;assemblyId\u0026quot;: \u0026quot;1c3bd18a-05e9-4f49-b510-0e4785b2f0ae\u0026quot;,\r\u0026quot;intendedState\u0026quot;: \u0026quot;Inactive\u0026quot;\r}\rResponse    Aspect Value     Response Code 201 (Created)    Headers    Field Description     location Endpoint to process created to handle the intent    Delete Assembly Request LM to remove an Assembly instance\nRequest    Aspect Value     Endpoint URL /api/intent/deleteAssembly   Content-Type application/json   HTTP Method POST    Body    Field Description Mandatory     assemblyName The name of the Assembly instance Yes (if assemblyId not supplied)   assemblyId The ID of the Assembly instance Yes (if assemblyName not supplied)    Example of removing an Assembly instance using name:\n{\r\u0026quot;assemblyName\u0026quot;: \u0026quot;WED_102\u0026quot;\r}\rExample of removing an Assembly instance using id:\n{\r\u0026quot;assemblyId\u0026quot;: \u0026quot;1c3bd18a-05e9-4f49-b510-0e4785b2f0ae\u0026quot;\r}\rResponse    Aspect Value     Response Code 201 (Created)    Headers    Field Description     location Endpoint to process created to handle the intent    Heal Component Request a broken component of an Assembly be healed.\nRequest    Aspect Value     Endpoint URL /api/intent/healAssembly   Content-Type application/json   HTTP Method POST    Body    Field Description Mandatory     assemblyName The name of the Assembly instance Yes (if assemblyId not supplied)   assemblyId The ID of the Assembly instance Yes (if assemblyName not supplied)   brokenComponentName The unique name by which the broken component is known externally Yes (if brokenComponentId and brokenComponentMetricKey are not supplied)   brokenComponentId The unique internal id for the broken component Yes (if brokenComponentName and brokenComponentMetricKey are not supplied)   brokenComponentMetricKey The unique metric key by which the broken component is known externally Yes (if brokenComponentId and brokenComponentName are not supplied)    Examples:\nHeal assembly component using names:\n{\r\u0026quot;assemblyName\u0026quot;: \u0026quot;WED_102\u0026quot;,\r\u0026quot;brokenComponentName\u0026quot;: \u0026quot;WED_102__t_single\u0026quot;\r}\r{\r\u0026quot;assemblyId\u0026quot;: \u0026quot;5fd27c1e-403c-402b-a033-fef0940974d5\u0026quot;,\r\u0026quot;brokenComponentId\u0026quot;: \u0026quot;15a07604-377d-4fa2-955f-2a379560c24d\u0026quot;\r}\r{\r\u0026quot;assemblyName\u0026quot;: \u0026quot;WED_102\u0026quot;,\r\u0026quot;brokenComponentId\u0026quot;: \u0026quot;15a07604-377d-4fa2-955f-2a379560c24d\u0026quot;\r}\rResponse    Aspect Value     Response Code 201 (Created)    Headers    Field Description     location Endpoint to process created to handle the intent    Scale Out Component Request to scale out a cluster component of an Assembly instance.\nRequest    Aspect Value     Endpoint URL /api/intent/scaleOutAssembly   Content-Type application/json   HTTP Method POST    Body    Field Description Mandatory     assemblyName The name of the Assembly instance Yes (if assemblyId not supplied)   assemblyId The ID of the Assembly instance Yes (if assemblyName not supplied)   clusterName The name of the cluster to be scaled out Yes    Example:\n{\r\u0026quot;assemblyName\u0026quot;: \u0026quot;WED_102\u0026quot;,\r\u0026quot;clusterName\u0026quot;: \u0026quot;storage_cluster\u0026quot;\r}\rResponse    Aspect Value     Response Code 201 (Created)    Headers    Field Description     location Endpoint to process created to handle the intent    Scale In Component Request to scale in a cluster component of an Assembly instance.\nRequest    Aspect Value     Endpoint URL /api/intent/scaleInAssembly   Content-Type application/json   HTTP Method POST    Body    Field Description Mandatory     assemblyName The name of the Assembly instance Yes (if assemblyId not supplied)   assemblyId The ID of the Assembly instance Yes (if assemblyName not supplied)   clusterName The name of the cluster to be scaled in Yes    Example:\n{\r\u0026quot;assemblyName\u0026quot;: \u0026quot;WED_102\u0026quot;,\r\u0026quot;clusterName\u0026quot;: \u0026quot;storage_cluster\u0026quot;\r}\rResponse    Aspect Value     Response Code 201 (Created)    Headers    Field Description     location Endpoint to process created to handle the intent    Upgrade Assembly Make changes to the descriptor and/or property values of an Assembly instance. This may cause the state of the Assembly components to change while the upgrade is achieved.\nRequest    Aspect Value     Endpoint URL /api/intent/upgradeAssembly   Content-Type application/json   HTTP Method POST    Body    Field Description Mandatory     assemblyName The name of the Assembly instance Yes (if assemblyId not supplied)   assemblyId The ID of the Assembly instance Yes (if assemblyName not supplied)   descriptorName The optional descriptor name to which this assembly should be upgraded No   properties An optional map of name and string value properties that updated values for the assembly No    Example:\n{\r\u0026quot;assemblyName\u0026quot;: \u0026quot;WED_102\u0026quot;,\r\u0026quot;descriptorName\u0026quot;: \u0026quot;assembly::t_single::2.0\u0026quot;,\r\u0026quot;properties\u0026quot;: {\r\u0026quot;data\u0026quot;: \u0026quot;exampleValue\u0026quot;,\r\u0026quot;deploymentLocation\u0026quot;: \u0026quot;demo@local\u0026quot;\r}\r}\rResponse    Aspect Value     Response Code 201 (Created)    Headers    Field Description     location Endpoint to process created to handle the intent    "},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/reference/cicdhub-software/","title":"CI/CD Hub Software Overview","tags":[],"description":"","content":"CI/CD Hub The CI/CD Hub is a reference deployment of a best practice suite of Continuous Integration and Continuous Delivery (CI/CD) tools. These collectively providing an infrastructure for Assembly and Resource package and descriptor development, release and distribution management. It leverages common, open source, third party tools augmenting Agile Lifecycle Manager (ALM) specific tools to enable an integration with one or more ALM instances as part of a holistic CI/CD environment. This spans multiple environments such as Dev, Preprod/staging and Production but it is not limited to these.\nThe integration of ALM with a CI/CD process does such as CI/CD Hub neither productizes nor mandates the set of third party tools it deploys. Indeed the CI/CD Hub does not provide these tools, rather it simplifies there installation. The selection of these tools does not prohibit any or all of their functions being realized using a number of other commercial or open source tools. Nor does their inclusion constitute an endorsement.\nMaintenance of the environment post installation falls outside the scope of the CI/CD Hub, excluding the set of ALM specific tools (e.g LMCTL)\nAt the core of CI/CD process for ALM are 6 components:\n Source Version Control Server Artifact Repository Build Automation Server LDAP Authentication \u0026amp; Authorization Server ALM command line tools A local, designer/developer git repository  The CI/CD Hub realize these using the following;\n GOGs Nexus Jenkins OpenLDAP LMCTL  Collectively, these constitute a CI/CD environment. With the exception of LMCTL, the remainder are third part tools.\nAn additional requirement exists for Designers/developers to have local git which is installed on a Developers local machine. THis differs from the remainder which are all deployed in a CI/CD server. As such it falls outside the remit of the CI/CD Hub.\nAdministration \u0026amp; Installation Prerequisites The CI/CD Hub suite of tools are by default installed on a single server by the provided scripts. This should be provisioned with sufficient resources to fulfill the projected demand on the system. Of particular importance is the amount of storage\nFor the destination server, the server hosting the CI/CD Hub, the following system requirements exist\n  A suitable Ubuntu Linux server on which to install the CI/CD Hub\n Hardware  Storage  Min 2TB disk space ( Actual amount will be a function of the data that will be stored in in GOGs and Nexus)   Memory  Min 12GB RAM (a test install will work with as little as 4GB but it will not support many users)   Compute  Min 4 cores     OS  Ubuntu 16.0.4 or later is currently the only supported OS      Computer from which an install of the CI/CD Hub is being performed (the host machine)\n Hardware  No specific hardware requirement are identified. It is only used during the install and any reasonably spec   Software  Ansible Python sshpass (conditional)      Additional dependencies may also exist. The readMe.md and bhsaidfugidsfg of you specific CI/CD Hub installer should be consulted for fuller details.\nComponents The CI/CD Hub realized the core components with the following;\nOpenLDAP \u0026ndash; LDAP Authentication \u0026amp; Authorization Server An open source LDAP server. Each ALM instance needs to be configured with an LDAP server so that UI and API users can be authenticated and to identify the RBAC roles associated with that user. In an environment with multiple ALM deployments it is beneficial to have a single LDAP server where users are defined and their individual roles for each ALM environment are defined. This reduces the cost of administrating users.\nGOGs \u0026ndash; Source Version Control Server A lightweight self hosting Git service, source code repository equivalent to GitHub. Within the CI/CD Hub it is used as a common managed repository CI/CD source repository for Assembly and Resource projects. Common, refers to the fact that it is not associated with any one ALM instance, rather it is the . It is important to understand that this is not a persistence layer for ALM itself. Gogs is intended as the source of truth for packages and descriptor that can be shared from this central location across all ALM instances.\nNexus \u0026ndash; Artifact Repository An open source repository manager. Nexus is used to store static artifacts such as images for Resources and where binary Assembly Packages packages are stored for use in other projects or deployment into production environments.\nJenkins \u0026ndash; Build Automation Server An open source Automation/build server that automates the CI/CD pipeline tasks. You will create jobs for NS and VNF project in jenkins.\nLMCTL \u0026ndash; ALM command line tools A ALM specific tool used to integrate ALM with the CI/CD Hub. It is used to support both the movement of data both from the CI/CD Hub to ALM and vice a versa. Allowing the managed design and deployment of Assemblies, Resources to one of a number of registered ALMs.\nInstallation if the CI/CD Hub How to obtain the CI/CD Hub The CI/CD Hub is available on GitHub from where it can be downloaded. Once downloaded, it can be unbundled and run on a suitable environment.\nInstallation Instructions The CI/CD Hub installation is achieved by following the instructions in Install Instructions and using the CI/CD Hub download\nYou will need the url and admin credentials to log into each of the CI/CD Hub components:\n Git Server (Gogs) artifact Repo (Nexus) CI Server (jenkins) Dev Env (ALM and VIM)  Third-Party Tools As can be seen, the CI/CD Hub deploys a number of third party tools to construct a CI/CD environment. Each of these tools are owned and published by third party companies.\nLicencing Each of the third party tools deployed with the CI/CD Hub installer is subject to its own licencing. The versions installed by the CI/CD Hub installer are all open source and subject to one or more of the following licenses at the time of writing;\n GNU General Public License version 2.0 Apache License Version 2.0, MIT License sonotype oss licence  Support The CI/CD Hub does not productize these third party tools. Rather it simplifies there installation. The selection of these tools does not prohibit any or all of their functions being realized using a number of other commercial or open source tools. Nor does their inclusion constitute an endorsement.\nMaintenance The CI/CD Hub installer is provided to facilitate the a rapid and consistent deployment of a CI/CD environment.\nThe ALM team will, on a best effort basis maintain the CI/CD Hub on GitHub and we encourage the wider community to contribute to the project to both maintain and evolve the set of deployment scripts.\nBackup and restore of repository content Gogs, Nexus and the OpenLDAP instance have persistent store. These should be backed up in accordance with the backup policy of your organization. Each tool provides a mechanism for backing up their contents which is documented in the official documentation for the individual products. These should be consulted and incorporate into your restore procedures as appropriate.\n GOGs Nexus Jenkins OpenLDAP  Restore As for backup, the restore procedure for individual products within the CI/CD Hub suite of tools is documented by each tools official documentation. These should be consulted and incorporate into your restore procedures as appropriate.\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/reference/deployment-model/","title":"Deployment Model","tags":[],"description":"","content":"Introduction This section introduces the deployment architecture and main configuration options for Agile Lifecycle Manager (ALM) for a typical CI/CD configuration. It is expected that multiple ALMs will be deployed to automate the various tasks for each stage of the CI/CD process. Each set of LMs are coordinated around a repository of artifacts representing the external resources under management. The picture below shows a typical deployment. See here for more information on a typical LM CI/CD process.\nThe following logical functions are typically deployed together to deliver a complete CI/CD pipeline:\n CI/CD Hub: This provides a set of source code, image and package repositories required to develop and publish all resource and assembly artifacts. Combining the repositories with a CI server and LM command line tools automates the end to end pipeline. Development LM Environment: A lightweight flavour of LM can be deployed on a laptop or standalone server. This is designed to accelerate early resource onboarding and development. This flavour is not for production use. Test LM Environment: The default flavour of LM deploys a single set of micro service instances, typically used as a build slave or test environment. Production LM Environment: HA configuration of LM is provided to deploy multiple micro service instances high performance and availability scenarios. External LDAP: All LM instances can be configured to use the same LDAP server, whether it is the OpenLDAP instance provided with the CI/CD Hub an existing LDAP server.  Each LM environment will also require a working Resource Manager.\nDevelopment and Production Production LM environments must be installed to an existing kubernetes cluster using Helm. However Development LM environments can either be deployed to an existing kubernetes cluster or using an Allinone installer a fully working lightweight deployment can be deployed on a minimal linux server or virtual machine.\nThe Allinone LM deployments can be run on a standalone server or a laptop and can interact with a CI/CD Hub to do early resource development.\nDeploying across multiple clusters The LM environments required to deliver a complete CI/CD pipeline can be deployed to a single kubernetes cluster or individual LM environments can be deployed to separate kubernetes clusters.\nIf deploying more than one LM environment to the same kubernetes cluster, consideration must be take for namespace and port conflicts. Also it is recommended a reverse proxy or a domain name server is used in front of the kubernetes clusters to access each individual LM instance.\nCluster storage configuration For production deployments a kubernetes storage class can be provided or the default will be used. For Development Allinone deployments, no storage class is required. A development Allnone environment ties down the kubernetes version and common options to make the footprint as small as possible.\nHow to design your Lifecycle Manager Deployment When designing your deployment there are a number of things that need to be taken into consideration. Some will be specific to your environment and to corporate policies, as such they may not be covered. Others are more universal and we will consider them here.\nTypically an LM deployment will have the following environments:\n Design \u0026ndash; One or more design environments Preproduction/Staging \u0026ndash; One or more test environments Production \u0026ndash; The destination for released descriptors  Binding the individual LM Deployment Environments is the CI/CD Hub. This provides a common, secure, resilient and managed repository for all the artifacts of projects from which they can be deployed as required.\nA typical deployment will have all three environments;\nUse of namespaces Namespaces can be a useful tool to allow the use of a single cluster to be used to support the deployment of multiple LM environments while maintaining isolation. This can potentially allow\nIn some cases customers chose to have a single LM env on the production cluster to ensure full isolation and the provision of the maximum set of cluster resources to the production instance.\nLMCTL LMCTL is typically present on a Jenkins slave to aid the CI/CD environment. However in the case of a designer where they are working from a local version control repository (git) then the designer will likely have their own LMCTL installed on the local machine for deployment to their development/design environment. This removes the burden of maintaining the full set of environments within the CI/CD Hub which can be somewhat onerous when an ephemeral design env model is adopted.\nDesign When engineers are designing resource and assembly descriptors and packages they will need to have access to an LM instance where they can perform testing. For individual designers the LM environment will ideally need to be connected to a deployment location on which the descriptors can be tested.\nThe advantage of having multiple environments is that designers are able to debug assembly packages and perform destructive testing isolated from other users.\nIt is possible to spin up a design environment for specific cases and by connecting it to the CI/CD environment publish the results to make it available to other users. This has the advantage of allowing a user to have a repeatable, clean environment in which design and testing is performed. The cost of the LM design deployment is relatively low. The higher cost is likely to be in having an environment to which resources can be deployed during this phase.\nAt the higher end of cost is where each developer has their own deployment location. This provides the most isolation. However typically this is not required and individual developer/designer tenancies/projects in a shared VIM is sufficient allow developers to remain isolated.\nThe number of design environments required is a function of how many designers an organization has have and the nature of their work and is likely change over time potentially scaling down after the first phase of LM adoption.\nA design environment will typically be a small allinone deployment. This is particularly true if an organization chooses to provide designers\nIt is imperative that in cases where unique ephemeral environments are used for individual designers that a CI/CD Hub be deployed as a shared resilient repository across\nPre-production/ Staging The pre-production environment which can be used for a number of functions\n- Performance of consistent/certification testing as part of the CI/CD process\r- Production behaviour is reproduced\r- What if testing\r In the use cases for LM a mirror of production can be difficult to attain. While the deployment of LM itself can be mirrored without issue the number and type of deployment locations/VIMs can be a prohibitive and impractical cost in many of not most applications.\nA pre-production environment may be either a full replica of Production or a scaled down version. It will be fully functional but may for example omit High Availability and the capacity of a production environment.\nProduction Production is the final environment this should be commensurate with the projected near to medium timescale requirements of the specific deployment. The number of assembler\u0026rsquo;s and their complexity both in terms of the number of resources and ultimately the projected number of LM transactions it will perform will determine how large it should be.\nA production environment should leverage the High Availability (HA) in ALM. This will typically mean over provisioning the number of instances deployed for each ALM service.\nResource Managers The Resource Manager (RM) acts as an intermediary between LM and the individual VIMs supporting the infrastructure in which LM managed resources are deployed. A resource manager may only service a single ALM instance. That is; An RM should be deployed with access to the Kafka instance used by ALM and Elastic Search filebeat is configured to pull the set of logs for its LM environs. Therefore care must be taken to ensure valid pairing. Failure to do so can result in anomalous behaviour.\nAt least one RM must be deployed and onboarded/registered with LM for any assembly design or instantiation to function.\nSizing Guidelines Sizing Guidelines\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/reference/descriptor-specification/","title":"Descriptor Specification","tags":[],"description":"","content":""},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/reference/lm-api/api-definition/topology/catalog-api/","title":"Descriptors","tags":[],"description":"","content":"The following details the API to manage descriptors within LM. Assembly descriptors can be added, listed, updated and deleted. Resource descriptors can be listed and deleted but can only be added or updated through the Resource Manager API.\nAssociated with each definition are details of the request parameters and responses. These include the name of each field plus a brief description and whether the field is mandatory. Whether a field is required or not is based on the context of the examples. The underlying API definition may mark a field as optional, but in some contexts, the fields must be supplied.\nFor a 40x, 50x or any other error response please see error response codes\nCreate Assembly Descriptor Creates a new Assembly descriptor\nRequest    Aspect Value     Endpoint URL /api/catalog/descriptors   Content-Type: application/yaml   HTTP Method POST    Body The body content should be an Assembly descriptor in YAML format. See Assembly Descriptor YAML specification\nResponse    Aspect Value     Response Code 201 (Created)    Headers    Field Description     location Endpoint to descriptor resource    Body    Field Description     descriptorName The name of the descriptor   validationWarnings Warnings returned from the validation of the descriptor (if any)    Example:\n{\r\u0026quot;descriptorName\u0026quot;: \u0026quot;assembly::example::1.0\u0026quot;,\r\u0026quot;validationWarnings\u0026quot;: []\r}\rRemove Assembly Descriptor Removes an Assembly descriptor from LM.\nRequest    Aspect Value     Endpoint URL /api/catalog/descriptors/{descriptorName}   HTTP Method DELETE    Path Parameters    Field Description Mandatory     descriptorName name of the descriptor to remove Yes    The descriptor name is the full name of the descriptor, e.g. assembly::t_single::1.0. This will need to be encoded appropriately for use as a url – i.e. assembly%3A%3At_single%3A%3A1.0\nResponse    Aspect Value     Response Code 204 (No content)    Update Assembly Descriptor Updates an existing Assembly descriptor in LM.\nRequest    Aspect Value     Endpoint URL /api/catalog/descriptors/{descriptorName}   Content-Type: application/yaml   HTTP Method PUT    Path Parameters    Field Description Mandatory     descriptorName name of the descriptor to retrieve Yes    The descriptor name is the full name of the descriptor, e.g. assembly::t_single::1.0. This will need to be encoded appropriately for use as a url – i.e. assembly%3A%3At_single%3A%3A1.0\nBody The body content should be an Assembly descriptor in YAML format. See Assembly Descriptor YAML specification\nResponse    Aspect Value     Response Code 200 (OK)    Body    Field Description     descriptorName The name of the descriptor   validationWarnings Warnings returned from the validation of the descriptor (if any)    Example:\n{\r\u0026quot;descriptorName\u0026quot;: \u0026quot;assembly::example::1.0\u0026quot;,\r\u0026quot;validationWarnings\u0026quot;: []\r}\rGet a summary of all Descriptors This request returns a summary of the descriptors known to LM.\nRequest    Aspect Value     Endpoint URL /api/catalog/descriptors   HTTP Method GET    Response    Aspect Value     Response Code 200 (OK)    Body The body includes a single list of summaries, each with the following fields:\n   Field Description     name The name of the descriptor   description Description of the descriptor   links References to the descriptor resource endpoint    Example:\n[\r{\r​ \u0026quot;name\u0026quot;: \u0026quot;resource::t_simple::1.0\u0026quot;,\r​ \u0026quot;description\u0026quot;: \u0026quot;resource for t_simple\u0026quot;,\r​ \u0026quot;links\u0026quot;: [\r​ {\r​ \u0026quot;rel\u0026quot;: \u0026quot;self\u0026quot;,\r​ \u0026quot;href\u0026quot;: \u0026quot;http://192.168.99.100:8280/api/ /catalog/descriptors/resource::t_simple::1.0\u0026quot;\r​ }\r​ ]\r},\r{\r​ \u0026quot;name\u0026quot;: \u0026quot;resource::h_simple::1.0\u0026quot;,\r​ \u0026quot;description\u0026quot;: \u0026quot;resource for t_simple\u0026quot;,\r​ \u0026quot;links\u0026quot;: [\r​ {\r​ \u0026quot;rel\u0026quot;: \u0026quot;self\u0026quot;,\r​ \u0026quot;href\u0026quot;: \u0026quot;http://192.168.99.100:8280 /api/catalog/descriptors/resource::h_simple::1.0\u0026quot;\r​ }\r​ ]\r}\r]\rGet Descriptor by Name Returns an existing descriptor from LM.\nRequest    Aspect Value     Endpoint URL /api/catalog/descriptors/{descriptorName}   HTTP Method Get    Path Parameters    Field Description Mandatory     descriptorName name of the descriptor to retrieve Yes    The descriptor name is the full name of the descriptor, e.g. assembly::t_single::1.0. This will need to be encoded appropriately for use as a url – i.e. assembly%3A%3At_single%3A%3A1.0\nResponse    Aspect Value     Content-Type application/yaml   Response Code 200 (OK)    Body The body content will be a descriptor in YAML format. See Assembly Descriptor YAML specification\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/behaviour-testing/designing-scenarios/executing-intents/","title":"Executing Intents","tags":[],"description":"","content":"The following guide shows you how to use the Intent Engine and Intent Request steps to execute lifecycle intents on Assemblies as part of a scenario.\nExecuting Synchronous Intents The steps in the \u0026ldquo;Intent Engine\u0026rdquo; group allow you to execute intents on Assemblies and wait for their completion. The target Assembly of the intent can be one created as part of the scenario or an existing Assembly you know exists.\n  Open an existing scenario and add a step from the \u0026ldquo;Intent Engine\u0026rdquo; group. In the below example we have added the \u0026ldquo;Change State\u0026rdquo; step to change the state of our Assembly named \u0026ldquo;Example\u0026rdquo; to \u0026ldquo;Active\u0026rdquo;.\n  To execute the intent against an Assembly not included in the scenario, enter it\u0026rsquo;s name into the \u0026ldquo;assemblyName\u0026rdquo; property\n  Execute the scenario and monitor the chosen Assembly (in a new tab navigate to the \u0026ldquo;Recent Assembly Instances\u0026rdquo; menu on the left hand menu). As the change state step is executing you will see the change state execution take place.\n  The step will wait to see if the change state (or your chosen intent) process completes successfully.\nExecuting Asynchronous Intents The steps in the \u0026ldquo;Intent Requests\u0026rdquo; group allow you to request intents on Assemblies, then proceed to complete other steps whilst the intent process takes place.\n  Open an existing scenario and add a step from the \u0026ldquo;Intent Requests\u0026rdquo; group. In the below example we have added the \u0026ldquo;Change State\u0026rdquo; step to change the state of our Assembly named \u0026ldquo;Example\u0026rdquo; to \u0026ldquo;Active\u0026rdquo;. This will request the state change and progress without checking to see if the process is successful.\n  We can add another intent after the earlier step, so both intents are started and run in parallel. The second change state request in our example is for an Assembly not created in this scenario, we expect it to already exist.\n  To check the processes completed successfully, add the \u0026ldquo;Expect Intent Success\u0026rdquo; step from the \u0026ldquo;Intent Requests\u0026rdquo; group. This confirms the last requested intent for the given Assembly completed.\n  Execute the scenario and monitor the Assemblies (in a new tab navigate to the \u0026ldquo;Recent Assembly Instances\u0026rdquo; menu on the left hand menu). As the change state steps execute, a process will begin on each Assembly. The \u0026ldquo;Expect Intent Success\u0026rdquo; will block until the process completes successfully. If the process has failed, the step will be marked as failed and display the error in the execution screen.\n  "},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/reference/lm-api/api-definition/behavior-testing-api/executions/","title":"Executions","tags":[],"description":"","content":"This section details the APIs for executing Scenarios and viewing the results.\nAssociated with each definition are details of the request parameters and responses. These include the name of each field plus a brief description and whether the field is mandatory. Whether a field is required or not is based on the context of the examples. The underlying API definition may mark a field as optional, but in some contexts, the fields must be supplied.\nFor a 40x, 50x or any other error response please see error response codes\nExecute Scenario Execute a Scenario, creating an execution record to view the outcome.\nRequest    Aspect Value     Endpoint URL /api/behaviour/executions   Content-Type application/json   HTTP Method POST    Headers    Field Description Mandatory     Authorization Executing user\u0026rsquo;s authorization token Yes, if LM is secure otherwise; no    Body    Field Description Mandatory     scenarioId ID of the Scenario to execute Yes   assemblyId ID of the assembly instance, if the Scenario requires a reference to an existing Assembly Yes, if an Assembly actor in the Scenario is marked as \u0026ldquo;provided\u0026rdquo; otherwise; no. Only need one of assemblyId or assemblyName   assemblyName Name of the assembly instance, if the Scenario requires a reference to an existing Assembly Yes, if an Assembly actor in the Scenario is marked as \u0026ldquo;provided\u0026rdquo; otherwise; no. Only need one of assemblyId or assemblyName    Example:\n{\r​ \u0026quot;scenarioId\u0026quot;: \u0026quot;8e266bc5-e613-4b0d-9ae0-50db6454b026\u0026quot;\r}\rResponse    Aspect Value     Content-Type application/json   Response Code 201 (Created)    Headers    Field Description     location Endpoint to created resource    Remove Execution Removes record of an Execution. This does not cancel the Execution if it is running.\nRequest    Aspect Value     Endpoint URL /api/behaviour/executions/{executionId}   HTTP Method DELETE    Path Parameters    Field Description Mandatory     executionId ID of the Execution Yes    Response    Aspect Value     Content-Type application/json   Response Code 204 (No Content)    Get all Executions Retrieve all Executions by Scenario or Project (so all Scenarios in that Project)\nRequest    Aspect Value     Endpoint URL /api/behaviour/executions?scenarioId={scenarioId}\u0026amp;projectId={projectId}   HTTP Method GET    Query Parameters    Field Description Mandatory     scenarioId ID of the Scenario to retrieve all Executions of Yes (if projectId is not set)   projectId ID of the Project to retrieve all Executions from all Scenarios within it Yes (if scenarioId is not set)    Response    Aspect Value     Content-Type application/json   Response Code 200 (OK)    Body The body will contain a single list of Execution summaries. Each entry will have the following fields:\n   Field Description     id ID assigned to the Execution   name Name assigned to the Execution   scenarioId ID of the Scenario being executed   createdAt Date and time the Execution was requested   lastModifiedAt Date and time the Execution instance was last modified   startedAt Date and time the Execution begun   finishedAt Date and time the Execution completed (may be empty if the Execution is currently running)   status Current execution status (PASS, IN_PROGRESS, ABORTED, FAIL, PENDING)   scenarioSummary Summary of the Scenario being executed    The scenarioSummary contains:\n   Field Description     name Name of the Scenario   description Description of the Scenario    Example:\n[\r{\r​ \u0026quot;id\u0026quot;: \u0026quot;7c132ad3-f126-2c1d-7fe1-42ac6234e145\u0026quot;,\r​ \u0026quot;name\u0026quot;: \u0026quot;EX-01-03-19-15-27-43\u0026quot;,\r​ \u0026quot;createdAt\u0026quot;: \u0026quot;2019-03-01T15:37:43.705Z\u0026quot;,\r​ \u0026quot;lastModifiedAt\u0026quot;: \u0026quot;2019-03-01T15:37:43.705Z\u0026quot;,\r​ \u0026quot;startedAt\u0026quot;: \u0026quot;2019-03-01T15:37:43.705Z\u0026quot;,\r​ \u0026quot;finishedAt\u0026quot;: \u0026quot;2019-03-01T15:37:43.705Z\u0026quot;,\r​ \u0026quot;scenarioId\u0026quot;: \u0026quot;8e266bc5-e613-4b0d-9ae0-50db6454b026\u0026quot;,\r​ \u0026quot;scenarioSummary\u0026quot;: {\r​ \u0026quot;description\u0026quot;: \u0026quot;Test scenario\u0026quot;,\r​ \u0026quot;name\u0026quot;: \u0026quot;Test Scenario\u0026quot;\r​ },\r​ \u0026quot;status\u0026quot;: \u0026quot;PASS\u0026quot;\r}\r]\rGet Execution Get the current status of an Execution\nRequest    Aspect Value     Endpoint URL /api/behaviour/executions/{executionId}   HTTP Method GET    Path Parameters    Field Description Mandatory     executionId ID of the Execution Yes    Response    Aspect Value     Content-Type application/json   Response Code 200 (OK)    Body    Field Description     id ID assigned to the Execution   name Name assigned to the Execution   scenarioId ID of the Scenario being executed   createdAt Date and time the Execution was requested   lastModifiedAt Date and time the Execution instance was last modified   startedAt Date and time the Execution started (may be empty if the Execution has not yet started)   finishedAt Date and time the Execution completed (may be empty if the Execution has not yet completed)   status Current execution status (PASS, IN_PROGRESS, ABORTED, FAIL, PENDING)   scenarioSummary Summary of the Scenario being executed   stageReports A list of reports for each Stage in the execution   registeredAssemblies Details any Assemblies created or referenced as part of the Execution   registeredMetrics List of IDs for any Metrics recorded during the Execution   error Details of an error if one has occurred    The scenarioSummary contains:\n   Field Description     name Name of the Scenario   description Description of the Scenario    Each stageReport entry contains:\n   Field Description     name Name of the Stage   steps List of reports for each step    Each stepReport entry contains:\n   Field Description     status Status of the step execution (PASS, IN_PROGRESS, ABORTED, FAIL, PENDING)   startTime Date and time the step execution started (may be empty if the step has not yet started)   endTime Date and time the step execution completed (may be empty if the step has not yet completed)   stepDisplayName Display name of the step being executed   stepType Type of step (PRECONDITION, ACTION, EXPECTATION)   slices List of sentences detailing the action the steps takes   error Message detailing an error if one has occurred    Each registeredAssemblies entry contains:\n   Field Description     assemblyName Name of the Assembly instance   assemblyId ID of the Assembly instance   assemblyDescriptorName Name of the descriptor for this Assembly instance   installed Boolean set to true if the Assembly was installed as part of this Scenario Execution   uninstalled Boolean set to true if the Assembly was uninstalled as part of this Scenario Execution    Example:\n{\r\u0026quot;id\u0026quot;: \u0026quot;7c132ad3-f126-2c1d-7fe1-42ac6234e145\u0026quot;,\r\u0026quot;name\u0026quot;: \u0026quot;EX-01-03-19-15-27-43\u0026quot;,\r\u0026quot;createdAt\u0026quot;: \u0026quot;2019-03-01T16:03:54.278Z\u0026quot;,\r\u0026quot;lastModifiedAt\u0026quot;: \u0026quot;2019-03-01T16:03:54.278Z\u0026quot;,\r\u0026quot;finishedAt\u0026quot;: \u0026quot;2019-03-01T16:03:54.278Z\u0026quot;,\r\u0026quot;startedAt\u0026quot;: \u0026quot;2019-03-01T16:03:54.278Z\u0026quot;,\r\u0026quot;status\u0026quot;: \u0026quot;PASS\u0026quot;\r\u0026quot;error\u0026quot;: null,\r\u0026quot;scenarioId\u0026quot;: \u0026quot;8e266bc5-e613-4b0d-9ae0-50db6454b026\u0026quot;,\r\u0026quot;scenarioSummary\u0026quot;: {\r​ \u0026quot;description\u0026quot;: \u0026quot;Test scenario\u0026quot;,\r​ \u0026quot;name\u0026quot;: \u0026quot;Test Scenario\u0026quot;\r},\r\u0026quot;registeredAssemblies\u0026quot;: [\r​ {\r​ \u0026quot;assemblyDescriptorName\u0026quot;: \u0026quot;TestInstance\u0026quot;,\r​ \u0026quot;assemblyId\u0026quot;: \u0026quot;7a243de5-a324-3c1a-8af1-40ac1256a136\u0026quot;,\r​ \u0026quot;assemblyName\u0026quot;: \u0026quot;TestInstance\u0026quot;,\r​ \u0026quot;installed\u0026quot;: true,\r​ \u0026quot;uninstalled\u0026quot;: true\r​ }\r],\r\u0026quot;registeredMetrics\u0026quot;: [\r\u0026quot;6f122fa4-a219-2a2f-7be5-81bd2346b010\u0026quot;\r],\r\u0026quot;stageReports\u0026quot;: [\r​ {\r​ \u0026quot;name\u0026quot;: \u0026quot;Test Stage\u0026quot;,\r​ \u0026quot;status\u0026quot;: \u0026quot;PASS\u0026quot;,\r​ \u0026quot;steps\u0026quot;: [\r​ {\r​ \u0026quot;endTime\u0026quot;: \u0026quot;2019-03-01T16:03:54.278Z\u0026quot;,\r​ \u0026quot;error\u0026quot;: \u0026quot;string\u0026quot;,\r​ \u0026quot;slices\u0026quot;: [\r​ \u0026quot;I change the state of an assembly called TestInstance to Inactive\u0026quot;\r​ ],\r​ \u0026quot;startTime\u0026quot;: \u0026quot;2019-03-01T16:03:54.278Z\u0026quot;,\r​ \u0026quot;status\u0026quot;: \u0026quot;PASS\u0026quot;,\r​ \u0026quot;stepDisplayName\u0026quot;: \u0026quot;Change Assembly State\u0026quot;,\r​ \u0026quot;stepType\u0026quot;: \u0026quot;ACTION\u0026quot;\r​ }\r​ ]\r​ }\r]\r}\rGet Execution Progress Summary Get a summarized version of the current status of an Execution\nRequest    Aspect Value     Endpoint URL /api/behaviour/executions/{executionId}/progress   HTTP Method GET    Path Parameters    Field Description Mandatory     executionId ID of the Execution Yes    Response    Aspect Value     Content-Type application/json   Response Code 200 (OK)    Body    Field Description     id ID assigned to the Execution   name Name assigned to the Execution   startedAt Date and time the Execution started (may be empty if the Execution has not yet started)   finishedAt Date and time the Execution completed (may be empty if the Execution has not yet completed)   status Current execution status (PASS, IN_PROGRESS, ABORTED, FAIL, PENDING)   stageReports A list of reports for each Stage in the execution   registeredAssemblies Details any Assemblies created or referenced as part of the Execution   registeredMetrics List of IDs for any Metrics recorded during the Execution   error Details of an error if one has occurred    Each stageReport entry contains:\n   Field Description     name Name of the Stage   steps List of reports for each step    Each stepReport entry contains:\n   Field Description     status Status of the step execution (PASS, IN_PROGRESS, ABORTED, FAIL, PENDING)   startTime Date and time the step execution started (may be empty if the step has not yet started)   endTime Date and time the step execution completed (may be empty if the step has not yet completed)   stepDisplayName Display name of the step being executed   stepType Type of step (PRECONDITION, ACTION, EXPECTATION)   slices List of sentences detailing the action the steps takes   error Message detailing an error if one has occurred    Each registeredAssemblies entry contains:\n   Field Description     assemblyName Name of the Assembly instance   assemblyId ID of the Assembly instance   assemblyDescriptorName Name of the descriptor for this Assembly instance   installed Boolean set to true if the Assembly was installed as part of this Scenario Execution   uninstalled Boolean set to true if the Assembly was uninstalled as part of this Scenario Execution    Get all Execution Metrics Get all of the Metrics recorded during an Execution of a Scenario. Metrics are only recorded if a step has been executed which starts recording.\nRequest    Aspect Value     Endpoint URL /api/behaviour/executions/{executionId}/metrics   HTTP Method GET    Path Parameters    Field Description Mandatory     executionId ID of the Execution Yes    Response    Aspect Value     Content-Type application/json   Response Code 200 (OK)    Body A list of a Metrics, each entry containing:\n   Field Description     id ID of the Metric (unique within the context of the Execution)   name Name of the Metric   executionId ID of the Execution this Metric belongs to   metricThresholds List of any thresholds set on this Metric. Only included if a step defining a Metric has been executed in a Scenario   data Key/value pairs of any values recorded, keyed by their timestamp    Example:\n[\r{\r​ \u0026quot;data\u0026quot;: {\r\u0026quot;1572260858613\u0026quot;: \u0026quot;5\u0026quot;,\r\u0026quot;1572260865985\u0026quot;: \u0026quot;7\u0026quot;\r​ },\r​ \u0026quot;executionId\u0026quot;: \u0026quot;7c132ad3-f126-2c1d-7fe1-42ac6234e145\u0026quot;,\r​ \u0026quot;id\u0026quot;: \u0026quot;6f122fa4-a219-2a2f-7be5-81bd2346b010\u0026quot;,\r​ \u0026quot;metricThresholds\u0026quot;: [\r​ {\r​ \u0026quot;endTime\u0026quot;: 1572260865985,\r​ \u0026quot;startTime\u0026quot;: 1572260858613,\r​ \u0026quot;type\u0026quot;: \u0026quot;less than\u0026quot;,\r​ \u0026quot;value\u0026quot;: 10\r​ }\r​ ],\r​ \u0026quot;name\u0026quot;: \u0026quot;Example Metric\u0026quot;\r}\r]\rGet Execution Metric Get a single Metrics recorded during an Execution of a Scenario. Metrics are only recorded if a step has been executed which starts recording.\nRequest    Aspect Value     Endpoint URL /api/behaviour/executions/{executionId}/metrics/{metricId}   HTTP Method GET    Path Parameters    Field Description Mandatory     executionId ID of the Execution Yes   metricId ID of the Metric Yes    Response    Aspect Value     Content-Type application/json   Response Code 200 (OK)    Body    Field Description     id ID of the Metric (unique within the context of the Execution)   name Name of the Metric   executionId ID of the Execution this Metric belongs to   metricThresholds List of any thresholds set on this Metric. Only included if a step defining a Metric has been executed in a Scenario   data Key/value pairs of any values recorded, keyed by their timestamp    Example:\n{\r​ \u0026quot;data\u0026quot;: {\r\u0026quot;1572260858613\u0026quot;: \u0026quot;5\u0026quot;,\r\u0026quot;1572260865985\u0026quot;: \u0026quot;7\u0026quot;\r​ },\r​ \u0026quot;executionId\u0026quot;: \u0026quot;7c132ad3-f126-2c1d-7fe1-42ac6234e145\u0026quot;,\r​ \u0026quot;id\u0026quot;: \u0026quot;6f122fa4-a219-2a2f-7be5-81bd2346b010\u0026quot;,\r​ \u0026quot;metricThresholds\u0026quot;: [\r​ {\r​ \u0026quot;endTime\u0026quot;: 1572260865985,\r​ \u0026quot;startTime\u0026quot;: 1572260858613,\r​ \u0026quot;type\u0026quot;: \u0026quot;less than\u0026quot;,\r​ \u0026quot;value\u0026quot;: 10\r​ }\r​ ],\r​ \u0026quot;name\u0026quot;: \u0026quot;Example Metric\u0026quot;\r}\rCancel Execution Cancel a currently running Execution. Any steps currently executing may still complete but future steps will be cancelled.\nRequest    Aspect Value     Endpoint URL /api/behaviour/executions/{executionId}/cancel   HTTP Method POST    Path Parameters    Field Description Mandatory     executionId ID of the Execution Yes    Response    Aspect Value     Content-Type application/json   Response Code 202 (Accepted)    Body    Field Description     success True, if the Execution was cancelled. Will return false only if the Execution had already completed, so could not be cancelled    "},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/behaviour-testing/designing-scenarios/existing-provided-assembly/","title":"Existing Provided Assembly","tags":[],"description":"","content":"The \u0026ldquo;Existing Provided Assembly\u0026rdquo; entry, found in the \u0026ldquo;Assembly Configurations\u0026rdquo; section of the scenario palette, is available to all scenarios. Adding it to the scenario will add an input property to the scenario, which must be set each time it is executed.\nThe following guide shows you to use the \u0026ldquo;Existing Provided Assembly\u0026rdquo; configuration in your scenarios.\nAdd Existing Provided Assembly   Add the \u0026ldquo;Existing Provided Assembly\u0026rdquo; entry by dragging it from the scenario palette into the \u0026ldquo;Assemblies\u0026rdquo; panel at the top of the scenario\n  Set the name of the input Assembly in the step. This name is of your choosing and is not supposed to match an existing Assembly. It is a reference to the Assembly which will be provided at runtime and used to refer to this Assembly throughout the scenario.\n  Execute Steps Against Existing Provided Assembly You may reference the existing provided assembly in any steps requiring the name of an Assembly.\n  Add a step which requires an Assembly name. Set the name to the same value as your \u0026ldquo;Existing Provided Assembly\u0026rdquo; entry\n  Execute Scenario When executing a scenario with an \u0026ldquo;Existing Provided Assembly\u0026rdquo; you will be prompted to provide the name of an existing Assembly, which will play the role of the provided Assembly. Any steps referring to the provided Assembly will execute against the Assembly given.\n  Execute the scenario by clicking the \u0026ldquo;Run Scenario\u0026rdquo; button at the top of the designer\n  A dialog box will appear requesting the name of an Assembly to use as the \u0026ldquo;Existing Provided Assembly\u0026rdquo;\n  Provide the name of an Assembly you want to execute this scenario with\n  Click \u0026ldquo;Execute\u0026rdquo; to be taken to the execution screen. You will see an additional step has been added to your scenario which identifies the Assembly you provided exists.\n  In our example, as the \u0026ldquo;Change State\u0026rdquo; step executes we will see the process is executed against the \u0026ldquo;ExternalExample\u0026rdquo; Assembly we provided.\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/reference/glossary/","title":"Glossary of Terms","tags":[],"description":"","content":"Agile Lifecycle Manager (ALM) uses a number of terms which have very specific meaning in its context. In some cases these terms are used in other products or industries to mean slightly different things. It is important that the user understand these terms in the context of ALM\nAssembly Assembly is a definition of a service and may comprise of one or more resources and/or other assemblies. It is defined in an Assembly Descriptor and can be instantiated as an Assembly Instance.\nAssembly Descriptor (Descriptor) Assembly descriptor is a computer readable definition of an Assembly implemented as a Yaml-file, see Descriptor for further details.\nAssembly Designer, Service Designer An Actor or end user role designing services using ALM. Assembly designer takes informal service design artifacts defined by service designers and translates them to a set of formal computer readable descriptors that model the target service\nAssembly Instance Instantiation of an Assembly Descriptor which is comprised of resource instances or assembly instances\nCapability Capabilities is a section of Assembly Descriptor or Resource Descriptor defining what functions the Resources or Assemblies are implementing.\nCatalog, Assembly Catalog The repository within ALM storing published Assembly Descriptors and Resources Descriptors.\nCloud The cloud is a common term referring to accessing computer, information technology (IT), and software applications through a network connection, often by accessing data centers using wide area networking (WAN) or Internet connectivity.\nCluster A cluster is a logical set of fungible resources or assemblies that can scaled up and down as required within the constraints of the scaling parameters for the cluster.\nEach cluster is defined with a single resource or assembly.\nComponent A component is a collective term for any one of a resource, assembly or a references\nCSAR, archive Cloud Service Archive (CSAR) describes a format used for describing Resource Packages. CSAR specification is a part of OASIS TOSCA.\nDeployment Location Deployment Location is a facility were Resources can be deployed when instantiated. In various contexts Deployment Locations are referred to as Data center, Project (Openstack), or Availability Zone (openstack).\nA deployment location is thus a logical separation of infrastructure that may refer to the infrastructure as a whole, particular tenants, regions or sub-divisions thereof\nResources are created within the context of a single deployment location\nDescriptor (Assembly / Resource) A descriptor is the definition of a resource or assembly, used by LM to instantiate instances as requested by the users (or remote systems). A descriptor describes the composition, properties, policies \u0026amp; metrics, relationships and behaviour of an assembly/resource\n Resource descriptors are owned and managed externally to LM and are typically onboarded through a resource manager Assembly descriptors can be viewed, edited and managed through the LM UI or the LM API  Descriptors have identifiers of the form type::name::version (for example: assembly::my_first_assembly::1.0), where\n Type is one of assembly or resource Name is the name for the descriptor Version is the semantic version of the descriptor (dot-separated)  Descriptors can be imported and exported from LM using the LMCTL command line tool and are represented as YAML documents\nThe syntax of the YAML documents can be found for resources and assemblies\nEuropean Telecommunications Standards Institute, ETSI Instance An instance is the realisation of a descriptor, as requested by a user (or remote system) Since assemblies can contain other assemblies as components, this hierarchy of instances is termed an assembly topology\nThe assembly that is at the top of the hierarchy is known as the root assembly instance The components contained within an assembly are often referred to as child components and the assembly they are in is referred to as the parent assembly\nThe descriptor that is specified when an assembly instance is created is referred to as the top-level assembly descriptor\nIntent A request to execute the most efficient steps to get a service into its intended state without the need for the manual programming of scripts or workflows.\nIntent Engine, Engine The entity responsible for generating the assembly deployment plan and instructing, step by step, Resource Managers to execute the plan.\nKafka Apache Kafka™ is a distributed streaming platform.\nLifecycle Event, Event ALM published intent and status change event onto a Kafka topic.\nLifecycle State, State ALM defines a simple but flexible lifecycle which is applicable to both Assemblies and Resources. This lifecycle consists of a number of states and on the transition between each state a Lifecycle Transition occurs.\nThe set of states are as follows. Stable indicates that the component (assembly/resource) can be elected to reside in this state. If a state is not stable then LM will auto-transition through this state and automatically promote the component to the next stable state.\nEach state transition on a resource can, optionally, be associated with a lifecycle transition action on the Resource Manager. That is, the resource manager is informed of the state transition (more correctly the intent to change state) so that it can take appropriate action.\n   State Stable Comment     null Y This state indicates that the component (assembly/resource) does not yet exist   Created N Created is a transitional state. That is, it is a temporary state which an component (assembly/resource) exists in between null and Installed. It is not possible to direct ALM to bring an component to this state.   Installed Y    Inactive Y    Active Y The state which indicates that the component (assembly/resource) is fully functioning and operational   Broken N This is a temporary state which a component (assembly/resource) will be brought to by the system if a heal is required.   Failed Y A state which only the system can transition a component (assembly/resource) to. It is a special state ALM will bring an component to if it detects that additional action by LM is futile. LM will take no active remedial action once a component has transitioned to this state without direction from an administrator.    Lifecycle Transition Each component (Assemblies \u0026amp; Resources) flow through a state model during its lifetime. Each state transition represents a Lifecycle Transition and the execution the opportunity to have a lifecycle transition script executed by the appropriate Resource-Manager for the given resource.\nBoth Assemblies and Resources undergo lifecycle transitions but only resources have lifecycle transitions scripts.\nThe set of transitions are fixed as are the set of states\n   Transition From State To State Post-transition Comment     Create null Created     Install Created Installed     Configure Installed Inactive     Start Inactive Active     Integrity Inactive Active Start The transition between Inactive and Active can have two scripts which are executed sequentially with Integrity only executed on successful completion of Start   Stop Active Inactive     Uninstall Inactive Created     Delete Created null     Reconfigure Active Active  Reconfigure a special case as it does not strictly speaking relate to a state transition. It is triggered as a result of changes to volatile properties    Lifecycle Transition Scripts Strictly speaking the role of Lifecycle Transition Scripts is outside the remit of ALM. They represent a set of scripts which are executed by the Resource Manager as a result of it receiving corresponding notifications of resource lifecycle state changes (intent). As ALM supports the binding of external Resource Managers the existence of Lifecycle Transition Scripts is not mandated but the flexibility they provide is the suggested method of realizing state transitions by a Resource Manager.\nFurther to this, with the productized Resource Manager shipped with ALM in v2.1 for the first time Lifecycle Transition scripts are prescribed for all transition bar null to Created where an associated action is performed by the Resource Manager. The associated Lifecycle Transition Script is provided to the Resource Driver to enact the transition. For null to Created transitions a Heat or TOSCA template can be provided.\nMicroservice Microservices is a variant of the service-oriented architecture (SOA) architectural style that structures an application as a collection of loosely coupled services. The benefit of decomposing an application into different smaller services is that it improves modularity and makes the application easier to understand, develop and test. It also parallelizes development by enabling small autonomous teams to develop, deploy and scale their respective services independently.\nMigration Migration is one of the Opinionated Patterns aiming to migrate a deployed VNF from one location to another.\nMonitoring Metrics Performance or health metrics published by Resource Managers and/or resources onto a Kafka topic. The Kafka cluster stores streams of records in categories called topics.\nNetwork A type of Resource\nNetwork Functions Virtualization, NFV Network functions virtualization (NFV) is an initiative to virtualise network services traditionally run on proprietary, dedicated hardware.\nOASIS OASIS is a non-profit consortium that drives the development, convergence and adoption of open standards for the global information society.\nOperation When a relationship is created or destroyed (ceased), ALM has the capacity to cause side-effects by executing one or more operations on the components on either end of the relationship (pre and post)\nWhen an operation is called on a resource manager, the set of properties of the relationship are passed as opposed to the properties of the resource on which it is executing.\nThe set of Operations defined in a relationship are run sequentially in the order they are defined for the relationship\nUltimately all Operations are defined within resource descriptors, but they can be promoted to their enclosing assemblies to make them available to relationships defined at higher-levels of the assembly hierarchy\nOperations is a section of a Assembly or Resource Descriptor defining sets of operations that can be called to enable relationships to be created between resources and/or assemblies.\nOperational UI ALM User Interface\nOpinionated patterns Group of lifecycle transitions to achieve a particular task. Examples of tasks include: heal, reconfigure, and upgrade.\nPolicy Policies is a section of Assembly Descriptor or Resource Descriptor containing the set of policies that are used to manage the assembly or resource instances\nPhysical Network Function, PNF A PNF or Physical Network Function references an instance of a network function which is realized by a physical resource such as a dedicated switch in the NFV architecture\nProperty Properties is a section of Assembly Descriptor or Resource Descriptor containing the properties that belong to the resource or assembly descriptors. These include the full set of properties that are required to orchestrate them through to the Active state. These can be understood as the context for the management of the item during its lifecycle.\nQuality Monitoring Quality Monitoring is a process to monitor the health of deployed Resources and NFV Infrastructure and to test, monitor and evaluate the end to end service performance.\nReference Relationships is a section of Assembly or Resource Descriptor identifying referenced components.\nIn addition to defining components managed by an assembly, an assembly can reference assemblies and resources that are managed independently. Thus a reference is a resource or assembly that has its lifecycle managed externally to the assembly in which the reference is defined though it is likely still managed by ALM.\nWhen defining a reference, we must specify the search criteria used to find this reference component at instantiation time.\n Assemblies are found by specifying their name, which will be searched within the current LM system Resources are found by specifying their name, deployment location and resource manager, which will be found by searching for the resource within the specified resource manager  Referenced components can be the source for a relationship (but cannot be a target)\nReferenced components can have their properties accessed for the purpose of passing to other components (or the enclosing assembly)\nRelationship A relationship is a directional link between two components within an assembly and it can be used to add orchestration dependencies between the components and/or add side-effects when they are created or ceased.\nIt is represented in the UI as an orange line between the two components\nWhen defining a relationship, we can configure the following aspects\n Source component for the relationship Target component for the relationship Properties Relationship properties can be fixed values, or data mapped from the source or target ends of the relationship or The keywords source and target are used in the property mapping syntax rather than the component names E.g. ${source.name} will get the name property from the source of the relationship. Operation A relationship has its own lifecycle which is governed by the state transitions of the source and target components. On any relationship lifecycle change the set of Operations defined for that lifecycle transition are triggered and each is supplied with the set of property values for the relationship .  Requirement Requirements is a section of Assembly Descriptor or Resource Descriptor explaining what functions the Resources or Assemblies need before they can work successfully.\nResource A piece of software that can be automatically deployed in a virtual environment and that supports key lifecycle states including install, configure, start, stop, and uninstall.\nResource are managed externally to LM using a Resource Manager and represents the smallest atomic building block available in LM\nA resource may for example represent a VNF, VNFC or the PNF in an NFV environment or any external entity depending on how the system is modeled\nResource Descriptor The list of resource attributes and properties written in Yaml, see Descriptor for further details.\nResource Health, Component Health Resource Health is a Microservice within ALM responsible for monitoring health related messages and initiating recovering actions related to deployed Resources. For instance, the Resource Health may send a heal message to the Intent Engine if a certain event indicating health issues is detected.\nResource Instance Represents the logical grouping of infrastructure being managed by an external resource manager\nResource Manager The entity instructing resources (e.g. Brent). ALM can have a number of bound Resource Managers but must have at least one to function\nThis system is responsible for transitioning resources through lifecycle states and performing operations on resources under the orchestration of ALM and returning the result to LM\nA resource is managed by one (and only one) resource manager\nResource Package Resource package is described as a CSAR archive. This is the bundle of everything needed for resource that is loaded into a Resource Manager.\nScale Scale is one of the Opinionated Patterns aiming to increase or decrease the amount of deployed Resources of a specific type.\nService Chain A series of actions that are applied to a data stream as it passes through a network device.\nTOSCA Topology and Orchestration Specification for Cloud Applications (TOSCA) is a standard defined by OASIS.\nTopology, Instance Inventory The repository storing key state information related to Assembly- and Resource Instances and topology of the Deployment Locations\nVirtual Infrastructure Manager The entity controlling the cloud infrastructure compute, storage and network resources (e.g. Openstack).\nVirtual Network Function, VNF Virtual Network Functions (VNFs) are virtualized tasks formerly carried out by proprietary, dedicated hardware. VNFs move individual network functions out of dedicated hardware devices into software that runs on commodity hardware.\nVirtual Network Function Component, VNFC Virtual Network Function Components (VNFs) are virtualized internal component of a VNF in the NFV architecture which provides a defined subfunction of a VNF. VNFCs map N:1 with their VNF and a single instance of a component maps 1:1 with a Virtualization Container.\nIn the ALM model a VNFC is realized as a Resource.\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/","title":"Home","tags":[],"description":"","content":"Welcome to the Learning Center This Learning Center Portal offers a user guide for the Agile Lifecycle Manager (ALM) product. It provides step by step introductions on the tools available for various user roles intending to develop, package, and operate VNFs and/or network services. These include administrators, VNF engineers, service designers or operations personnel. The purpose of this portal is to provide high-level instructions on how to perform the basic tasks with the ALM product.\nALM is a cloud networking automation platform that embraces the IT DevOps movement. ALM enables complex network services to be designed, created and continuously optimized across hybrid and distributed cloud environments. ALM\u0026rsquo;s unique Intent Engine delivers dramatic \u0026ldquo;lights out\u0026rdquo; operational automation, lowering the cost of managing edge cloud fabrics for Enterprise and 5G, whilst increasing the pace and possibilities for innovation.\n   Understanding The Basics Getting Started Reference Links     What is Agile Lifecycle Manager (ALM)? Install ALM Assembly Descriptor   What\u0026rsquo;s the business problem? How to Develop a Resource Resource Descriptor   Users How to Design a Network Service LMCTL Command line tool   Key concepts  LM API Interface   What is CI/CD Hub?  Resource Manager   Architecture         CI/CD Methodology Best Practices      Introduction Demos    CI/CD Hub Software Overview How-to-Guides    Getting Started     Create CI Pipeline     Create Release Pipeline     CI/CD Pipeline Demo      "},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/reference/icp-pre-install/","title":"ICP Advice","tags":[],"description":"","content":"ICP Pre-Install Considerations This section details topics that should be considered when installing the CI/CD Hub and/or Agile Lifecycle Manager (ALM) to ICP.\nHelm Install the Helm client using the ICP instructions.\nPod Security Policy If you are installing CI/CD Hub or Agile Lifecycle Manager (ALM) on ICP, you should do so in a namespace that is bound to a less restrictive Pod Security Policy. We recommend creating a new namespace and binding it to an existing policy called ibm-anyuid-psp. This can be done through the ICP dashboard, see ICP - Create a namespace with pod security policy binding for more details.\nDocker Images To pull docker images from Docker Hub and/or docker registries you may need to add an ImagePolicyto the namespace you plan to install in (or a ClusterImagePolicy to apply the rules across the cluster).\nThe policies can be added through the ICP dashboard, see ICP - Image Security for more details.\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/operations/identifying-issues/","title":"Identifying Issues","tags":[],"description":"","content":"View Log Files While Agile Lifecycle Manager (ALM) logs are generated on and can be viewed on individual pods (per service instances) using kubectl logs, in real terms this is impractical to do and gain a holistic view of system operation through the logs. Viewing logs from the collective logs in Elasticsearch using Kibana is both more effective and usable.\nWhen viewing logs it is important to first ensure that the log level has the level of detail you require. While that cannot be altered retrospectively, it is possible to increase the logging level on a running system or to set the desired log level as the default. Details of how to do this can be found here\nTracing Contexts and Transaction Ids All actions taken by ALM have an associated Tracing Context. This is a collection of Ids (min 1) which can be used to collate information in a log stream to the set of logs specific to the initiating event.\nA Tracing Context can be provided in any API call to the API gateway as a header. Likewise in any external API call, such as that to a Resource Manager is passed this tracing context. This allows end-to-end tracing both within the LM application and in those which it communicates.\nShould an application on ALM\u0026rsquo;s northbound interface not provide a tracing context then LM will create one on the API gateway so that it is available for the remainder of the transaction.\nWhen an external system provides a tracing context it must at a minimum provide a Transaction Id. This Id should be unique from the perspective of the LM system. It is incumbent on the calling system to ensure that this is the case. The calling system may not be the only API client and when selecting an Id, it is encouraged (but not enforced) to use a UUID/GUID if it is not possible to ensure the id is not unique. As this is a Transaction Id, it is not sufficient to use an assit id or a customer id as it is possible to have multiple transactions for each within a time window. The ALM UI generates its own Transaction Ids when communicating with the \u0026lsquo;core LM microservices\u0026rsquo;,\nAPI transaction id Header    Key Value     X-TraceCtx-TransactionId \u0026lt;any Unique Id\u0026gt;    Kibana offers such a feature, Opening any log of interest and selecting 'view surrounding documents' (logs) will have the desired effect.\nAn example of such a case is when an operation is being called on a referenced resource while a concurrent intent being run on that same resource. This has a relatively low probability of occurring, but it should not be exclude, viewing surrounding documents (logs) will help reveal this.\nFiltering to an Id Filtering to an Transaction id Id can be achieved in Kibana by setting an appropriate filter. The tracing context attributes (transactionId etc) are configured as filterable fields for logs. This allows the user to define a filter in Kibana by selecting Add Filter on the Discover page, selecting tracectx.transactionid and appropriate condition for the filter (is , is one of - when checking for multiple values ) and the\nFor example, the following filter will filter all logs containing the Transaction Id e5efa064-2043-44dc-95b8-7872c8fc7370\n{\r\u0026quot;query\u0026quot;: {\r\u0026quot;match\u0026quot;: {\r\u0026quot;tracectx.transactionid\u0026quot;: {\r\u0026quot;query\u0026quot;: \u0026quot;e5efa064-2043-44dc-95b8-7872c8fc7370\u0026quot;,\r\u0026quot;type\u0026quot;: \u0026quot;phrase\u0026quot;\r}\r}\r}\r}\rThe transaction logging on any ALM system can be found with an index pattern of lm-logs-6*\nDiscovering the transaction id If an error has been generated on the system as teh result of an API call then the transaction id will be in the response header. And if by the front end then the context id will be displayed in the error dialogue.\nHowever if it is not possible to provide the transaction id when debugging then secondary information such as\n Assembly Name Time window Deployment Location etc  restricting logs to a subset of ALM Microservices Particularly if it is not possible to identify a Transaction Id on which to establish a context it can be beneficial in some cases to restrict the set of log displayed to a subset (one or more) of the ALM Microservices. This can be done by applying a filter based on kubernetes.labels.app this can be set to match one or more of the ALM microservices or to a Resource Manager instance for example.\nFor example to restrict the displayed logs to only those generated by the ALM Intent Engine the following Kibana filter can be applied\n{\r\u0026quot;query\u0026quot;: {\r\u0026quot;match\u0026quot;: {\r\u0026quot;kubernetes.labels.app\u0026quot;: {\r\u0026quot;query\u0026quot;: \u0026quot;daytona\u0026quot;,\r\u0026quot;type\u0026quot;: \u0026quot;phrase\u0026quot;\r}\r}\r}\r}\rOr for the ALM Intent Engine and the Intent Process persistence service\n{\r\u0026quot;query\u0026quot;: {\r\u0026quot;bool\u0026quot;: {\r\u0026quot;should\u0026quot;: [\r{\r\u0026quot;match_phrase\u0026quot;: {\r\u0026quot;kubernetes.labels.app\u0026quot;: \u0026quot;daytona\u0026quot;\r}\r},\r{\r\u0026quot;match_phrase\u0026quot;: {\r\u0026quot;kubernetes.labels.app\u0026quot;: \u0026quot;talledega\u0026quot;\r}\r}\r],\r\u0026quot;minimum_should_match\u0026quot;: 1\r}\r}\r}\rView Intent History Each Assembly instance has an Intent History. By virtue of the assembly existing at least one Intent must have been executed on the Assembly Instance.\nOne can view the Intent History from the ALM UI in one of two places\n From the main Operations --\u0026gt; Recent Assembly Instance view From within the Process Execution View of an assembly instance  Intent History from Operations --\u0026gt; Recent Assembly Instance view Each assembly instance in this view has via the ellipsis (...) an Execution History option. This will bring up a model with every historical Intent execution for the target Assembly Instance listed, along with timestamps and a result (status).\nTo view the Intent in detail one should select Inspect to be taken to a snapshot of the completed process in a Process Execution View.\nIntent History from Process Execution View of an assembly instance From any \u0026lsquo;Process Execution View\u0026rsquo; (PEV) on the target Assembly Instance the user can select Execution History from the PEV header. This will bring up a model with every historical Intent execution for the in context Assembly Instance listed, along with timestamps and a result (status).\nTo view the Intent in detail one should select Inspect to be taken to a snapshot of the completed process in a Process Execution View.\nView Designs The design for an Assembly descriptor can be viewed in the ALM UI under Designer \u0026ndash;\u0026gt; Assembly Designer from which an Assembly descriptor can be selected (searched for) and viewed in detail both in terms of its composition and its Behaviour Tests.\nRuntime diagnostics ALM support the execution fo Behaviour Tests on an instantiated Assembly Instance. This can be performed using the Behaviour Tests defined in the Assembly Design and executed through the Behaviour Test runner.\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/operations/infrastructure-key-management/","title":"Infrastructure Key Management","tags":[],"description":"","content":"Introduction An Infrastructure Key is a key comprising private and/or public keys (SSH keys) used when provisioning and communicating with infrastructure. Infrastructure keys are managed by LM (Brent), which provides REST APIs to manage them. The keys themselves are not exposed north-bound of Brent, only their names see resource descriptor key properties. Brent will dynamically substitute any key names in properties with the real key (public and private portions, if defined) before sending the key material over an SSH HTTP connection to Resource drivers.\nThere are two types of infrastructure key:\n shared: these form a global pool of infrastructure keys that are created using Brent REST APIs and can be shared between resource instances. resource instance: these are generated by infrastructure and are linked to the resource instance that created them (so that they will be removed when the resource instance is removed).  The structure of an infrastructure key is:\n id: this is generated by Brent name: the name of the infrastructure key. For shared keys, this is provided when the key is created using the REST APIs. For resource instance infrastructure keys, this is provided with the key material sent back by the driver. This is mandatory. description: a description for the infrastructure key. privateKey: private key portion of the infrastructure key, stored securely at rest by Brent. publicKey: public key portion of the infrastructure key.  Note that either or both privateKey and publicKey can be provided.\nUse Cases LM infrastructure keys support a number of use cases:\n Provisioning (compute) infrastructure, such as VMs, with pre-defined SSH keys from the shared infrastructure key pool at infrastructure instantiation time. Note that this is dependant on the capabilities of the underlying VIM technology, and whether it supports the provisioning of new SSH keys from existing key material provided by LM. Storing SSH keys in LM that have been generated during provisioning of infrastructure (e.g. VMs) by a Resource driver. They can be stored either as per-resource keys, or in the shared infrastructure key pool so that the keys can be used to provision other infrastructure.  In both cases, the storage of infrastructure keys in LM allows LM to securely communicate with infrastructure.\nREST API See the Infrastructure Keys API Definition.\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/reference/lm-api/","title":"Lifecycle Manager API","tags":[],"description":"","content":""},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/reference/stratoss-lm-software-architecture/","title":"Lifecycle Manager Architecture","tags":[],"description":"","content":"Overview Agile Lifecycle Manager (ALM) software application is deployed as separated distinct component sets which can be considered in two groups;\n Core ALM consisting of a number of microservices Dependencies: Foundation functionality consisting of industry standard third party implementations  Message Bus Data Persistence Authentication Server    ALM Core Microservices The ALM microservices are namely:\n User Interface  Host for ALM User Interface.   API Gateway  A gateway for the full set of published ALM APIs used by the ALM User Interface and available for use by any number of configured Client applications. It supports full management of the set of Network Services governed by ALM   Service Registry  The first instantiated ALM service, The Service Registry is responsible for making per service configuration available to each ALM Service (instance) as it is instantiated   Intent Engine  Core ALM service, the Intent Engine is responsible for deriving the set of lifecycle transitions on each deployed resource to transition the Network Service from its current state to an intended/desired state in real-time and optimally orchestrating their execution leveraging opportunities for concurrency while reflecting transition dependencies.   Process Repository  Helper service for the Intent engine supporting its High Availability operation for stateful operations   Component Catalogue  A Data Presentation service for the set of defined NS, VNF and VNFCs   Graph Network Service Topology  A Graph of deployed Network Services and their constituent components to the leaf VNFC level   Deployment Topology  Service maintaining a view of deployment environments and their capability in support of Deployment Policy enforcement (affinity \u0026amp; pinning support)   Behavioral Scenario Execution Engine  Test and certification service. Allowing managed, in situ scenarios to be executed against individual Network Services and the collection of specific metrics while the test is in play   ALM Policy Enforcement  Integrity (Heal) and Scaling (Horizontal) Metric Monitor and Policy execution. This is responsible for enforcing the policy across the set of all VNFCs under management   Resource Manager  ALM supports the binding of multiple Resource Managers but comes with an expandable resoruce manager already integrated. In operation, the resource manager is issued with individual Lifecycle transition instructions by the Intent Engine. It is responsible for performing infrastructure provisioning and lifecycle transitions at the individual VNFC level. It utilizes modularized Infrastructure Manager specific drivers to allow it to communicate with a number of Infrastructure Managers.  Resource Drivers  One logical Driver per NFVI/VIM (e.g OpenStack, Kubernetes) Scaled set of Lifecycle deployment worker nodes        Deployment Model ALM Core application is realized by a number of microservices deployed on Kubernetes environment. Each microservice has a specific role/function within the ALM application and communicate with each other via RESTful APIs and through a Kafka message bus.\nEach ALM microservice is realized as a Kubernetes service and implemented as a set of service instances which are reachable via a load balancer. Every service operates in a fully active instance set. That is, all service instances within the set are fully operational and capable of servicing any request made of it concurrent to its peers within the service.\nA ALM instance is deployed in full in a single Data Center. The colocation of the set of microservices that make up a ALM instance to a single Data Center is for efficiency and to reduce latency in inter microservice communication\nALM installation assumes an existing Kubernetes cluster with a sufficient number of nodes to deploy the services on. Distribution of Kubernetes nodes on virtual machines and eventually on physical resources in a data center to secure resilience against various infrastructure failures is managed by Kubernetes and layers under it.\nHigh Availability Features of Kubernetes are leveraged to provide High Availability of ALM. High Availability is realized by managing a set of service instances for each logical service. By default ALM HA configuration defaults to 3 instances per service. Although, the actual number of instances needs to factor in the load and the expectation.\nThis is based on a simple case of:\n HA = R + L, where:  R: The number of required redundant instances. That is the number of failed instances the system must cater for without the ability to cater for the current load. L: The number of required instances for the system to maintain operation.    High Availability for ALM is configurable on a per service (service set) basis for all services in ALM. Each individual ALM Microservice is configured to be deployed with the following Kubernetes parameters:\n minReplicas maxReplicas Replicas  Of these minReplicas and maxReplicas are used to manage the number of instances per service. Replicas is used to identify how many are created on deployment of ALM. This will represent the number of instances that are required to support the HA requirement of minReplicas.\nWithin a Service Set, the set of service instances should ideally be distribution across the set of available Kubernetes nodes. This provides resilience to the loss of a node will result in the loss of not more than one instance within a given service set. This is managed using a preferredDuringScheduling request in the configuration. This allows the system to maintain the number of instances in a set even when the number of worker nodes is insufficient to maintain one service instance per worker node. This may occur if there is the loss of a node or if scaling results in the need to ‘double-up’ due to the size of the service set exceeding the number of available nodes.\nmaxReplicas can be used to cap a particular service at a level. This prevents run away and the risk of an individual service scaling to a level that results in the exhaustion of underlying resources to the detriment of other services and their ability to scale.\nSolution Architecture Figure below illustrates the positioning of ALM with other main components involved in service development and orchestration.\nThe main responsibilities of ALM is to enable modeling of network services (NS) and drive them through their lifecycles according to requests it receives. ALM integrates on its northbound interface with Order management tools to take in orders to deploy, maintain or decommission service instances.\nIncoming requests are resolved to required actions, broken down to VNF and eventually to VNFC level lifecycle transitions to be executed. Finally, the resource level requests are passed to responsible VIMs, SDN Controllers, VNF Managers and VNFs to fulfill their responsibilities.\nIntegration to OSS enables among other things closed loop monitoring of service health and load, and access to resource inventory.\nALM provides also seamless integration to CI/CD tools and processes to ensure automation of service development process and testing. ALM comes with a CI/CD hub providing pre-integrated CI/CD toolset and framework to be used as a reference implementation of CI/CD pipeline.\nCentralized user and log management can also be integrated through corresponding interfaces.\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/reference/state-models/","title":"Lifecycle Manager Architecture -- State Models","tags":[],"description":"","content":"Component State Model Assemblies and resources have a predefined state model (comparable to TOSCA) as defined in the diagram below.\nComponents flow through this state model by performing lifecycle transitions. Resources can define which transitions they support (in their descriptors) but an assembly will always transition through\nIn addition to the five transitions shown below, resources can implement the Integrity transition, representing a functional “smoke test” which is performed prior to the resource becoming Active and after the Start Lifecycle Transition\nSince assemblies are logical entities, an assembly is transitioned by transitioning each of its child components to the same state. More correctly it is that all of the assemblies child components have reached at least the same state. There will be cases where due to property dependencies in order transition all of the child components to a target state it may be necessary to transition some to a 'higher' state.\nLifecycle Transitions ALM performs lifecycle transitions as a component transitions from one state to another. More correctly a transition occurs while a component is in the source state and the transition to the target state occurs on the successful execution the Lifecycle transition script in the case of a resource and on the successful transition of all child components in the case of an assembly. ALM supports the following transitions;\n   Transition Source State Target State On error Comment     Create \u0026lt;null\u0026gt; Created Failed Create occurs on creation of a resource   Install Created Installed Failed    Configure Installed Inactive Installed    Start Inactive Active Inactive    Integrity Inactive Active Inactive Integrity is only performed on successful completion of Start transition   Stop Active | Broken Inactive Failed    Uninstall Inactive Created Inactive    Delete Created \u0026lt;null\u0026gt; Created    Reconfigure Active Active Active A reconfigure is triggered by a Volatile Property change and there is no change of state    Relationship State Model A relationship has its own lifecycle. It is much simpler as there is only a single state for a relationship and two transitions. On each transition a zero or more Operations are executed against either the source or the target components\nRelationship State Transitions    Transition Source State Target State On error Comment     Create \u0026lt;null\u0026gt; Created \u0026lt;null\u0026gt; Create occurs on creation of a resource   Install Created \u0026lt;null\u0026gt; \u0026lt;null\u0026gt;     "},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/reference/lmctl/","title":"LMCTL","tags":[],"description":"","content":"LMCTL is a command-line client for the Agile Lifecycle Manager (ALM) that provides commands for interacting with LM environments. It includes an opinionated pattern for managing service designs during the CI/CD lifecycle as projects, to produce packages suitable for release into production.\nPlease use version 2.5.0 of the LMCTL documentation with 2.2.0 of ALM.\nlmctl - LM version compatibility:\n   LM Version lmctl Version     2.0.3 2.0.7.1   2.1.0 2.4.1   2.2.x \u0026gt;= 2.5.0    https://github.com/accanto-systems/lmctl/releases/\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/operations/log-management/","title":"Log Management","tags":[],"description":"","content":"Logging and Log Management Agile Lifecycle Manager (ALM) generates logs across all of its microservices. These logs are generated locally and shipped to Elasticsearch where they are persisted and can be retrieved/searched\nLog tracing across ALM can be a challenge if not done correctly:\n There are ~9 discrete microservices Each is realized by N service instances Most services are stateless thus load/requests are evenly load balanced across these service instances.  This creates multiple possible paths an action request can take for the total set of internal API calls required to service that request. This in turn means that any associated logs are spread across multiple log files. By taking these and shipping them to Elasticsearch a single searchable chronologically ordered log stream is created. This eliminates a considerable challenge when viewing logs.\nHowever in a busy system there remains an issue of being able to isolate logs for a specific request.\nTo eliminate this and make it easier to follow the set of communication requests and the resulting logs for a specific request, ALM provides supports Tracing Contexts and primarily a Transaction Id. At the point of ingress to ALM a Tracing Context (traceCtx) can be provided. In API calls this is achieved through the addition of a X-TraceCtx-TransactionId header. If one is not provided then the point of ingress (typically the API gateway) will generate one so that it is available for the remainder of the of the transaction\nThis Tracing Context is passed between services on each interaction. By providing this as in indexed field in any resulting logs it is possible trace the end to end set of logs for a given transaction and to present them chronologically.\nLogs generated by the core ALM services and optionally any service feeding into it or which it interacts with (such as the RM) are supplied with the set of X-TracingCtx headers so true end to end log isolation can be achieved in highly concurrent system even when under load This can be used in conjunction with an intent (process) Id where applicable\nView Log Files ALM Logging is at standard levels:\n INFO WARNING ERROR DEBUG  Each service instance can have its own log level set individually. This is achieved in one of two ways and details of how to do so are discussed below. The default level, if not overridden, is INFO\nAll ALM Logs are generated locally to the docker logs and are accessible on the individual pod via kubectl logs on the Kubernetes cluster\nCore ALM service generated logs are in JSON format to facilitate ingestion to Elasticsearch\nLogs are pulled by Filebeat from the individual log streams on the service instances (pods) and inserted into appropriate indices in Elasticsearch. This creates a single chronologically ordered* set of log records ( indices )\nKibana is the preferred tool for examining logs and it is recommended that it be used whenever viewing logs, especially in a production system\nSetting log levels As referenced earlier, the default logging level for ALM defaults to INFO, the lowest level of logging generating the least output. There are two ways in which the logging level can be altered\n Alter the default setting on per service level in Vault Alter the runtime setting on a per service instance/pod level  Setting default logging Default Logging Level is set on a per ALM microservice level by configuring the preferred level in the service configuration (secret) in Vault. The Default Logging Level for the service is persisted such that on startup, the individual LM service instances read this value and set the log level accordingly.\nlogging.level.com.accantosystems: DEBUG\rTo set the Default Logging Level for a service the user should\n log on to Vault and navigate to the appropriate service secret (if one does not exist for the target service then it should be created). Add the logging level statement to the config at the top level. In the following example this is as a peer to \u0026quot;alm\u0026quot;  {\r\u0026quot;alm\u0026quot;: {\r\u0026quot;apollo\u0026quot;: {\r\u0026quot;cassandra\u0026quot;: {\r\u0026quot;keyspaceManager\u0026quot;: {\r\u0026quot;replicationFactor\u0026quot;: 1\r}\r},\r\u0026quot;janus\u0026quot;: {\r\u0026quot;cluster.max-partitions\u0026quot;: 4,\r\u0026quot;index\u0026quot;: {\r\u0026quot;search\u0026quot;: {\r\u0026quot;elasticsearch.create.ext.index.number_of_replicas\u0026quot;: 0,\r\u0026quot;elasticsearch.create.ext.index.number_of_shards\u0026quot;: 1\r}\r},\r\u0026quot;storage.cql.replication-factor\u0026quot;: 1\r}\r}\r},\r\u0026quot;logging.level.com.accantosystems\u0026quot;: \u0026quot;DEBUG\u0026quot;\r}\rIf being set on a running system, all the impacted service instances (pods) will need to be bounced in order to retrieve the new setting.  Altering the log level at run-time Alternatively the level on service instances can be set individually at runtime without requiring a restart of the service instance.\nMultiple service instances per service will exist in a HA deployment. The means that the level will effectively need to be set on all service instances for target service so that desired logs are generated.\nThis is achieved by calling an internal API for a running pod. It is post simply POST to the management endpoint on each service. For example to set the logs to debug for all ALM logging in Daytona instance, you could exec onto the pod and do this:\ncurl -i -X POST -H 'Content-Type: application/json' -d '{\u0026quot;configuredLevel\u0026quot;: \u0026quot;DEBUG\u0026quot;}' http://localhost:\u0026lt;8281\u0026gt;/management/loggers/com.accantosystems\rin general the form is;\ncurl -i -X POST -H 'Content-Type: application/json' -d '{\u0026quot;configuredLevel\u0026quot;: \u0026quot;\\\u0026lt;level\\\u0026gt;\u0026quot;}' http://\\\u0026lt;IP Address | localhost\\\u0026gt;:\\\u0026lt;port\\\u0026gt;/management/loggers/com.accantosystems\rAdditional authentication header needs to be added when setting the log level for the Ishtar service. please see here for details\nLog level One of the ALM supported Log Levels (INFO being the most terse and DEBUG the most verbose):\n INFO WARNING ERROR DEBUG  IP Address The internal, cluster IP address of the pod being targeted. Alternatively if the call is being made from the pod itself then \u0026lsquo;localhost\u0026rsquo; should be used\nPort The port for the service is deployed on for that Pod. If not know, this can be identified using kubectl:\nkubectl get svc\rThis will return the TCP port for the target ALM service.\nAuthentication headers for Ishtar In a secure environment the log level for Ishtar gateway can only be set via an authenticated API call. This can be performed in either the internal or external API endpoint (this is unique to Ishtar)\nTo do so, the user must first obtain an access token.\nObtaining an authentication token An accessToken can be obtained using the published ALM API endpoint using appropriate user/client credentials.\nOnce obtained, the accessToken should be used in the API request as follows when using curl, replacing \\\u0026lt;level\\\u0026gt; with the desired Log Level\ncurl -i -X POST -H 'Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR............Nh5QJEdZSyf6YxOOeJ9FRlUWcBQJPENd-xTDEDs' -H 'Content-Type: application/json' -d '{\u0026quot;configuredLevel\u0026quot;: \u0026quot;\\\u0026lt;level\\\u0026gt;\u0026quot;}' http://\\\u0026lt;IP Address | localhost\\\u0026gt;:\\\u0026lt;port\\\u0026gt;/management/loggers/com.accantosystems\r Log Management ALM, and the associated Resource Manager(s), Resource drivers can collectively generate a large number of logs which can have an impact on load of both the system itself but also the downstream services such as Elasticsearch. Thus it is suggested that this is either allowed for in the engineering of the ALM deployment of the system be kept at a low level of logging (INFO). When required the level of the logging can be \u0026lsquo;raised\u0026rsquo;. This of course needs to be done prior to any event which needs to be captured and is a judgment call.\nElastic Search can be configured to remove historical logs based on their Elasticsearch index.\nDocker Logs Log management on individual containers should be performed in accordance with individual deployment policy. For details on how to do so the it is recommended that the user consult docker documentation. Though not typically an issue the configuration for docker must be compatible with the polling rate of Filebeat so that the docker log policy is not deleting logs quicker than Filebeat can retrieve them.\nElasticsearch Indices It is recommend that in creation of index patterns and the design of Kibana dashboards, that the appropriate index pattens be created such that they it is possible for the user select Index Patterns which match the real scope of applicable logs (records). This helps in the performance of the system, managing disk usage and in ensuring that all relevant logs are in scope.\nAdditionally, it should be noted that Opensource Elasticsearch does not have an inbuilt facility for deleting logs (records/indices). However a separate Opensource tool, Elasticsearch-Curator, exists which can be used to perform management of logs (indices). This can be leveraged to manage (delete) older logs from elastic search. Documentation for how to use this tool is accessible here. It is possible to design curator actions with appropriate filters to meet the specific needs of your organization.\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/operations/manage-instances/","title":"Manage Instances","tags":[],"description":"","content":"This section describes the steps required to request an intent on a Network Service Instance, and then view the topology of the instance and the execution of the intent. A Network Service Instance is created from an already existing Network Service Design.\nObjectives  Learn how to request an intent on a Network Service Instance View the execution of the intent View the topology of the Network Service Instance  Pre-requisites  An existing Network Service Design Access to the UI of a running instance of Agile Lifecycle Manager (ALM)  Assembly Instances Recent Assembly Instances Once Network Services and VNFs are designed, they can be put into production. The Operations section in LM allows for assemblies to be searched, and actions to be taken. To find an assembly, you can go to the Recent Assembly Instances section to see the last Assembly Instances that have changed state.\nYou see the name of the assembly instance, the status, descriptor name, and the last Intent and time that was executed. For each assembly instance, you have the option to manually change the intent (e.g. make inactive, upgrade, uninstall). You can also open that assembly instance which will bring you to the topology view. From the Assembly Instance when clicking the \u0026hellip; button, you can open a current intent execution graph (when running), the assembly design, or view the intent execution history that shows all intents for that assembly instance. You can also Create a new Assembly Instant.\nSearch for Assembly Instances You can search for any assembly instance by clicking Search Assembly Instance in the Operations section of LM.\nCreate an Assembly Instance Once the design is finalized, the first thing you will do is to create an Assembly Instance. You can do this by clicking Create Instance. This will ask you to provide a name, select a descriptor, and in what intended state the instance should be in after installation (active, inactive or installed). it will then ask you to change any properties if needed and allows you to review before continuing.\nOnce you press Create, the Assembly Instance will show up on the Recent Assembly Instances tab and you will see that is being brought into its intended state by the Intent Engine. You are able to follow the Intent Engine execution tasks by going to the Execution Graph (see next section).\nIntent Execution Graph ALM includes intent based orchestration capabilities. Once you have onboarded and designed your VNF and Network Services, the Intent Engine automatically calculates and executes the best path to get an assembly from the current state to the desired state without having to manually program any of the tasks. The Intent Execution graph is a view all the tasks that the Intent Engine is executing to bring the Network Service or VNF into its intended state.\nWhen an Intent is being executed, you can view all the tasks the Intent Engine is performing in real-time in a moving graph. You can see what task is performed first, and what lifecycle action or operation is being performed.\nYou can also view the Execution History of an Intent that has been executed. A pop-up screen will show all the Intents that were run for that assembly and you can can select which one to view.\nTopology View When you click Open in the Recent Assembly Instance view, you are taken to the Topology view. This shows the Network Service or VNF, and the components inside of that and their status. As before in the design, you will see VNF/NS elements, clustered elements and referenced elements. You can also see the properties and relationships on the right panel, and change the Intent. You can also go to the Execution Graph by clicking the tab on the bottom.\n Lifecycle Transitions Create a New Network Service A new network service instance can be created by clicking the “Create” button in the top right corner of either “Recent Assembly Instances” or “Assembly Instance Search” page. Below screenshot shows the “Create” button on “Assembly Instance Search” page.\nWhen clicking “Create” button a dialog is opened to fill in details required to create the service instance. This includes a unique name for the new instance, descriptor to be used, intended target state, and necessary property values. After filling in all necessary data, the definitions can be reviewed before triggering the instantiation. Example dialog to create a service instance is shown in below.\nTerminate Network Service A network service instance can be deleted by clicking the “Change Intent” button associated to the instance and selecting “Uninstall” from the dropdown menu. The “Change Intent” button can be found on “Recent Assembly Instances”, “Assembly Instance Search”, and opened “Topology” and “Execution” pages. Below screenshot shows the “Change Intent” button clicked on “Recent Assembly Instances” page.\nChange Network Service State The operational state of an existing network service instance can be changed by clicking the “Change Intent” button associated to the instance and selecting desired transition from the dropdown menu. The “Change Intent” button can be found on “Recent Assembly Instances”, “Assembly Instance Search”, and opened “Topology” and “Execution” pages. The options available in the dropdown menu are dependent on the current state of the service instance. Below screenshot shows the “Change Intent” button clicked on “Recent Assembly Instances” page.\nTrigger Healing of a VNFC Manual healing for a VNFC can be triggered by navigating to the “Topology” view of a network service and selecting the target element from the service topology. If the service contains nested assemblies and the actual VNFC is not on the highest level of service elements, lower level service elements can be seen by clicking the icon of the corresponding parent element. Once the target VNFC is selected, the healing sequence can be triggered by clicking the “Heal” button shown in the header of the right hand panel as illustrated in below screenshot.\nTrigger Scaling of a VNF Similarly to healing, scaling of a clustered element can be triggered by navigating to the “Topology” view of a network service and selecting the target element from the service topology. Scaling is only available for clustered service elements and thus the selected element should be defined as a cluster. Once the target cluster is selected, the scaling sequence can be triggered by clicking either the “Scale In” or “Scale Out” button shown in the header of the right hand panel as illustrated in below screenshot.\nUpgrade Network Service Upgrade of a network service instance can be performed by clicking the “Change Intent” button associated to the instance and selecting “Upgrade” from the dropdown menu. The “Change Intent” button can be found on “Recent Assembly Instances”, “Assembly Instance Search”, and opened “Topology” and “Execution” pages. Below screenshot shows the “Change Intent” button clicked on “Recent Assembly Instances” page.\nOnce “Upgrade” option is selected, a dialog is opened to fill in details about the targeted upgrade. An upgrade can include changing the whole service descriptor and/or changing property values defined for the service. Actual upgrade sequence depends on the defined changes. Below screenshots illustrate the sequence of upgrade definition.\n"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/behaviour-testing/designing-scenarios/monitoring-assembly-events/","title":"Monitor Assembly Events","tags":[],"description":"","content":"The steps in the \u0026ldquo;Assembly Events\u0026rdquo; group allow you to check intents have occurred on an Assembly. This is usually done to check a Heal or Scale has taken place on an Assembly when a policy is triggered.\nCheck Assembly Process Success If you have a scenario which simulates traffic/metrics to trigger a policy on your Assembly, then you can check the expected Heal or Scale takes place by adding the \u0026ldquo;Check for currently in progress process\u0026rdquo; or \u0026ldquo;Check for successful process\u0026rdquo; steps.\nIn this example, shown below, we have a scenario which will manually trigger a Scale Out of a cluster named \u0026ldquo;A\u0026rdquo; in our example Assembly:\nTo check a process takes place complete the following steps:\n  Add the \u0026ldquo;Check for successful process\u0026rdquo; step from the right hand panel\n  Select the expected process to set the \u0026ldquo;processType\u0026rdquo; property and enter the name of the Assembly you expect\n  Execute the scenario by clicking the \u0026ldquo;Run Scenario\u0026rdquo; button at the top of the designer. In the execution screen you will see the step wait for a successful scale out to complete (in our case it\u0026rsquo;s the scale out we manually triggered)\n  "},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/reference/lm-api/api-definition/topology/process-api/","title":"Processes","tags":[],"description":"","content":"This section covers the APIs used to view processes running against Assembly instances during their life.\nAssociated with each definition are details of the request parameters and responses. These include the name of each field plus a brief description and whether the field is mandatory. Whether a field is required or not is based on the context of the examples. The underlying API definition may mark a field as optional, but in some contexts, the fields must be supplied.\nFor a 40x, 50x or any other error response please see error response codes\nCheck Process Check the status of a process\nRequest    Aspect Value     Endpoint URL /api/processes/{id}   HTTP Method GET    Path Parameters    Field Description Mandatory     id ID of the process Yes    Response    Aspect Value     Content-Type application/json   Response Code 200 (OK)    Body    Field Description     id Unique identifier for the process   assemblyId Unique identifier for the Assembly being used by the process   assemblyName Name of the Assembly being used by the process   assemblyType Name of the Assembly Type   lifecycleAction The Lifecycle action being performed by the process   intent Definition of the lifecycle change that the process shall perform/has performed. The properties within this entry will therefore vary for each different Intent   intentType The type of lifecycle change will be/has been performed by the process (CreateAssembly, DeleteAssembly, ChangeAssemblyState, HealAssembly, ScaleInAssembly, ScaleOutAssembly, HealAssembly, UpgradeAssembly)   status Current state of the process. (Planned, Pending, In Progress, Completed, Cancelled, Failed   statusReason Reason for the status (e.g. a reason for failure)   startTime Time the process started expressed in ISO 1806 format. This time is taken from the server on which LM is running. If the process is yet to start then this property will not be present   endTime Time the process finished expressed in ISO 1806 format. This time is taken from the server on which LM is running. If the process has not completed or been cancelled, then this property will not be present   assemblyProperties Properties passed in with the request   context Any additional contextual parameters required by the process (typically empty)   previousInstance The state of the Assembly instance that the Process shall start with   desiredInstance The desired state of the Assembly instance that the Process shall migrate the Assembly to using the executionPlan   executionPlan The definition of the process and the list of tasks the process shall execute to achieve the required lifecycle change    Example:\n{\r​ \u0026quot;id\u0026quot; : \u0026quot;1b9c6e04-1cce-4cb6-8c30-ffda960964ec\u0026quot;,\r​ \u0026quot;assemblyId\u0026quot; : \u0026quot;fd262ceb-9cdc-480a-aba4-691ba50febd6\u0026quot;,\r​ \u0026quot;assemblyName\u0026quot; : \u0026quot;BasicTestAssembly\u0026quot;,\r​ \u0026quot;assemblyType\u0026quot; : \u0026quot;assembly::t_bta_noStopStart::1.0\u0026quot;,\r​ \u0026quot;lifecycleAction\u0026quot; : \u0026quot;Heal\u0026quot;,\r​ \u0026quot;intent\u0026quot; : {\r​ \u0026quot;assemblyName\u0026quot; : \u0026quot;BasicTestAssembly\u0026quot;,\r​ ...\r​ },\r​ \u0026quot;intentType\u0026quot; : \u0026quot;HealAssembly\u0026quot;,\r​ \u0026quot;status\u0026quot; : \u0026quot;Completed\u0026quot;,\r​ \u0026quot;startTime\u0026quot; : \u0026quot;2018-08-08T16:29:23.218+01:00\u0026quot;,\r​ \u0026quot;endTime\u0026quot; : \u0026quot;2018-08-08T16:29:23.714+01:00\u0026quot;,\r​ \u0026quot;context\u0026quot; : {},\r​ \u0026quot;previousInstance\u0026quot; : { ... },\r​ \u0026quot;desiredInstance\u0026quot; : { ... },\r​ \u0026quot;executionPlan\u0026quot; : { ... }\r}\rThe content of some of the above fields has been shortened and replaced with ellipses as it will vary greatly depending on the exact lifecycle change required.\nSearch for a Process Return a list of processes based on a set of query parameters. The Query parameters are supplied as URL parameters.\nRequest    Aspect Value     Endpoint URL /api/processes?{query-params}   Content-Type application/json   HTTP Method GET    Query Parameters    Field Description Mandatory     assemblyId The unique id of the Assembly being used by the process(es). No   assemblyName Name of the Assembly being used by the process(es). This is case sensitive. The “*” wildcard characters is supported anywhere within the text string and matches any number of characters No   assemblyType The Type of Assembly. This is case sensitive and must be an exact match. No wildcards are supported No   startDateTime The time the process must have started after, expressed in ISO 1806 format No   endDateTime The time the process must have ended before, expressed in ISO 1806 format No   processStatuses A comma-separated list of process states, at least one of which the process must be in No   intentTypes A comma-separated list of Intent Types, at least one of which the process must be using No   limit The maximum number of matching processes to be returned by this query. If this is not supplied then all matching processes will be returned No    Response    Aspect Value     Content-Type application/json   Response Code 200 (OK)    Body The body includes a single list of matching processes, each with the following fields:\n   Field Description     id Unique identifier for the process   assemblyId Unique identifier for the Assembly being used by the process   assemblyName Name of the Assembly being used by the process   assemblyType Name of the Assembly Type   lifecycleAction The Lifecycle action being performed by the process   intent Definition of the lifecycle change that the process shall perform/has performed. The properties within this entry will therefore vary for each different Intent   intentType The type of lifecycle change will be/has been performed by the process (CreateAssembly, DeleteAssembly, ChangeAssemblyState, HealAssembly, ScaleInAssembly, ScaleOutAssembly, HealAssembly, UpgradeAssembly)   status Current state of the process. (Planned, Pending, In Progress, Completed, Cancelled, Failed   statusReason Reason for the status (e.g. a reason for failure)   startTime Time the process started expressed in ISO 1806 format. This time is taken from the server on which LM is running. If the process is yet to start then this property will not be present   endTime Time the process finished expressed in ISO 1806 format. This time is taken from the server on which LM is running. If the process has not completed or been cancelled, then this property will not be present   assemblyProperties Properties passed in with the request   context Any additional contextual parameters required by the process (typically empty)   previousInstance The state of the Assembly instance that the Process shall start with   desiredInstance The desired state of the Assembly instance that the Process shall migrate the Assembly to using the executionPlan   executionPlan The definition of the process and the list of tasks the process shall execute to achieve the required lifecycle change    Example:\n[\r{\r​ \u0026quot;id\u0026quot; : \u0026quot;1b9c6e04-1cce-4cb6-8c30-ffda960964ec\u0026quot;,\r​ \u0026quot;assemblyId\u0026quot; : \u0026quot;fd262ceb-9cdc-480a-aba4-691ba50febd6\u0026quot;,\r​ \u0026quot;assemblyName\u0026quot; : \u0026quot;BasicTestAssembly\u0026quot;,\r​ \u0026quot;assemblyType\u0026quot; : \u0026quot;assembly::t_bta_noStopStart::1.0\u0026quot;,\r​ \u0026quot;lifecycleAction\u0026quot; : \u0026quot;Heal\u0026quot;,\r​ \u0026quot;intent\u0026quot; : {\r​ \u0026quot;assemblyName\u0026quot; : \u0026quot;BasicTestAssembly\u0026quot;,\r​ ...\r​ },\r​ \u0026quot;intentType\u0026quot; : \u0026quot;HealAssembly\u0026quot;,\r​ \u0026quot;status\u0026quot; : \u0026quot;Completed\u0026quot;,\r​ \u0026quot;startTime\u0026quot; : \u0026quot;2018-08-08T16:29:23.218+01:00\u0026quot;,\r​ \u0026quot;endTime\u0026quot; : \u0026quot;2018-08-08T16:29:23.714+01:00\u0026quot;,\r​ \u0026quot;context\u0026quot; : {},\r​ \u0026quot;previousInstance\u0026quot; : { ... },\r​ \u0026quot;desiredInstance\u0026quot; : { ... },\r​ \u0026quot;executionPlan\u0026quot; : { ... }\r},\r...\r]\r"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/reference/lm-api/api-definition/behavior-testing-api/projects/","title":"Projects","tags":[],"description":"","content":"The following details the API to manage Projects within LM. A Project is auto-created for every Assembly descriptor added to LM.\nAssociated with each definition are details of the request parameters and responses. These include the name of each field plus a brief description and whether the field is mandatory. Whether a field is required or not is based on the context of the examples. The underlying API definition may mark a field as optional, but in some contexts, the fields must be supplied.\nFor a 40x, 50x or any other error response please see error response codes\nCreate Project Create a new Assembly Configuration\nRequest    Aspect Value     Endpoint URL /api/behaviour/projects   Content-Type application/json   HTTP Method POST    Body    Field Description Mandatory     name Unique name of the Project Yes   description Supplied description of the Project No    Example:\n{\r​ \u0026quot;name\u0026quot;: \u0026quot;assembly::t_simple::1.0\u0026quot;,\r​ \u0026quot;description\u0026quot;: \u0026quot;Project for assembly::t_simple::1.0\u0026quot;\r}\rResponse    Aspect Value     Content-Type application/json   Response Code 201 (Created)    Headers    Field Description     location Endpoint to created resource    Body    Field Description     id ID assigned to the Project (will be the same as the name value)   name Name of the Project   description Supplied description of the Project    Example:\n{\r\u0026quot;id\u0026quot;: \u0026quot;assembly::t_simple::1.0\u0026quot;,\r​ \u0026quot;name\u0026quot;: \u0026quot;assembly::t_simple::1.0\u0026quot;,\r​ \u0026quot;description\u0026quot;: \u0026quot;Project for assembly::t_simple::1.0\u0026quot;\r}\rUpdate Project Update a Project\nRequest    Aspect Value     Endpoint URL /api/behaviour/projects/{projectId}   HTTP Method PUT    Path Parameters    Field Description Mandatory     projectId ID of the Project Yes    Body    Field Description Mandatory     name Unique name of the Project Yes   description Supplied description of the Project No    Example:\n{\r​ \u0026quot;name\u0026quot;: \u0026quot;assembly::t_simple::1.0\u0026quot;,\r​ \u0026quot;description\u0026quot;: \u0026quot;Project for assembly::t_simple::1.0\u0026quot;\r}\rResponse    Aspect Value     Content-Type application/json   Response Code 200 (OK)    Body    Field Description     id ID assigned to the Project (will be the same as the name value)   name Name of the Project   description Supplied description of the Project    Example:\n{\r\u0026quot;id\u0026quot;: \u0026quot;assembly::t_simple::1.0\u0026quot;,\r​ \u0026quot;name\u0026quot;: \u0026quot;assembly::t_simple::1.0\u0026quot;,\r​ \u0026quot;description\u0026quot;: \u0026quot;Project for assembly::t_simple::1.0\u0026quot;\r}\rRemove Project Removes a Project and all contained Assembly Configurations, Scenarios and Scenario Executions.\nRequest    Aspect Value     Endpoint URL /api/behaviour/projects/{projectId}   HTTP Method DELETE    Path Parameters    Field Description Mandatory     projectId ID of the Project Yes    Response    Aspect Value     Content-Type application/json   Response Code 204 (No Content)    Get all Projects Retrieve all Projects\nRequest    Aspect Value     Endpoint URL /api/behaviour/projects   HTTP Method GET    Response    Aspect Value     Content-Type application/json   Response Code 200 (OK)    Body The body will contain a single list of Projects. Each entry will have the following fields:\n   Field Description     id ID assigned to the Project (will be the same as the name value)   name Name of the Project   description Supplied description of the Project    Example:\n{\r\u0026quot;id\u0026quot;: \u0026quot;assembly::t_simple::1.0\u0026quot;,\r​ \u0026quot;name\u0026quot;: \u0026quot;assembly::t_simple::1.0\u0026quot;,\r​ \u0026quot;description\u0026quot;: \u0026quot;Project for assembly::t_simple::1.0\u0026quot;\r}\rGet Project Retrieve a single Project by ID\nRequest    Aspect Value     Endpoint URL /api/behaviour/projects/{projectId}   HTTP Method GET    Path Parameters    Field Description Mandatory     projectId ID of the Project Yes    Response    Aspect Value     Content-Type application/json   Response Code 200 (OK)    Body    Field Description     id ID assigned to the Project (will be the same as the name value)   name Name of the Project   description Supplied description of the Project    Example:\n{\r\u0026quot;id\u0026quot;: \u0026quot;assembly::t_simple::1.0\u0026quot;,\r​ \u0026quot;name\u0026quot;: \u0026quot;assembly::t_simple::1.0\u0026quot;,\r​ \u0026quot;description\u0026quot;: \u0026quot;Project for assembly::t_simple::1.0\u0026quot;\r}\r"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/user-guides/behaviour-testing/designing-scenarios/recording-metrics/","title":"Recording Metrics","tags":[],"description":"","content":"The behaviour testing engine is capable of consuming metrics from Kafka topics in the background, whilst a scenario executes. This can be used to test a service is behaving as expected.\nThe following guide shows you how to use the Metric Definitions, Metric Recording and Metric Assertions steps to record a metric and check it\u0026rsquo;s value.\nDefining the Metric To record metrics, you will need a Resource that produces metrics. In this guide, we use an example Assembly with a Resource that produces metrics for \u0026ldquo;Failed Calls\u0026rdquo;.\nAn example message for a failed calls metric measurement is shown below:\n{\r\u0026quot;resource\u0026quot;: \u0026quot;Example_A\u0026quot;,\r\u0026quot;time\u0026quot;: \u0026quot;1570177230416\u0026quot;,\r\u0026quot;type\u0026quot;: \u0026quot;failed_calls\u0026quot;,\r\u0026quot;value\u0026quot;: \u0026quot;1\u0026quot;\r}\rWe create a metric definition for this message type with the following steps:\n  Open a new Scenario and add an Assembly which produces the metrics we plan to record\n  Configure the first stage, with a name of \u0026ldquo;Setup Definitions\u0026rdquo; (or a name of your choice)\n  Add a \u0026ldquo;Specify the definition for a metric\u0026hellip;\u0026rdquo; step from the \u0026ldquo;Metric Definition\u0026rdquo; group in the scenario palette\n  Give the definition a name, in our example we call it \u0026ldquo;Example_A_Metrics_Format\u0026rdquo;\n  Open the definition table by clicking on \u0026ldquo;Open property: metricDefinition\u0026rdquo;. Input the names of each field in the value column of the table. It\u0026rsquo;s important that you assign the correct field name from your metric messages to the correct field in the table. In our example, we assign the \u0026ldquo;resource\u0026rdquo; field of our message to the \u0026ldquo;resourceIdField\u0026rdquo; of the definition, the \u0026ldquo;time\u0026rdquo; field to the \u0026ldquo;timestampField\u0026rdquo;, the \u0026ldquo;type\u0026rdquo; field to the \u0026ldquo;metricTypeField\u0026rdquo; and \u0026ldquo;value\u0026rdquo; to the \u0026ldquo;valueField\u0026rdquo;.\n  To read more about the fields required on a metric definition, see the step reference\nRecording the Metric To begin recording the metric, complete the following:\n  Add a new stage and call it \u0026ldquo;Start Recording\u0026rdquo; (this is optional, you can add all of the steps to the same stage if preferred)\n  Add the recording step from the \u0026ldquo;Metric Recording\u0026rdquo; group of the scenario palette\n  Fill in the step so it will record the target metric:\n The \u0026ldquo;metricName\u0026rdquo; can be any value of your choosing. This is used to refer to the metric in your scenario, so pick a descriptive name. In our example we will set it to \u0026ldquo;Example_A Failed Calls\u0026rdquo; The \u0026ldquo;metricDefinitionName\u0026rdquo; should be set to a valid definition, created in this scenario, which describes the format of the message for the metric we are recording. In our example we will set it to \u0026ldquo;Example_A_Metrics_Format\u0026rdquo; The \u0026ldquo;metricType\u0026rdquo; should be set to the value expected on the \u0026ldquo;metricTypeField\u0026rdquo; (of your definition) in the metric messages. In our example, we want to record the messages with a \u0026ldquo;type\u0026rdquo; field value of \u0026ldquo;failed_calls\u0026rdquo;, so we set this to \u0026ldquo;failed_calls\u0026rdquo; (\u0026ldquo;type\u0026rdquo; is the \u0026ldquo;metricTypeField\u0026rdquo; we specified on our definition). The \u0026ldquo;topicName\u0026rdquo; should be set to the name of the Kafka topic you expect the messages to be on The \u0026ldquo;resourceId\u0026rdquo; should be set to the value expected on the \u0026ldquo;resourceIdField\u0026rdquo; (of your definition) in the metric messages. In our example, we want to record the metrics for resource \u0026ldquo;Example_A\u0026rdquo; and we know the metric messages will have this value on the \u0026ldquo;resource\u0026rdquo; field of our message, which we used as the \u0026ldquo;resourceIdField\u0026rdquo; on our definition. Therefore, we set this value to \u0026ldquo;Example_A\u0026rdquo;    Checking the Metric Value Now we are recording a metric, we can use \u0026ldquo;Metric Assertion\u0026rdquo; steps to check the measurements, failing the scenario if they are not a suitable value.\n  Add a new stage and call it \u0026ldquo;Check Metrics\u0026rdquo; (this is optional, you can add all the steps to the same stage if preferred)\n  Add the \u0026ldquo;Verify recorded metric always under threshold\u0026rdquo; step from the \u0026ldquo;Metric Assertions\u0026rdquo; group in the scenario palette\n  Fill in the step so it will check our recorded metric:\n The \u0026ldquo;metricName\u0026rdquo; must match the \u0026ldquo;metricName\u0026rdquo; from our record step (i.e. \u0026ldquo;Example_A Failed Calls\u0026rdquo;) The \u0026ldquo;thresholdValue\u0026rdquo; is the value we expect all values from the recorded metrics to be below (i.e. \u0026ldquo;5\u0026rdquo;)    Complete the Scenario Currently our scenario will begin recording metrics then immediately check it\u0026rsquo;s values and finish, meaning we would not record the metric for very long. Usually you would add other steps in the scenario that would take some time. In our example, we\u0026rsquo;re going to simulate this time by adding a \u0026ldquo;Delay\u0026rdquo; step.\n  Add a \u0026ldquo;Delay\u0026rdquo; step from the \u0026ldquo;Utilities\u0026rdquo; group, place it before we assert the metric value\n  Fill in the input values for the delay step. In our example we will delay for 30 seconds\n  Save your changes by clicking \u0026ldquo;Save\u0026rdquo; at the top of the designer\n  We have now completed the design for our scenario.\nExecute the Scenario   Execute the scenario by clicking the \u0026ldquo;Run Scenario\u0026rdquo; button at the top of the designer\n  You will be taken to the execution screen. Once the \u0026ldquo;Start recording\u0026rdquo; step has been executed and some metrics are recorded, you will see a chart appear in the right hand panel. This chart will show the recorded values for your metric\n  Once the step to verify the value is executed, you will see a red dotted line appear on the chart showing the threshold. If all the values are below this line then the step will pass\n  If the metric has gone above this threshold line you will see the step marked as \u0026ldquo;Failed\u0026rdquo;\n  "},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/reference/lm-api/api-definition/behavior-testing-api/scenarios/","title":"Scenarios","tags":[],"description":"","content":"The following details the API to manage Scenarios within LM.\nAssociated with each definition are details of the request parameters and responses. These include the name of each field plus a brief description and whether the field is mandatory. Whether a field is required or not is based on the context of the examples. The underlying API definition may mark a field as optional, but in some contexts, the fields must be supplied.\nFor a 40x, 50x or any other error response please see error response codes\nCreate Scenario Create a new Scenario\nRequest    Aspect Value     Endpoint URL /api/behaviour/scenarios   Content-Type application/json   HTTP Method POST    Body    Field Description Mandatory     name Name of the Scenario Yes   description Supplied description of the Scenario No   projectId Unique identifier for the behaviour test Project this Assembly Configuration belongs to Yes   stages A list of stages, each containing the steps to execute in this Scenario Yes   assemblyActors A list of Assembly Configurations to use within the Scenario Yes    Each stages entry contains the following fields:\n   Field Description Mandatory     name Name of the Stage Yes   steps List of steps to execute in this Stage Yes    Each assemblyActors entry contains the following fields:\n   Field Description Mandatory     instanceName Name to refer to this actor throughout the Scenario. If the instance is one to be instantiated, this name will be used as the instance name Yes   assemblyConfigurationId ID of the Assembly Configuration to instantiate this instance from Yes (if provided is false)   initialState State to create the instance in (Installed, Inactive, Active) Yes (if provided is false)   uninstallOnExit Enable/disable the uninstallation of the instance after the Scenario completes successfully Yes (if provided is false)   provided Dictates if the actor is an existing instance of an Assembly OR to be instantiated from an Assembly Configuration Yes    Example:\n{\r​ \u0026quot;name\u0026quot;: \u0026quot;test-scale-in\u0026quot;,\r​ \u0026quot;projectId\u0026quot;: \u0026quot;assembly::Test::1.0\u0026quot;,\r​ \u0026quot;description\u0026quot;: \u0026quot;description\u0026quot;,\r​ \u0026quot;stages\u0026quot;: [\r​ {\r​ \u0026quot;name\u0026quot;: \u0026quot;Test Stage\u0026quot;,\r​ \u0026quot;steps\u0026quot;: [\r​ {\r​ \u0026quot;stepDefinitionName\u0026quot;: \u0026quot;IntentEngine::CreateAssembly\u0026quot;\r​ \u0026quot;properties\u0026quot;: {\r​ \u0026quot;additionalProp1\u0026quot;: \u0026quot;string\u0026quot;,\r​ \u0026quot;additionalProp2\u0026quot;: \u0026quot;string\u0026quot;,\r​ \u0026quot;additionalProp3\u0026quot;: \u0026quot;string\u0026quot;\r​ },\r​ }\r​ ]\r​ }\r​ ],\r​ \u0026quot;assemblyActors\u0026quot;: [\r​ {\r​ \u0026quot;assemblyConfigurationId\u0026quot;: \u0026quot;7f456ac3-a523-2c1d-2cd1-42dc6124b012\u0026quot;,\r​ \u0026quot;initialState\u0026quot;: \u0026quot;Active\u0026quot;,\r​ \u0026quot;instanceName\u0026quot;: \u0026quot;TestInstance\u0026quot;,\r​ \u0026quot;provided\u0026quot;: false,\r​ \u0026quot;uninstallOnExit\u0026quot;: true\r​ }\r​ ]\r}\rResponse    Aspect Value     Content-Type application/json   Response Code 201 (Created)    Headers    Field Description     location Endpoint to created resource    Body    Field Description     id ID assigned to the Scenario   name Name of the Scenario   description Supplied description of the Scenario   projectId Unique identifier for the behaviour test Project this Assembly Configuration belongs to   stages A list of stages, each containing the steps to execute in this Scenario   assemblyActors A list of Assembly Configurations to use within the Scenario   createdAt Date and time the Scenario instance was created   lastModifiedAt Date and time the Scenario instance was last modified    Each stages entry contains the following fields:\n   Field Description     name Name of the Stage   steps List of steps to execute in this Stage    Each assemblyActors entry contains the following fields:\n   Field Description     instanceName Name to refer to this actor throughout the Scenario. If the instance is one to be instantiated, this name will be used as the instance name   assemblyConfigurationId ID of the Assembly Configuration to instantiate this instance from   initialState State to create the instance in (Installed, Inactive, Active)   uninstallOnExit Enable/disable the uninstallation of the instance after the Scenario completes successfully   provided Dictates if the actor is an existing instance of an Assembly OR to be instantiated from an Assembly Configuration    Example:\n{\r​ \u0026quot;id\u0026quot;: \u0026quot;8e266bc5-e613-4b0d-9ae0-50db6454b026\u0026quot;,\r​ \u0026quot;name\u0026quot;: \u0026quot;test-scale-in\u0026quot;,\r​ \u0026quot;projectId\u0026quot;: \u0026quot;assembly::Test::1.0\u0026quot;,\r​ \u0026quot;description\u0026quot;: \u0026quot;description\u0026quot;,\r\u0026quot;createdAt\u0026quot;: \u0026quot;2019-03-01T14:41:07.716Z\u0026quot;,\r​ \u0026quot;lastModifiedAt\u0026quot;: \u0026quot;2019-03-01T14:41:07.716Z\u0026quot;,\r​ \u0026quot;stages\u0026quot;: [\r​ {\r​ \u0026quot;name\u0026quot;: \u0026quot;Test Stage\u0026quot;,\r​ \u0026quot;steps\u0026quot;: [\r​ {\r​ \u0026quot;stepDefinitionName\u0026quot;: \u0026quot;IntentEngine::CreateAssembly\u0026quot;\r​ \u0026quot;properties\u0026quot;: {\r​ \u0026quot;additionalProp1\u0026quot;: \u0026quot;string\u0026quot;,\r​ \u0026quot;additionalProp2\u0026quot;: \u0026quot;string\u0026quot;,\r​ \u0026quot;additionalProp3\u0026quot;: \u0026quot;string\u0026quot;\r​ },\r​ }\r​ ]\r​ }\r​ ],\r​ \u0026quot;assemblyActors\u0026quot;: [\r​ {\r​ \u0026quot;assemblyConfigurationId\u0026quot;: \u0026quot;7f456ac3-a523-2c1d-2cd1-42dc6124b012\u0026quot;,\r​ \u0026quot;initialState\u0026quot;: \u0026quot;Active\u0026quot;,\r​ \u0026quot;instanceName\u0026quot;: \u0026quot;TestInstance\u0026quot;,\r​ \u0026quot;provided\u0026quot;: false,\r​ \u0026quot;uninstallOnExit\u0026quot;: true\r​ }\r​ ]\r}\rUpdate Scenario Update a Scenario\nRequest    Aspect Value     Endpoint URL /api/behaviour/scenarios/{scenarioId}   HTTP Method PUT    Path Parameters    Field Description Mandatory     scenarioId ID of the Scenario Yes    Body    Field Description Mandatory     name Name of the Scenario Yes   description Supplied description of the Scenario Yes   projectId Unique identifier for the behaviour test Project this Assembly Configuration belongs to Yes   stages A list of stages, each containing the steps to execute in this Scenario Yes   assemblyActors A list of Assembly Configurations to use within the Scenario Yes    Each stages entry contains the following fields:\n   Field Description Mandatory     name Name of the Stage Yes   steps List of steps to execute in this Stage Yes    Each assemblyActors entry contains the following fields:\n   Field Description Mandatory     instanceName Name to refer to this actor throughout the Scenario. If the instance is one to be instantiated, this name will be used as the instance name Yes   assemblyConfigurationId ID of the Assembly Configuration to instantiate this instance from Yes (if provided is false)   initialState State to create the instance in (Installed, Inactive, Active) Yes (if provided is false)   uninstallOnExit Enable/disable the uninstallation of the instance after the Scenario completes successfully Yes (if provided is false)   provided Dictates if the actor is an existing instance of an Assembly OR to be instantiated from an Assembly Configuration Yes    Example:\n{\r​ \u0026quot;name\u0026quot;: \u0026quot;test-scale-in\u0026quot;,\r​ \u0026quot;projectId\u0026quot;: \u0026quot;assembly::Test::1.0\u0026quot;,\r​ \u0026quot;description\u0026quot;: \u0026quot;description\u0026quot;,\r​ \u0026quot;stages\u0026quot;: [\r​ {\r​ \u0026quot;name\u0026quot;: \u0026quot;Test Stage\u0026quot;,\r​ \u0026quot;steps\u0026quot;: [\r​ {\r​ \u0026quot;stepDefinitionName\u0026quot;: \u0026quot;IntentEngine::CreateAssembly\u0026quot;\r​ \u0026quot;properties\u0026quot;: {\r​ \u0026quot;additionalProp1\u0026quot;: \u0026quot;string\u0026quot;,\r​ \u0026quot;additionalProp2\u0026quot;: \u0026quot;string\u0026quot;,\r​ \u0026quot;additionalProp3\u0026quot;: \u0026quot;string\u0026quot;\r​ },\r​ }\r​ ]\r​ }\r​ ],\r​ \u0026quot;assemblyActors\u0026quot;: [\r​ {\r​ \u0026quot;assemblyConfigurationId\u0026quot;: \u0026quot;7f456ac3-a523-2c1d-2cd1-42dc6124b012\u0026quot;,\r​ \u0026quot;initialState\u0026quot;: \u0026quot;Active\u0026quot;,\r​ \u0026quot;instanceName\u0026quot;: \u0026quot;TestInstance\u0026quot;,\r​ \u0026quot;provided\u0026quot;: false,\r​ \u0026quot;uninstallOnExit\u0026quot;: true\r​ }\r​ ]\r}\rResponse    Aspect Value     Content-Type application/json   Response Code 200 (OK)    Body    Field Description     id ID assigned to the Scenario   name Name of the Scenario   description Supplied description of the Scenario   projectId Unique identifier for the behaviour test Project this Assembly Configuration belongs to   stages A list of stages, each containing the steps to execute in this Scenario   assemblyActors A list of Assembly Configurations to use within the Scenario   createdAt Date and time the Scenario instance was created   lastModifiedAt Date and time the Scenario instance was last modified    Each stages entry contains the following fields:\n   Field Description     name Name of the Stage   steps List of steps to execute in this Stage    Each assemblyActors entry contains the following fields:\n   Field Description     instanceName Name to refer to this actor throughout the Scenario. If the instance is one to be instantiated, this name will be used as the instance name   assemblyConfigurationId ID of the Assembly Configuration to instantiate this instance from   initialState State to create the instance in (Installed, Inactive, Active)   uninstallOnExit Enable/disable the uninstallation of the instance after the Scenario completes successfully   provided Dictates if the actor is an existing instance of an Assembly OR to be instantiated from an Assembly Configuration    Example:\n{\r​ \u0026quot;id\u0026quot;: \u0026quot;8e266bc5-e613-4b0d-9ae0-50db6454b026\u0026quot;,\r​ \u0026quot;name\u0026quot;: \u0026quot;test-scale-in\u0026quot;,\r​ \u0026quot;projectId\u0026quot;: \u0026quot;assembly::Test::1.0\u0026quot;,\r​ \u0026quot;description\u0026quot;: \u0026quot;description\u0026quot;,\r\u0026quot;createdAt\u0026quot;: \u0026quot;2019-03-01T14:41:07.716Z\u0026quot;,\r​ \u0026quot;lastModifiedAt\u0026quot;: \u0026quot;2019-03-01T14:41:07.716Z\u0026quot;,\r​ \u0026quot;stages\u0026quot;: [\r​ {\r​ \u0026quot;name\u0026quot;: \u0026quot;Test Stage\u0026quot;,\r​ \u0026quot;steps\u0026quot;: [\r​ {\r​ \u0026quot;stepDefinitionName\u0026quot;: \u0026quot;IntentEngine::CreateAssembly\u0026quot;\r​ \u0026quot;properties\u0026quot;: {\r​ \u0026quot;additionalProp1\u0026quot;: \u0026quot;string\u0026quot;,\r​ \u0026quot;additionalProp2\u0026quot;: \u0026quot;string\u0026quot;,\r​ \u0026quot;additionalProp3\u0026quot;: \u0026quot;string\u0026quot;\r​ },\r​ }\r​ ]\r​ }\r​ ],\r​ \u0026quot;assemblyActors\u0026quot;: [\r​ {\r​ \u0026quot;assemblyConfigurationId\u0026quot;: \u0026quot;7f456ac3-a523-2c1d-2cd1-42dc6124b012\u0026quot;,\r​ \u0026quot;initialState\u0026quot;: \u0026quot;Active\u0026quot;,\r​ \u0026quot;instanceName\u0026quot;: \u0026quot;TestInstance\u0026quot;,\r​ \u0026quot;provided\u0026quot;: false,\r​ \u0026quot;uninstallOnExit\u0026quot;: true\r​ }\r​ ]\r}\rRemove Scenario Remove a Scenario (and all it\u0026rsquo;s executions)\nRequest    Aspect Value     Endpoint URL /api/behaviour/scenarios/{scenarioId}   HTTP Method DELETE    Path Parameters    Field Description Mandatory     scenarioId ID of the Scenario Yes    Response    Aspect Value     Content-Type application/json   Response Code 204 (No Content)    Get all Scenarios Retrieve all Scenarios in a behaviour testing Project (descriptor)\nRequest    Aspect Value     Endpoint URL /api/behaviour/scenarios?projectId={projectId}   HTTP Method GET    Query Parameters    Field Description Mandatory     projectId ID of the behaviour testing project (usually the descriptor name) Yes    Response    Aspect Value     Content-Type application/json   Response Code 200 (OK)    Body The body will contain a single list of Scenarios. Each Scenario will have the following fields:\n   Field Description     id ID assigned to the Scenario   name Name of the Scenario   description Supplied description of the Scenario   projectId Unique identifier for the behaviour test Project this Assembly Configuration belongs to   stages A list of stages, each containing the steps to execute in this Scenario   assemblyActors A list of Assembly Configurations to use within the Scenario   createdAt Date and time the Scenario instance was created   lastModifiedAt Date and time the Scenario instance was last modified    Each stages entry contains the following fields:\n   Field Description     name Name of the Stage   steps List of steps to execute in this Stage    Each assemblyActors entry contains the following fields:\n   Field Description     instanceName Name to refer to this actor throughout the Scenario. If the instance is one to be instantiated, this name will be used as the instance name   assemblyConfigurationId ID of the Assembly Configuration to instantiate this instance from   initialState State to create the instance in (Installed, Inactive, Active)   uninstallOnExit Enable/disable the uninstallation of the instance after the Scenario completes successfully   provided Dictates if the actor is an existing instance of an Assembly OR to be instantiated from an Assembly Configuration    Example:\n[\r{\r​ \u0026quot;id\u0026quot;: \u0026quot;8e266bc5-e613-4b0d-9ae0-50db6454b026\u0026quot;,\r​ \u0026quot;name\u0026quot;: \u0026quot;test-scale-in\u0026quot;,\r​ \u0026quot;projectId\u0026quot;: \u0026quot;assembly::Test::1.0\u0026quot;,\r​ \u0026quot;description\u0026quot;: \u0026quot;description\u0026quot;,\r\u0026quot;createdAt\u0026quot;: \u0026quot;2019-03-01T14:41:07.716Z\u0026quot;,\r​ \u0026quot;lastModifiedAt\u0026quot;: \u0026quot;2019-03-01T14:41:07.716Z\u0026quot;,\r​ \u0026quot;stages\u0026quot;: [\r​ {\r​ \u0026quot;name\u0026quot;: \u0026quot;Test Stage\u0026quot;,\r​ \u0026quot;steps\u0026quot;: [\r​ {\r​ \u0026quot;stepDefinitionName\u0026quot;: \u0026quot;IntentEngine::CreateAssembly\u0026quot;\r​ \u0026quot;properties\u0026quot;: {\r​ \u0026quot;additionalProp1\u0026quot;: \u0026quot;string\u0026quot;,\r​ \u0026quot;additionalProp2\u0026quot;: \u0026quot;string\u0026quot;,\r​ \u0026quot;additionalProp3\u0026quot;: \u0026quot;string\u0026quot;\r​ },\r​ }\r​ ]\r​ }\r​ ],\r​ \u0026quot;assemblyActors\u0026quot;: [\r​ {\r​ \u0026quot;assemblyConfigurationId\u0026quot;: \u0026quot;7f456ac3-a523-2c1d-2cd1-42dc6124b012\u0026quot;,\r​ \u0026quot;initialState\u0026quot;: \u0026quot;Active\u0026quot;,\r​ \u0026quot;instanceName\u0026quot;: \u0026quot;TestInstance\u0026quot;,\r​ \u0026quot;provided\u0026quot;: false,\r​ \u0026quot;uninstallOnExit\u0026quot;: true\r​ }\r​ ]\r},\r...\r]\rGet Scenario Retrieve a single Scenario by ID\nRequest    Aspect Value     Endpoint URL /api/behaviour/scenarios/{scenarioId}   HTTP Method GET    Path Parameters    Field Description Mandatory     scenarioId ID of the Scenario Yes    Response    Aspect Value     Content-Type application/json   Response Code 200 (OK)    Body    Field Description     id ID assigned to the Scenario   name Name of the Scenario   description Supplied description of the Scenario   projectId Unique identifier for the behaviour test Project this Assembly Configuration belongs to   stages A list of stages, each containing the steps to execute in this Scenario   assemblyActors A list of Assembly Configurations to use within the Scenario   createdAt Date and time the Scenario instance was created   lastModifiedAt Date and time the Scenario instance was last modified    Each stages entry contains the following fields:\n   Field Description     name Name of the Stage   steps List of steps to execute in this Stage    Each assemblyActors entry contains the following fields:\n   Field Description     instanceName Name to refer to this actor throughout the Scenario. If the instance is one to be instantiated, this name will be used as the instance name   assemblyConfigurationId ID of the Assembly Configuration to instantiate this instance from   initialState State to create the instance in (Installed, Inactive, Active)   uninstallOnExit Enable/disable the uninstallation of the instance after the Scenario completes successfully   provided Dictates if the actor is an existing instance of an Assembly OR to be instantiated from an Assembly Configuration    Example:\n{\r​ \u0026quot;id\u0026quot;: \u0026quot;8e266bc5-e613-4b0d-9ae0-50db6454b026\u0026quot;,\r​ \u0026quot;name\u0026quot;: \u0026quot;test-scale-in\u0026quot;,\r​ \u0026quot;projectId\u0026quot;: \u0026quot;assembly::Test::1.0\u0026quot;,\r​ \u0026quot;description\u0026quot;: \u0026quot;description\u0026quot;,\r\u0026quot;createdAt\u0026quot;: \u0026quot;2019-03-01T14:41:07.716Z\u0026quot;,\r​ \u0026quot;lastModifiedAt\u0026quot;: \u0026quot;2019-03-01T14:41:07.716Z\u0026quot;,\r​ \u0026quot;stages\u0026quot;: [\r​ {\r​ \u0026quot;name\u0026quot;: \u0026quot;Test Stage\u0026quot;,\r​ \u0026quot;steps\u0026quot;: [\r​ {\r​ \u0026quot;stepDefinitionName\u0026quot;: \u0026quot;IntentEngine::CreateAssembly\u0026quot;\r​ \u0026quot;properties\u0026quot;: {\r​ \u0026quot;additionalProp1\u0026quot;: \u0026quot;string\u0026quot;,\r​ \u0026quot;additionalProp2\u0026quot;: \u0026quot;string\u0026quot;,\r​ \u0026quot;additionalProp3\u0026quot;: \u0026quot;string\u0026quot;\r​ },\r​ }\r​ ]\r​ }\r​ ],\r​ \u0026quot;assemblyActors\u0026quot;: [\r​ {\r​ \u0026quot;assemblyConfigurationId\u0026quot;: \u0026quot;7f456ac3-a523-2c1d-2cd1-42dc6124b012\u0026quot;,\r​ \u0026quot;initialState\u0026quot;: \u0026quot;Active\u0026quot;,\r​ \u0026quot;instanceName\u0026quot;: \u0026quot;TestInstance\u0026quot;,\r​ \u0026quot;provided\u0026quot;: false,\r​ \u0026quot;uninstallOnExit\u0026quot;: true\r​ }\r​ ]\r}\r"},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/reference/security/","title":"Security","tags":[],"description":"","content":""},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/reference/behaviour-testing/","title":"Service Behaviour","tags":[],"description":"","content":""},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/reference/sizing-guidelines/","title":"Sizing your deployment","tags":[],"description":"","content":"Sizing guideline for ICP This guideline is for hardware requirements of installing Lifecycle Manager on top of IBM Cloud Private\nThen, download the Agile Lifecycle Manager (ALM) sizing guideline from the attachment of this page. You will need Microsoft Excel to open this file.\nPut your ICP cluster hardware details, and the desired number of resources you plan for ALM (assuming standard daily lifecycle actions). Make sure there\u0026rsquo;s enough hardware resources for your case.  \u0026lt;li\u0026gt;\r\u0026lt;a href=\u0026quot;https://pages.github.ibm.com/TNC/alm-docs.github.io//reference/sizing-guidelines.files/ALM_Sizing_guideline_062019.xlsx\u0026quot; \u0026gt;\rALM_Sizing_guideline_062019.xlsx\r\u0026lt;/a\u0026gt;\r(39 kB)\r\u0026lt;/li\u0026gt;\r "},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/tags/","title":"Tags","tags":[],"description":"","content":""},{"uri":"https://pages.github.ibm.com/TNC/alm-docs.github.io/reference/lm-api/api-definition/topology/topology/","title":"Topology","tags":[],"description":"","content":"The following details the API used to retrieve information about Assembly instances within LM.\nAssociated with each definition are details of the request parameters and responses. These include the name of each field plus a brief description and whether the field is mandatory. Whether a field is required or not is based on the context of the examples. The underlying API definition may mark a field as optional, but in some contexts, the fields must be supplied.\nFor a 40x, 50x or any other error response please see error response codes\nGet Assembly Gets the assembly with the given ID.\nRequest    Aspect Value     Endpoint URL /api/topology/assemblies/{assemblyId}   HTTP Method GET    Path Parameters    Field Description Mandatory     assemblyId ID of the Assembly instance Yes    Response    Aspect Value     Content-Type application/json   Response Code 200 (OK)    Body    Field Description     type The type of entity being returned – always \u0026ldquo;Assembly\u0026rdquo;   id The internal id of the Assembly   name The name of the Assembly   state The current state of the Assembly (Installed, Inactive, Active). This field may be missing if the Assembly has not reached the Installed state   descriptorName The name of the Assembly descriptor associated with the Assembly instance   properties A collection of Assembly level properties. Each property will have a name and value field   createdAt The date and time the Assembly was created   lastModifiedAt The date and time the Assembly was last modified   children A collection of components that make up the Assembly. When the component is of \u0026lsquo;type\u0026rsquo; \u0026lsquo;Assembly\u0026rsquo; the contents are the same as for the top level assembly. When the \u0026lsquo;type\u0026rsquo; is \u0026lsquo;component\u0026rsquo; the entry is in fact a Resource. This will have a type, name, ID and a set of associated properties   relationships A collection of relationships associated with the Assembly instance. Each relationship has a name and the ID of the source and target components involved in the relationship. Relationships also have a property section   references A collection of references used by the Assembly. References can be to Resources provided by Resource Managers but not created using any Assembly and other existing Assembly instances    Example:\n{\r\u0026quot;type\u0026quot;: \u0026quot;Assembly\u0026quot;,\r\u0026quot;id\u0026quot;: \u0026quot;bf649336-c8c5-49d9-9f4e-60567fe54135\u0026quot;,\r\u0026quot;name\u0026quot;: \u0026quot;test_1\u0026quot;,\r\u0026quot;state\u0026quot;: \u0026quot;Active\u0026quot;,\r\u0026quot;descriptorName\u0026quot;: \u0026quot;assembly::t_bta::1.0\u0026quot;,\r\u0026quot;properties\u0026quot;: [\r​ {\r​ \u0026quot;name\u0026quot;: \u0026quot;data\u0026quot;,\r​ \u0026quot;value\u0026quot;: \u0026quot;data\u0026quot;\r​ },\r​ ...\r],\r\u0026quot;createdAt\u0026quot;: \u0026quot;2017-08-02T22:28:41.906+0000\u0026quot;,\r\u0026quot;lastModifiedAt\u0026quot;: \u0026quot;2017-08-02T22:47:46.189+0000\u0026quot;,\r\u0026quot;children\u0026quot;: [\r​ {\r​ \u0026quot;type\u0026quot;: \u0026quot;Component\u0026quot;,\r​ \u0026quot;id\u0026quot;: \u0026quot;aa56626d-cfec-410b-afb7-7160019bdff0\u0026quot;,\r​ \u0026quot;name\u0026quot;: \u0026quot;test_1__A\u0026quot;,\r​ ...\r],\r\u0026quot;relationships\u0026quot;: [\r​ {\r​ \u0026quot;name\u0026quot;: \u0026quot;third-relationship__1\u0026quot;,\r​ \u0026quot;sourceId\u0026quot;: \u0026quot;aa56626d-cfec-410b-afb7-7160019bdff0\u0026quot;,\r​ \u0026quot;targetId\u0026quot;: \u0026quot;9c525d0c-18d4-404f-a5b2-8a55480660a8\u0026quot;,\r​ \u0026quot;properties\u0026quot;: [\r​ {\r​ \u0026quot;name\u0026quot;: \u0026quot;source\u0026quot;,\r​ \u0026quot;value\u0026quot;: \u0026quot;test_1__A\u0026quot;\r​ }\r​ ]\r​ }\r],\r\u0026quot;references\u0026quot;: [\r​ {\r​ \u0026quot;id\u0026quot;: \u0026quot;1c269f9d-fcca-4754-946c-6f3e6179bf38\u0026quot;,\r​ \u0026quot;name\u0026quot;: \u0026quot;internal-network\u0026quot;,\r​ \u0026quot;type\u0026quot;: \u0026quot;resource::openstack_neutron_network::1.0\u0026quot;,\r...\r​ },\r​ ...\r]\r}\rGet Assembly by Name Retrieve information about an Assembly instance using it\u0026rsquo;s name.\nRequest    Aspect Value     Endpoint URL /api/topology/assemblies?name={name}   HTTP Method GET    Query Parameters    Field Description Mandatory     name name of the Assembly to retrieve Yes    Response    Aspect Value     Content-Type application/json   Response Code 200 (OK)    Body    Field Description     type The type of entity being returned – always \u0026ldquo;Assembly\u0026rdquo;   id The internal id of the Assembly   name The name of the Assembly   state The current state of the Assembly (Installed, Inactive, Active). This field may be missing if the Assembly has not reached the Installed state   descriptorName The name of the Assembly descriptor associated with the Assembly instance   properties A collection of Assembly level properties. Each property will have a name and value field   createdAt The date and time the Assembly was created   lastModifiedAt The date and time the Assembly was last modified   children A collection of components that make up the Assembly. When the component is of \u0026lsquo;type\u0026rsquo; \u0026lsquo;Assembly\u0026rsquo; the contents are the same as for the top level assembly. When the \u0026lsquo;type\u0026rsquo; is \u0026lsquo;component\u0026rsquo; the entry is in fact a Resource. This will have a type, name, ID and a set of associated properties   relationships A collection of relationships associated with the Assembly instance. Each relationship has a name and the ID of the source and target components involved in the relationship. Relationships also have a property section   references A collection of references used by the Assembly. References can be to Resources provided by Resource Managers but not created using any Assembly and other existing Assembly instances    Example:\n{\r\u0026quot;type\u0026quot;: \u0026quot;Assembly\u0026quot;,\r\u0026quot;id\u0026quot;: \u0026quot;bf649336-c8c5-49d9-9f4e-60567fe54135\u0026quot;,\r\u0026quot;name\u0026quot;: \u0026quot;test_1\u0026quot;,\r\u0026quot;state\u0026quot;: \u0026quot;Active\u0026quot;,\r\u0026quot;descriptorName\u0026quot;: \u0026quot;assembly::t_bta::1.0\u0026quot;,\r\u0026quot;properties\u0026quot;: [\r​ {\r​ \u0026quot;name\u0026quot;: \u0026quot;data\u0026quot;,\r​ \u0026quot;value\u0026quot;: \u0026quot;data\u0026quot;\r​ },\r​ ...\r],\r\u0026quot;createdAt\u0026quot;: \u0026quot;2017-08-02T22:28:41.906+0000\u0026quot;,\r\u0026quot;lastModifiedAt\u0026quot;: \u0026quot;2017-08-02T22:47:46.189+0000\u0026quot;,\r\u0026quot;children\u0026quot;: [\r​ {\r​ \u0026quot;type\u0026quot;: \u0026quot;Component\u0026quot;,\r​ \u0026quot;id\u0026quot;: \u0026quot;aa56626d-cfec-410b-afb7-7160019bdff0\u0026quot;,\r​ \u0026quot;name\u0026quot;: \u0026quot;test_1__A\u0026quot;,\r​ ...\r],\r\u0026quot;relationships\u0026quot;: [\r​ {\r​ \u0026quot;name\u0026quot;: \u0026quot;third-relationship__1\u0026quot;,\r​ \u0026quot;sourceId\u0026quot;: \u0026quot;aa56626d-cfec-410b-afb7-7160019bdff0\u0026quot;,\r​ \u0026quot;targetId\u0026quot;: \u0026quot;9c525d0c-18d4-404f-a5b2-8a55480660a8\u0026quot;,\r​ \u0026quot;properties\u0026quot;: [\r​ {\r​ \u0026quot;name\u0026quot;: \u0026quot;source\u0026quot;,\r​ \u0026quot;value\u0026quot;: \u0026quot;test_1__A\u0026quot;\r​ }\r​ ]\r​ }\r],\r\u0026quot;references\u0026quot;: [\r​ {\r​ \u0026quot;id\u0026quot;: \u0026quot;1c269f9d-fcca-4754-946c-6f3e6179bf38\u0026quot;,\r​ \u0026quot;name\u0026quot;: \u0026quot;internal-network\u0026quot;,\r​ \u0026quot;type\u0026quot;: \u0026quot;resource::openstack_neutron_network::1.0\u0026quot;,\r...\r​ },\r​ ...\r]\r}\rGet Assemblies by Partial Name Search Retrieve a list of Assembly instances using a partial name search.\nRequest    Aspect Value     Endpoint URL /api/topology/assemblies?nameContains={partialName}   HTTP Method GET    Query Parameters    Field Description Mandatory     partialName partial name of the Assembly to retrieve Yes    Response    Aspect Value     Content-Type application/json   Response Code 200 (OK)    Body    Field Description     id The internal id of the Assembly   name The name of the Assembly   state The current state of the Assembly (Installed, Inactive, Active). This field may be missing if the Assembly has not reached the Installed state   descriptorName The name of the Assembly descriptor associated with the Assembly instance   createdAt The date and time the Assembly was created   lastModifiedAt The date and time the Assembly was last modified   partial A value of \u0026ldquo;true\u0026rdquo; indicates that there were more than 50 search results and not all results could be returned (there is a maximum limit of 50 results that can be returned by the search). A value of \u0026ldquo;false\u0026rdquo; means there were 50 or fewer search results and all of them were returned.    Example:\n{\r\u0026quot;assemblies\u0026quot;: [\r{\r\u0026quot;id\u0026quot;: \u0026quot;51e3150b-728b-49a3-97fc-8e8533f114a6\u0026quot;,\r\u0026quot;name\u0026quot;: \u0026quot;test_1\u0026quot;,\r\u0026quot;descriptorName\u0026quot;: \u0026quot;assembly::h_bta::1.0\u0026quot;,\r\u0026quot;state\u0026quot;: \u0026quot;Active\u0026quot;,\r\u0026quot;createdAt\u0026quot;: \u0026quot;2020-02-17T14:57:27.745Z\u0026quot;,\r\u0026quot;lastModifiedAt\u0026quot;: \u0026quot;2020-02-18T14:26:13.95Z\u0026quot;,\r\u0026quot;lastTransition\u0026quot;: {\r\u0026quot;id\u0026quot;: \u0026quot;4ae61264-91ec-4a8a-ae79-5f63b6ae900f\u0026quot;,\r\u0026quot;intentType\u0026quot;: \u0026quot;HealAssembly\u0026quot;,\r\u0026quot;status\u0026quot;: \u0026quot;Completed\u0026quot;,\r\u0026quot;startDateTime\u0026quot;: \u0026quot;2020-02-18T14:26:10.522Z\u0026quot;,\r\u0026quot;endDateTime\u0026quot;: \u0026quot;2020-02-18T14:26:26.846Z\u0026quot;\r}\r}\r],\r\u0026quot;partial\u0026quot;: false\r}\r"}]